import torch
import torch.nn.functional as F
from einops.layers.torch import Rearrange
from torch import einsum, nn
from torch.utils.checkpoint import checkpoint_sequential
from einops import rearrange, repeat
# helpers


def exists(val):
    return val is not None


def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d

def init_zero_(layer):
    nn.init.constant_(layer.weight, 0.0)
    if exists(layer.bias):
        nn.init.constant_(layer.bias, 0.0)


# attention
class Attention(nn.Module):
    def __init__(
        self,
        dim,
        seq_len=None,
        heads=8,
        dim_head=64,
        dropout=0.0,
        gating=True,
    ):
        super().__init__()
        inner_dim = dim_head * heads
        self.seq_len = seq_len
        self.heads = heads
        self.scale = dim_head**-0.5

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, dim)

        self.gating = nn.Linear(dim, inner_dim)
        nn.init.constant_(self.gating.weight, 0.0)
        nn.init.constant_(self.gating.bias, 1.0)

        self.dropout = nn.Dropout(dropout)
        init_zero_(self.to_out)

    def forward(
        self,
        x,
        mask=None,
        attn_bias=None,
    ):
        device, orig_shape, h = (
            x.device,
            x.shape,
            self.heads,
        )

        q, k, v = (
            self.to_q(x),
            *self.to_kv(x).chunk(2, dim=-1),
        )

        i, j = q.shape[-2], k.shape[-2]

        q, k, v = map(
            lambda t: rearrange(t, "b n (h d) -> b h n d", h=h),
            (q, k, v),
        )

        # scale

        q = q * self.scale

        # query / key similarities

        dots = einsum("b h i d, b h j d -> b h i j", q, k)
        attn = dots.softmax(dim=-1)
        attn = self.dropout(attn)

        # aggregate

        out = einsum("b h i j, b h j d -> b h i d", attn, v)

        # merge heads

        out = rearrange(out, "b h n d -> b n (h d)")

        # gating

        gates = self.gating(x)
        out = out * gates.sigmoid()

        # combine to out

        out = self.to_out(out)
        return out

class AxialAttention(nn.Module):
    def __init__(
        self,
        dim: int,
        heads: int,
        row_attn: bool = True,
        col_attn: bool = True,
        accept_edges: bool = False,
        global_query_attn: bool = False,
        **kwargs,
    ):
        """
        Axial Attention module.

        Args:
            dim (int): The input dimension.
            heads (int): The number of attention heads.
            row_attn (bool, optional): Whether to perform row attention. Defaults to True.
            col_attn (bool, optional): Whether to perform column attention. Defaults to True.
            accept_edges (bool, optional): Whether to accept edges for attention bias. Defaults to False.
            global_query_attn (bool, optional): Whether to perform global query attention. Defaults to False.
            **kwargs: Additional keyword arguments for the Attention module.
        """
        super().__init__()
        assert not (
            not row_attn and not col_attn
        ), "row or column attention must be turned on"

        self.row_attn = row_attn
        self.col_attn = col_attn
        self.global_query_attn = global_query_attn

        self.norm = nn.LayerNorm(dim)

        self.attn = Attention(dim=dim, heads=heads, **kwargs)

        self.edges_to_attn_bias = (
            nn.Sequential(
                nn.Linear(dim, heads, bias=False),
                Rearrange("b i j h -> b h i j"),
            )
            if accept_edges
            else None
        )

    def forward(
        self,
        x: torch.Tensor,
        edges: torch.Tensor = None,
        mask: torch.Tensor = None,
    ) -> torch.Tensor:
        """
        Forward pass of the Axial Attention module.

        Args:
            x (torch.Tensor): The input tensor of shape (batch_size, height, width, dim).
            edges (torch.Tensor, optional): The edges tensor for attention bias. Defaults to None.
            mask (torch.Tensor, optional): The mask tensor for masking attention. Defaults to None.

        Returns:
            torch.Tensor: The output tensor of shape (batch_size, height, width, dim).
        """
        assert (
            self.row_attn ^ self.col_attn
        ), "has to be either row or column attention, but not both"

        b, h, w, d = x.shape

        x = self.norm(x)

        # axial attention

        if self.col_attn:
            axial_dim = w
            mask_fold_axial_eq = "b h w -> (b w) h"
            input_fold_eq = "b h w d -> (b w) h d"
            output_fold_eq = "(b w) h d -> b h w d"

        elif self.row_attn:
            axial_dim = h
            mask_fold_axial_eq = "b h w -> (b h) w"
            input_fold_eq = "b h w d -> (b h) w d"
            output_fold_eq = "(b h) w d -> b h w d"

        x = rearrange(x, input_fold_eq)

        if exists(mask):
            mask = rearrange(mask, mask_fold_axial_eq)

        attn_bias = None
        if exists(self.edges_to_attn_bias) and exists(edges):
            attn_bias = self.edges_to_attn_bias(edges)
            attn_bias = repeat(
                attn_bias, "b h i j -> (b x) h i j", x=axial_dim
            )


        out = self.attn(
            x, mask=mask, attn_bias=attn_bias)
        out = rearrange(out, output_fold_eq, h=h, w=w)

        return out



class MSASelfAttentionBlock(nn.Module):
    def __init__(self, dim, seq_len, heads, dim_head, dropout=0.0):
        super().__init__()
        self.row_attn = AxialAttention(
            dim=dim,
            heads=heads,
            dim_head=dim_head,
            row_attn=True,
            col_attn=False,
            accept_edges=True,
        )
        self.col_attn = AxialAttention(
            dim=dim,
            heads=heads,
            dim_head=dim_head,
            row_attn=False,
            col_attn=True,
        )

    def forward(self, x, mask=None, pairwise_repr=None):
        x = self.row_attn(x, mask=mask, edges=pairwise_repr) + x
        x = self.col_attn(x, mask=mask) + x
        return x

# Main model that uses the trainable tokenizer, and attention-based mechanism
class MSAModel(nn.Module):
    def __init__(self, vocab_size, seq_len, embed_dim, num_heads, num_layers):
        super().__init__()
        VOCAB = ["PAD","A", "R", "N", "D", "C", "Q", "E", "G", "H", "I", "L", "K", "M", "F", "P", "S", "T", "W", "Y", "V", "-"]
        token_to_idx = {token: idx for idx, token in enumerate(VOCAB)}
        # Trainable embedding layer for tokenization
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=token_to_idx["PAD"])
        self.MSASelfAttention = MSASelfAttentionBlock(dim=embed_dim, seq_len=seq_len, heads=8, dim_head=64, dropout=0.3)        
        self.MSABidirectionalCrossAttention = MSABidirectionalCrossAttention(embed_dim=embed_dim, num_heads=num_heads)
        # Final prediction layer (e.g., for structural prediction)
        self.fc = nn.Linear(embed_dim * seq_len, 1)  # Output size depends on prediction task (e.g., contact map, binding score)

    def forward(self, sequences1, sequences2):
        # Token embedding (turn sequences into embeddings)
#        print("sequences_shape",sequences.shape)
        batch_size = sequences1.shape[0]
        embedded1 = self.embedding(sequences1)
        embedded2 = self.embedding(sequences2)
#        print("embedded_shape",embedded.shape)
        output1 = self.MSASelfAttention(embedded1)
        output2 = self.MSASelfAttention(embedded2)
#        print("output_afteratt_shape",output.shape)
        # Bidirectional cross attention
        output1, output2 = MSABidirectionalCrossAttention(output1, output2)
        # Take information only from the original sequence
        output1 = output1[:,0,:,:]
        output2 = output2[:,0,:,:]
#        print("output_1_shape",output.shape)
        output1 = output1.reshape(batch_size,-1)
        output2 = output2.reshape(batch_size,-1)
#        print("output_reshape",output.shape)
        # Final prediction
        prediction1 = self.fc(output1)
        prediction2 = self.fc(output2)
        return prediction1, prediction2
 

class MSABidirectionalCrossAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MSABidirectionalCrossAttention, self).__init__()
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.head_dim = embed_dim // num_heads
        
        assert self.head_dim * num_heads == embed_dim, "Embedding dimension must be divisible by the number of heads."
        
        # Linear layers for Q, K, V transformations for both directions
        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        
        # Output linear transformations
        self.out1 = nn.Linear(embed_dim, embed_dim)  # For mat1 -> mat2 attention output
        self.out2 = nn.Linear(embed_dim, embed_dim)  # For mat2 -> mat1 attention output
        
        # Scaling factor for dot-product attention
        self.scale = self.head_dim ** 0.5

    def forward(self, mat1, mat2):
        """
        Performs cross attention between mat1 and mat2 (both directions).

        Args:
        - mat1: First protein's MSA matrix (b, n, l, d)
        - mat2: Second protein's MSA matrix (b, n, l, d)

        Returns:
        - mat1_to_mat2_attention: Attention output for mat1 attending to mat2 (b, n, l, d)
        - mat2_to_mat1_attention: Attention output for mat2 attending to mat1 (b, n, l, d)
        """
        batch_size, num_sequences, seq_len, embed_dim = mat1.shape
        
        # 1. mat1 attends to mat2 (Q from mat1, K/V from mat2)
        Q1 = self.q_linear(mat1)  # Query from mat1
        K2 = self.k_linear(mat2)  # Key from mat2
        V2 = self.v_linear(mat2)  # Value from mat2
        
        # Reshape for multi-head attention
        Q1 = Q1.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        K2 = K2.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        V2 = V2.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        
        # Calculate attention from mat1 to mat2
        attention_scores_1 = torch.matmul(Q1, K2.transpose(-2, -1)) / self.scale  # (b, n, h, l, l)
        attention_weights_1 = F.softmax(attention_scores_1, dim=-1)  # (b, n, h, l, l)
        mat1_to_mat2_attention = torch.matmul(attention_weights_1, V2)  # (b, n, h, l, head_dim)
        
        # Reshape back to (b, n, l, d)
        mat1_to_mat2_attention = mat1_to_mat2_attention.transpose(2, 3).contiguous().view(batch_size, num_sequences, seq_len, embed_dim)
        
        # Apply output linear layer
        mat1_to_mat2_attention = self.out1(mat1_to_mat2_attention)  # (b, n, l, d)

        
        # 2. mat2 attends to mat1 (Q from mat2, K/V from mat1)
        Q2 = self.q_linear(mat2)  # Query from mat2
        K1 = self.k_linear(mat1)  # Key from mat1
        V1 = self.v_linear(mat1)  # Value from mat1
        
        # Reshape for multi-head attention
        Q2 = Q2.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        K1 = K1.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        V1 = V1.view(batch_size, num_sequences, seq_len, self.num_heads, self.head_dim).transpose(2, 3)  # (b, n, h, l, head_dim)
        
        # Calculate attention from mat2 to mat1
        attention_scores_2 = torch.matmul(Q2, K1.transpose(-2, -1)) / self.scale  # (b, n, h, l, l)
        attention_weights_2 = F.softmax(attention_scores_2, dim=-1)  # (b, n, h, l, l)
        mat2_to_mat1_attention = torch.matmul(attention_weights_2, V1)  # (b, n, h, l, head_dim)
        
        # Reshape back to (b, n, l, d)
        mat2_to_mat1_attention = mat2_to_mat1_attention.transpose(2, 3).contiguous().view(batch_size, num_sequences, seq_len, embed_dim)
        
        # Apply output linear layer
        mat2_to_mat1_attention = self.out2(mat2_to_mat1_attention)  # (b, n, l, d)
        
        return mat1_to_mat2_attention, mat2_to_mat1_attention


