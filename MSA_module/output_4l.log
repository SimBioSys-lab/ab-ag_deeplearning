{'batch_size': 6, 'sequence_file': 'preprocessed_seq_ab_train_1200.npz', 'pt_file': 'pt_train_data.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 4, 'num_classes': 2, 'num_epochs': 1000, 'learning_rate': 0.0001, 'max_grad_norm': 0.1, 'validation_split': 0.1, 'early_stop_patience': 30, 'gradient_accumulation_steps': 4, 'gradient_noise_std': 0.01, 'checkpoint_path': 'checkpoint.pt'}
Number of GPUs available: 1
Initializing ParatopeModel with 4 self-attention layers...
