{'batch_size': 16, 'sequence_file': 'preprocessed_seq_ab_1200.npz', 'pt_file': 'cdrs_output.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 1, 'num_classes': 2, 'num_epochs': 20, 'learning_rate': 0.003, 'n_splits': 3, 'max_grad_norm': 0.5}
Number of GPUs available: 4
Starting fold 1/3
Initializing ParatopeModel with 1 repeated layers...
Using 4 GPUs with DataParallel.
Fold 1, Epoch [1/20], Training Loss: 5.2342
Fold 1, Epoch [2/20], Training Loss: 0.6679
Fold 1, Epoch [3/20], Training Loss: 0.6463
Fold 1, Epoch [4/20], Training Loss: 0.6316
Fold 1, Epoch [5/20], Training Loss: 0.6158
Fold 1, Epoch [6/20], Training Loss: 0.6069
Fold 1, Epoch [7/20], Training Loss: 0.5888
Fold 1, Epoch [8/20], Training Loss: 0.5753
Fold 1, Epoch [9/20], Training Loss: 0.5637
Fold 1, Epoch [10/20], Training Loss: 0.5481
Fold 1, Epoch [11/20], Training Loss: 0.5106
Fold 1, Epoch [12/20], Training Loss: 0.4958
Fold 1, Epoch [13/20], Training Loss: 0.4800
Fold 1, Epoch [14/20], Training Loss: 0.4677
Fold 1, Epoch [15/20], Training Loss: 0.4558
Fold 1, Epoch [16/20], Training Loss: 0.4486
Fold 1, Epoch [17/20], Training Loss: 0.4356
Fold 1, Epoch [18/20], Training Loss: 0.4283
Fold 1, Epoch [19/20], Training Loss: 0.4187
Fold 1, Epoch [20/20], Training Loss: 0.4088
Fold 1 Validation Loss: 0.4445
New best model found at fold 1 with validation loss: 0.4445
Starting fold 2/3
Initializing ParatopeModel with 1 repeated layers...
Using 4 GPUs with DataParallel.
Fold 2, Epoch [1/20], Training Loss: 5.2892
Fold 2, Epoch [2/20], Training Loss: 0.6676
Fold 2, Epoch [3/20], Training Loss: 0.6489
Fold 2, Epoch [4/20], Training Loss: 0.6298
Fold 2, Epoch [5/20], Training Loss: 0.6164
Fold 2, Epoch [6/20], Training Loss: 0.6057
Fold 2, Epoch [7/20], Training Loss: 0.5874
Fold 2, Epoch [8/20], Training Loss: 0.5722
Fold 2, Epoch [9/20], Training Loss: 0.5612
Fold 2, Epoch [10/20], Training Loss: 0.5439
Fold 2, Epoch [11/20], Training Loss: 0.5060
Fold 2, Epoch [12/20], Training Loss: 0.4844
Fold 2, Epoch [13/20], Training Loss: 0.4678
Fold 2, Epoch [14/20], Training Loss: 0.4556
Fold 2, Epoch [15/20], Training Loss: 0.4466
Fold 2, Epoch [16/20], Training Loss: 0.4359
Fold 2, Epoch [17/20], Training Loss: 0.4245
Fold 2, Epoch [18/20], Training Loss: 0.4192
Fold 2, Epoch [19/20], Training Loss: 0.4107
Fold 2, Epoch [20/20], Training Loss: 0.4001
Fold 2 Validation Loss: 0.4513
Starting fold 3/3
Initializing ParatopeModel with 1 repeated layers...
Using 4 GPUs with DataParallel.
Fold 3, Epoch [1/20], Training Loss: 4.1325
Fold 3, Epoch [2/20], Training Loss: 0.6637
Fold 3, Epoch [3/20], Training Loss: 0.6472
Fold 3, Epoch [4/20], Training Loss: 0.6161
Fold 3, Epoch [5/20], Training Loss: 0.5842
Fold 3, Epoch [6/20], Training Loss: 0.5638
Fold 3, Epoch [7/20], Training Loss: 0.5376
Fold 3, Epoch [8/20], Training Loss: 0.5091
Fold 3, Epoch [9/20], Training Loss: 0.4901
Fold 3, Epoch [10/20], Training Loss: 0.4737
Fold 3, Epoch [11/20], Training Loss: 0.4498
Fold 3, Epoch [12/20], Training Loss: 0.4267
Fold 3, Epoch [13/20], Training Loss: 0.4221
Fold 3, Epoch [14/20], Training Loss: 0.4096
Fold 3, Epoch [15/20], Training Loss: 0.4004
Fold 3, Epoch [16/20], Training Loss: 0.3892
Fold 3, Epoch [17/20], Training Loss: 0.3828
Fold 3, Epoch [18/20], Training Loss: 0.3782
Fold 3, Epoch [19/20], Training Loss: 0.3677
Fold 3, Epoch [20/20], Training Loss: 0.3597
Fold 3 Validation Loss: 0.4243
New best model found at fold 3 with validation loss: 0.4243
Average Validation Loss across 3 folds: 0.4400
Best model saved successfully.
Layer: module.embedding.weight | Size: torch.Size([22, 128]) | Values : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [-1.4374, -0.2525,  0.5980, -1.1906,  0.4055, -0.6893,  1.3809, -1.3555,
          0.6605,  0.1125, -2.1015,  1.3295,  0.5953,  0.1070,  0.0296, -0.8367,
          1.4438, -1.1417,  0.0203, -0.0411,  0.0168, -0.3049,  1.3079, -0.9256,
         -1.5624, -0.5839, -0.4471,  0.3428, -1.4946, -1.0618, -1.4224,  2.0135,
         -0.4357,  0.5026,  0.8764, -0.3765,  0.3548,  1.9371,  0.1365, -0.7749,
          2.0603,  0.1811, -0.6888,  0.2600,  0.1615, -0.1941, -1.2207, -1.6128,
          0.0690,  0.4365, -0.8264, -1.0415,  0.1513,  0.3550,  0.1842, -1.0196,
         -0.1357,  0.3126,  2.1178,  1.6774, -0.3311,  0.0664,  0.5317,  0.6017,
         -0.4614,  0.5506, -0.2863, -0.3117,  2.0713,  0.5767, -0.4433,  0.7515,
          0.0530,  0.7576, -1.1684,  0.4645,  0.0523,  1.4746, -0.2731,  2.2389,
          0.9913,  0.9303,  1.6357,  0.4713, -1.9931, -1.3096,  1.3106, -0.9812,
          0.3544,  0.4337, -1.6018, -0.6365, -0.2595, -0.8607,  1.6136, -0.6595,
         -1.4072,  0.2722, -1.8592, -0.3008,  0.1861, -0.3103,  0.1603,  2.3807,
          0.7211,  1.6175,  0.1532, -0.1613, -0.9434,  1.1089, -0.0266, -1.6983,
         -1.5373, -0.1979,  0.4448, -1.3290,  0.7820,  0.0410, -0.8760, -0.7956,
          1.2344,  0.9276, -1.5623, -0.0271,  1.6065, -0.0085, -0.9297,  0.8812]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.9160, 0.9033], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([-0.0387, -0.0513], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-0.0170,  0.0592, -0.0481,  0.0601,  0.0590,  0.0352,  0.0662, -0.0054,
         -0.0155,  0.0593,  0.0545, -0.0060, -0.0398, -0.0305, -0.0785,  0.0444,
          0.0511,  0.0328,  0.0038, -0.0423,  0.0496, -0.0419,  0.0520, -0.0503,
          0.0671,  0.0618, -0.0361, -0.0877, -0.0861, -0.0088, -0.0431,  0.0807,
          0.0652,  0.0453, -0.0462, -0.0399,  0.0699,  0.0525,  0.0055, -0.0281,
         -0.0847, -0.0334,  0.0538,  0.0128,  0.0844,  0.0279,  0.0820, -0.0706,
          0.0736, -0.0430, -0.0014,  0.0585,  0.0319,  0.0387,  0.0702,  0.0332,
         -0.0453,  0.0072, -0.0138,  0.0561,  0.0263,  0.0318, -0.0727,  0.0821,
         -0.0579,  0.0849, -0.0227, -0.0768, -0.0203,  0.0682,  0.0607,  0.0159,
         -0.0459, -0.0877, -0.0734, -0.0836,  0.0364, -0.0614,  0.0151, -0.0842,
         -0.0604,  0.0862, -0.0114,  0.0526, -0.0289,  0.0209, -0.0395, -0.0535,
          0.0858,  0.0001, -0.0478, -0.0689,  0.0440,  0.0726, -0.0645, -0.0721,
         -0.0585, -0.0201,  0.0862, -0.0283, -0.0148,  0.0138, -0.0828, -0.0534,
         -0.0309, -0.0617,  0.0746,  0.0772, -0.0552, -0.0167,  0.0748,  0.0505,
         -0.0360,  0.0145, -0.0036,  0.0772,  0.0221, -0.0348,  0.0558,  0.0591,
          0.0719,  0.0310, -0.0014,  0.0526,  0.0217, -0.0753,  0.0831, -0.0708],
        [ 0.0612, -0.0588, -0.0285, -0.0515,  0.0239, -0.0500,  0.0842,  0.0047,
         -0.0628, -0.0347, -0.0780,  0.0747,  0.0437,  0.0260,  0.0317,  0.0317,
         -0.0258,  0.0745, -0.0561, -0.0852,  0.0041, -0.0430, -0.0797, -0.0086,
         -0.0730,  0.0571,  0.0547, -0.0517, -0.0644, -0.0058, -0.0035, -0.0374,
          0.0490, -0.0050, -0.0115, -0.0877,  0.0041, -0.0314, -0.0022, -0.0465,
         -0.0077,  0.0208, -0.0315,  0.0050, -0.0033,  0.0646, -0.0380, -0.0785,
         -0.0437, -0.0001,  0.0872,  0.0459, -0.0712, -0.0094,  0.0128, -0.0289,
         -0.0227,  0.0818, -0.0597,  0.0138,  0.0205,  0.0690,  0.0706, -0.0184,
         -0.0590,  0.0045, -0.0674, -0.0554,  0.0372,  0.0282, -0.0054, -0.0292,
          0.0416,  0.0514,  0.0608, -0.0338, -0.0068,  0.0611, -0.0473, -0.0426,
         -0.0106,  0.0816, -0.0259,  0.0410, -0.0429, -0.0640, -0.0822,  0.0590,
          0.0088,  0.0242, -0.0425,  0.0619,  0.0471,  0.0395,  0.0724,  0.0073,
          0.0760,  0.0020, -0.0189, -0.0692, -0.0467, -0.0092,  0.0209, -0.0850,
         -0.0555, -0.0796,  0.0480,  0.0128, -0.0628,  0.0221,  0.0177, -0.0858,
          0.0838, -0.0081, -0.0685, -0.0181,  0.0014, -0.0705,  0.0476,  0.0358,
         -0.0103,  0.0730, -0.0738,  0.0619,  0.0292,  0.0739, -0.0047, -0.0771]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[-0.0535,  0.0429, -0.0797,  0.0289, -0.0309,  0.0049,  0.0734, -0.0206,
          0.0573,  0.0603,  0.0198, -0.0338,  0.0025,  0.0382,  0.0879,  0.0112,
         -0.0768,  0.0508,  0.0111,  0.0569, -0.0535,  0.0415,  0.0403,  0.0865,
          0.0376,  0.0783, -0.0539, -0.0687,  0.0043,  0.0484,  0.0323,  0.0466,
          0.0879,  0.0756,  0.0568, -0.0205,  0.0338, -0.0394, -0.0758,  0.0386,
          0.0101,  0.0606, -0.0163,  0.0813, -0.0231, -0.0623, -0.0231, -0.0124,
         -0.0486,  0.0289,  0.0463, -0.0742, -0.0280, -0.0656, -0.0826, -0.0796,
          0.0732,  0.0728, -0.0116, -0.0859, -0.0086, -0.0513,  0.0072, -0.0241,
          0.0203,  0.0850, -0.0035,  0.0747, -0.0726, -0.0028,  0.0279,  0.0142,
          0.0641,  0.0105, -0.0573,  0.0348,  0.0493,  0.0309, -0.0361,  0.0203,
         -0.0691, -0.0778, -0.0771, -0.0286,  0.0437,  0.0521, -0.0182, -0.0767,
         -0.0056,  0.0320,  0.0763, -0.0767, -0.0697, -0.0189, -0.0479, -0.0445,
         -0.0302, -0.0397, -0.0304,  0.0434,  0.0730, -0.0276, -0.0049,  0.0533,
          0.0614, -0.0123, -0.0788,  0.0814,  0.0610, -0.0570,  0.0300, -0.0428,
          0.0720,  0.0719,  0.0832, -0.0090,  0.0305, -0.0248,  0.0097, -0.0283,
         -0.0069, -0.0281,  0.0737, -0.0779, -0.0759, -0.0426, -0.0038, -0.0393],
        [ 0.0327,  0.0572, -0.0093, -0.0745, -0.0392, -0.0330, -0.0656, -0.0546,
          0.0456, -0.0847,  0.0636,  0.0288, -0.0068,  0.0423,  0.0490,  0.0377,
          0.0106, -0.0453,  0.0859, -0.0551,  0.0363,  0.0149, -0.0600, -0.0221,
          0.0874,  0.0649, -0.0049, -0.0797, -0.0408,  0.0544,  0.0740,  0.0825,
          0.0425, -0.0765, -0.0605,  0.0841,  0.0596, -0.0427,  0.0331,  0.0321,
         -0.0682, -0.0619,  0.0486, -0.0548,  0.0750,  0.0249,  0.0643, -0.0684,
          0.0805, -0.0777,  0.0428, -0.0461, -0.0760, -0.0618, -0.0505,  0.0784,
         -0.0829, -0.0448,  0.0024, -0.0130,  0.0829,  0.0203, -0.0443, -0.0505,
          0.0127,  0.0783,  0.0463,  0.0774, -0.0763,  0.0033,  0.0304, -0.0569,
         -0.0039,  0.0151,  0.0800,  0.0143,  0.0329,  0.0246,  0.0320, -0.0113,
          0.0741,  0.0591,  0.0569,  0.0202, -0.0487,  0.0686,  0.0640,  0.0467,
         -0.0159, -0.0755,  0.0308, -0.0411,  0.0813, -0.0519, -0.0376, -0.0861,
          0.0151,  0.0330, -0.0578, -0.0235, -0.0752,  0.0744,  0.0666,  0.0558,
          0.0502,  0.0603,  0.0623,  0.0583,  0.0551,  0.0102,  0.0591,  0.0729,
          0.0830,  0.0607, -0.0630, -0.0841, -0.0723,  0.0725,  0.0199,  0.0772,
         -0.0483,  0.0121, -0.0599, -0.0074,  0.0204, -0.0765, -0.0287,  0.0738]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[-0.0045,  0.0178, -0.0489,  ..., -0.0347,  0.0518, -0.0181],
        [ 0.0185, -0.0532, -0.0302,  ..., -0.0266,  0.0271,  0.0003]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.0295,  0.0049], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-0.1673, -0.2735, -0.0100, -0.3290, -0.0587, -0.2613,  0.0245, -0.2008,
         -0.2545, -0.1721,  0.2860,  0.1474, -0.1600,  0.1734,  0.1486, -0.0135,
         -0.1750,  0.1588, -0.1874,  0.3258, -0.1660,  0.2968, -0.2091, -0.0551,
          0.2918,  0.1112, -0.0991, -0.1827,  0.2250,  0.2161,  0.1208, -0.2865,
         -0.1253, -0.2098,  0.1708, -0.3342, -0.0995, -0.2287,  0.1909, -0.2913,
          0.1816, -0.2195, -0.0230, -0.0117, -0.0770,  0.1208,  0.0608,  0.2077,
          0.1634,  0.1006,  0.3231,  0.1924, -0.2624, -0.0309,  0.3224,  0.1094,
         -0.2099, -0.1604,  0.0228,  0.2118,  0.2268,  0.2153, -0.3021, -0.2271,
          0.0280,  0.0901, -0.2008, -0.1945,  0.0705, -0.1560,  0.1680, -0.0728,
          0.2135, -0.0448,  0.2309,  0.1730,  0.0487,  0.0726,  0.2304, -0.2861,
          0.0139,  0.0389, -0.1058, -0.2348,  0.2760,  0.1220,  0.2578,  0.2016,
          0.1230, -0.1492, -0.1225,  0.2849, -0.1183, -0.0447, -0.1696,  0.2003,
          0.2143,  0.2199, -0.1089,  0.2704,  0.0875,  0.1133,  0.2136, -0.1856,
         -0.1704, -0.2632,  0.2124, -0.0770, -0.2675, -0.3437, -0.0492, -0.0019,
         -0.4162,  0.1839, -0.1911,  0.1404, -0.1434,  0.0141, -0.0735, -0.2771,
         -0.1070,  0.1883, -0.0608,  0.2174,  0.1837, -0.0881,  0.1738, -0.1989],
        [-0.2279, -0.1625,  0.0447, -0.2116, -0.0424, -0.0803, -0.0389, -0.3733,
         -0.1924, -0.2077,  0.3095,  0.0683, -0.0801,  0.1363,  0.2843, -0.0208,
         -0.1229,  0.2528, -0.2101,  0.3646, -0.3544,  0.3704, -0.0338, -0.1297,
          0.3221,  0.1302, -0.1943, -0.2658,  0.1571,  0.1711,  0.1398, -0.3381,
         -0.1212, -0.0584,  0.1122, -0.1710, -0.0820, -0.0173,  0.0117, -0.2837,
          0.2011, -0.2674, -0.0826,  0.0571,  0.0504,  0.3385,  0.0712,  0.2560,
          0.1582,  0.1225,  0.1732,  0.0936, -0.3004,  0.0885,  0.1900, -0.0272,
         -0.1628, -0.1939,  0.0967,  0.1893,  0.1720,  0.1935, -0.2693, -0.2226,
          0.0473,  0.2303, -0.1627, -0.2241,  0.0157, -0.1177, -0.0024, -0.0832,
          0.2069, -0.0507,  0.0912,  0.1887,  0.0377,  0.1509,  0.2631, -0.3812,
          0.0542,  0.0200, -0.1398, -0.2266,  0.2712,  0.0849,  0.0679,  0.1432,
          0.1850,  0.0912, -0.1153,  0.2964, -0.0912, -0.0247, -0.1823,  0.1803,
          0.2812,  0.3098, -0.0384,  0.1579,  0.0483, -0.0346,  0.3383, -0.2359,
         -0.1219, -0.2933,  0.1773, -0.1888, -0.1045, -0.3197, -0.0942,  0.0895,
         -0.3344,  0.0892, -0.2935,  0.1680, -0.1424,  0.0101,  0.0720, -0.2353,
         -0.0901,  0.0161, -0.2028,  0.1728,  0.1013, -0.1091,  0.1545, -0.1256]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.6234, 0.5085], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.8907, 0.7506], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([-0.0708, -0.0214], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-3.7585e-01, -1.1549e-01, -3.0426e-01,  5.4765e-02, -5.3160e-02,
          1.8455e-01,  3.9915e-02, -1.5343e-01, -1.0213e-01, -9.7770e-02,
          1.4690e-01,  5.9169e-02, -5.9520e-02, -2.5781e-02, -3.7894e-02,
          1.3883e-01, -3.5061e-02, -2.7501e-01,  3.5169e-02, -6.6600e-02,
          1.4775e-01, -2.8566e-04, -1.3532e-01, -4.7032e-02,  3.3657e-02,
          1.3359e-01,  1.3088e-01, -5.5893e-02,  1.5985e-02, -8.3365e-02,
         -1.0326e-01,  1.2189e-01,  7.9627e-02, -1.2898e-02,  1.3215e-01,
          5.5213e-02,  2.1820e-02,  2.0189e-02,  1.5730e-01,  1.7683e-02,
          7.2959e-02,  4.8350e-02,  1.8922e-01,  1.6574e-01,  1.5674e-01,
         -2.7035e-01, -1.0864e-02,  5.9711e-02, -1.4671e-01,  8.8556e-02,
         -7.3187e-03, -8.0788e-03, -1.4223e-01,  1.0472e-01, -8.1149e-02,
         -1.2847e-01,  7.8997e-02, -4.3407e-02,  7.3357e-02, -3.0132e-02,
         -1.0154e-01,  7.6328e-02,  4.7337e-02, -1.2899e-01,  9.1388e-02,
          6.0746e-02, -2.1355e-01, -1.8493e-01, -1.9345e-02,  1.9987e-01,
         -5.1922e-02, -2.7029e-01,  1.5704e-01,  1.2366e-01, -7.5392e-02,
          1.4465e-01,  7.5272e-03, -3.3407e-02,  6.6237e-02,  1.9542e-02,
         -5.6157e-02,  3.7313e-02,  9.0425e-02, -2.8537e-01, -1.2619e-01,
          6.7752e-03,  3.8496e-02,  6.3226e-02,  3.3952e-02,  3.8588e-02,
         -4.7208e-02, -1.2716e-01,  2.0418e-01,  3.9537e-02, -2.2032e-02,
         -5.3922e-03,  1.8430e-02,  4.6750e-02,  1.3158e-02,  2.1181e-01,
          6.0480e-02, -1.8054e-01,  1.8594e-02,  1.1734e-01,  1.1793e-01,
          5.7193e-02,  4.7545e-02,  1.0629e-01, -4.8051e-02, -1.7953e-02,
          6.1431e-02,  2.1706e-02, -9.7299e-02, -3.0799e-01, -1.7178e-03,
          9.2618e-02, -1.1098e-01, -7.8371e-02, -2.3047e-01,  2.7160e-02,
          8.9566e-02, -2.2346e-02,  2.3114e-03, -2.1721e-01,  2.2965e-01,
         -6.9339e-02, -2.1634e-02,  4.8362e-02],
        [-3.8849e-02, -1.8848e-01, -2.5240e-02,  1.2866e-01, -1.4961e-01,
          9.6700e-02, -7.2320e-02,  1.1110e-01,  5.4485e-02,  5.1299e-02,
          3.4303e-02,  4.9554e-02,  8.4223e-02,  9.2917e-02, -7.9395e-02,
         -9.6609e-02,  3.5360e-02, -7.7963e-02,  1.7825e-01,  1.8589e-01,
         -6.8866e-02,  1.5213e-01,  1.0724e-01, -1.6819e-02, -1.0011e-01,
         -8.5412e-02, -6.8821e-02, -1.4938e-01,  8.3809e-02,  1.3803e-01,
         -1.0490e-01,  2.6276e-02,  3.8639e-02,  4.8808e-02,  5.5571e-02,
          7.3787e-02,  1.3247e-02,  8.3771e-02, -1.5658e-01, -1.1790e-01,
         -4.9804e-02,  1.8200e-01, -5.4348e-02,  9.6547e-03,  1.6953e-02,
          2.0002e-01,  2.4848e-01,  1.1390e-01, -1.5556e-03,  4.9128e-02,
          1.4398e-01, -2.9837e-02, -8.4754e-03, -3.1353e-02,  2.6896e-02,
          5.9959e-02,  1.2142e-02,  1.0435e-01, -1.1486e-01, -1.4411e-01,
          1.1567e-01,  1.1999e-01,  2.9924e-02, -2.7568e-01, -2.5963e-02,
         -1.3915e-01, -1.5665e-01, -1.2058e-01,  1.4425e-01,  2.1295e-01,
         -1.3264e-01, -2.5187e-01, -7.9477e-02, -4.6847e-02, -7.2125e-02,
          9.7475e-02, -1.3300e-01, -1.7309e-01, -5.1666e-02, -9.8786e-02,
          6.4131e-02, -6.6204e-02,  8.4340e-02,  9.1504e-02,  1.1665e-01,
          1.2038e-01, -4.2425e-02, -1.2430e-01,  5.9704e-02,  6.4545e-02,
         -3.5970e-02,  2.0109e-01, -1.3149e-01,  4.3989e-02, -5.3084e-03,
          1.7416e-02,  7.0187e-02, -6.0955e-02,  2.3189e-01,  2.6046e-01,
          1.4202e-01, -4.7438e-02, -5.4618e-02,  3.4634e-03,  1.1025e-01,
          8.8680e-02,  7.7255e-03,  4.0939e-02,  1.6580e-02, -3.4390e-02,
          3.3957e-02, -1.2476e-01, -1.6007e-01, -8.4380e-02, -5.7412e-02,
         -5.2680e-02, -5.3161e-02,  2.0442e-02, -1.6432e-02, -1.2722e-01,
         -1.5048e-01,  4.5005e-02, -4.9609e-02,  1.5298e-02, -2.0111e-03,
         -2.4383e-01,  1.1098e-01,  3.6212e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[ 0.1628, -0.1404,  0.1019,  0.0815, -0.0375,  0.1322,  0.0118, -0.1313,
          0.1741,  0.2192,  0.0381, -0.0131,  0.1120, -0.2072,  0.1819, -0.0314,
          0.0828,  0.1008, -0.1333, -0.1149,  0.1049,  0.2429,  0.1078, -0.0316,
         -0.0151,  0.0308,  0.1755,  0.0261,  0.1513, -0.0283, -0.1386, -0.0658,
         -0.0947, -0.1563,  0.0953,  0.1217, -0.0273, -0.0510,  0.0109,  0.0513,
         -0.0995, -0.1460, -0.1002,  0.1330,  0.0647, -0.1000,  0.0469, -0.1454,
          0.0383,  0.1475,  0.1671, -0.0040, -0.0486, -0.0345, -0.1367,  0.0476,
          0.0742,  0.0302, -0.0494,  0.1877,  0.0245, -0.1881, -0.0880,  0.0268,
         -0.0263, -0.1198, -0.0779,  0.0884,  0.1615,  0.0531, -0.0575, -0.0277,
          0.1646,  0.0371,  0.0647, -0.0859, -0.0098, -0.0425,  0.0356,  0.1558,
          0.1610, -0.0033,  0.1569,  0.0575, -0.1663, -0.0823, -0.0122,  0.0983,
         -0.0520,  0.0008, -0.1165, -0.0329, -0.1633, -0.0802,  0.2203, -0.0239,
         -0.0696, -0.0544, -0.0451,  0.0399,  0.0120, -0.1836, -0.1199,  0.0172,
          0.1202, -0.0553,  0.0249, -0.1195,  0.1279, -0.1233, -0.0631,  0.0994,
         -0.1528,  0.0898, -0.0993, -0.3793,  0.0080,  0.0553,  0.0158,  0.1368,
         -0.1624,  0.0854,  0.0331, -0.1178, -0.0713,  0.0174, -0.0024, -0.0882],
        [-0.1079,  0.0881,  0.2200, -0.0829,  0.1300, -0.1022, -0.1207,  0.0547,
         -0.0864,  0.1135,  0.0498, -0.2341, -0.0375,  0.2303,  0.1403, -0.1986,
         -0.0139,  0.0269, -0.0811, -0.0235,  0.0983, -0.0249,  0.1017, -0.2585,
          0.0744, -0.0123, -0.1497,  0.0923,  0.0709, -0.1063, -0.0377,  0.0841,
         -0.0999,  0.1128,  0.0502, -0.0597,  0.0050, -0.0738,  0.0690, -0.1266,
         -0.0080, -0.1401, -0.0618,  0.0954, -0.0280, -0.0570, -0.1701,  0.1144,
         -0.0916,  0.1383,  0.0533, -0.0695, -0.0304,  0.0895, -0.0923, -0.1815,
          0.0054,  0.0177,  0.0618,  0.0071,  0.1486,  0.1618,  0.0424,  0.1489,
         -0.0330, -0.0021,  0.1723,  0.0892, -0.1484,  0.0282,  0.1284,  0.0277,
          0.0894,  0.1701,  0.0172,  0.0995,  0.1329,  0.0569,  0.1558, -0.1172,
         -0.0615, -0.0119, -0.0335,  0.0661, -0.0035, -0.1602,  0.0865, -0.0396,
         -0.0121,  0.0368,  0.0738, -0.0043, -0.0102,  0.0600, -0.0679,  0.0595,
          0.2261,  0.1130, -0.0667, -0.1395, -0.0812,  0.1913,  0.0006,  0.1884,
          0.0784, -0.0847,  0.0476, -0.0739, -0.0624,  0.0835, -0.0177,  0.0084,
         -0.1639, -0.0693, -0.0197, -0.1007,  0.1455,  0.0010, -0.0413,  0.0629,
         -0.2134, -0.0577, -0.0710,  0.0483, -0.0261, -0.0476,  0.0778, -0.0568]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[-4.3953e-02,  6.5872e-03, -6.6074e-05,  ..., -2.3618e-02,
         -1.2090e-02, -7.8639e-04],
        [-1.5953e-01,  4.9356e-02, -6.1183e-03,  ..., -4.6772e-02,
         -4.1127e-03, -2.1868e-02]], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.0036, -0.0153], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-8.4676e-02,  2.8954e-02,  1.5459e-01, -4.6718e-02, -7.7847e-02,
          4.3023e-02, -6.1675e-02, -1.6883e-01, -1.5625e-01,  2.0040e-01,
          1.5004e-01,  1.4979e-01, -5.6771e-02,  2.2265e-01, -7.2673e-02,
          7.9121e-02, -3.0928e-01,  2.7867e-01,  2.8359e-01, -5.1057e-02,
          2.4155e-01, -3.2093e-01,  3.4949e-01,  9.1872e-02,  1.7383e-01,
         -2.2918e-02, -3.9567e-01, -1.3300e-01,  9.1805e-02,  1.1297e-01,
          3.6744e-01, -4.9941e-01, -1.2862e-02, -4.0276e-02, -2.0496e-01,
         -2.7933e-01, -2.4178e-01,  2.0268e-01,  4.4031e-02,  2.3207e-01,
          1.8316e-01,  3.7989e-01, -7.8542e-02, -2.4071e-01,  2.5325e-01,
          2.7895e-01,  2.1933e-01,  3.4533e-02,  6.5783e-02, -5.6761e-02,
          2.8971e-01,  1.9162e-01, -2.2535e-01,  3.2741e-01,  3.4254e-02,
          6.2725e-02, -3.5453e-01, -3.3651e-01, -3.3468e-02, -2.3074e-02,
         -5.7007e-02, -3.4199e-02, -4.2325e-01, -1.0979e-01,  9.4401e-02,
          1.1624e-01, -4.3531e-02, -2.7044e-01,  2.5849e-01,  5.8722e-01,
          9.8095e-02, -4.0544e-01,  1.5276e-01,  7.0493e-02,  1.6952e-01,
          4.7588e-03, -1.8433e-01, -1.5891e-02,  7.1718e-02, -1.6821e-01,
         -2.4660e-01,  2.2244e-01,  2.0458e-01, -2.3895e-01,  3.4619e-01,
         -1.2238e-01, -3.7004e-01,  1.5756e-01, -1.8308e-01, -9.7905e-02,
         -9.1646e-02,  1.1574e-01,  2.5590e-02,  6.4613e-02, -3.6648e-01,
         -2.8250e-01, -1.3493e-01,  2.4077e-01,  2.0328e-01, -9.0306e-02,
         -7.8204e-02,  2.2804e-02, -2.9157e-01,  6.4388e-02,  2.4046e-01,
         -1.4256e-01,  9.7028e-02, -5.1930e-02, -1.4200e-01,  9.8361e-02,
         -1.0241e-01,  3.2977e-01,  1.2840e-01, -8.1145e-02, -2.6567e-01,
          1.5367e-01,  1.5622e-01,  1.7633e-01, -7.2322e-02,  1.1189e-01,
         -1.5621e-01, -3.2290e-01, -4.9230e-02,  2.3478e-02, -5.7013e-02,
         -8.1916e-02, -6.5398e-02, -1.7791e-01],
        [-5.8436e-02, -2.0872e-02,  3.6147e-02,  5.9433e-02,  1.0125e-02,
         -3.5323e-02, -2.6826e-02, -1.7133e-01, -1.1162e-01,  7.3932e-02,
          5.0626e-02,  9.1442e-02, -1.4312e-01, -1.6027e-02, -1.2961e-02,
          1.9191e-01,  9.0661e-02,  1.0890e-01,  3.0744e-02,  1.4622e-01,
          2.4873e-01, -4.5509e-02,  1.6896e-02, -1.3174e-01,  1.5337e-01,
          7.6202e-02, -1.6051e-01, -6.9518e-02,  1.5119e-01,  7.8177e-02,
          3.8530e-02, -8.6643e-02,  3.3987e-02, -7.2792e-02, -7.3595e-02,
          4.8679e-03, -3.9007e-03, -9.1959e-02,  7.6848e-02, -5.6576e-02,
          2.1776e-01, -3.6033e-02,  3.8420e-02, -3.7943e-02, -9.2187e-02,
         -4.6396e-02,  2.2073e-02, -5.1275e-02, -1.0970e-01, -1.4476e-02,
          9.0216e-02,  4.7555e-02, -1.0690e-01, -2.1248e-03, -1.2081e-01,
         -7.2271e-02,  3.8117e-02, -1.1743e-01, -5.9105e-03, -3.8779e-04,
          2.0495e-03,  1.9726e-01, -1.0452e-01, -1.6841e-02,  1.8123e-01,
          1.0699e-01, -3.0098e-02,  2.7160e-02,  1.3614e-02,  5.8119e-02,
          8.8193e-02, -1.6869e-01, -1.7592e-01,  1.3237e-01, -2.9856e-02,
          1.1521e-01, -7.9838e-02,  4.9099e-02,  1.2821e-01, -2.4497e-02,
         -2.5203e-01, -5.9892e-02, -1.6435e-01, -2.3596e-01,  2.6084e-02,
         -6.6307e-02,  2.4940e-02,  1.1901e-01,  9.9350e-02, -4.4276e-02,
         -1.0036e-02, -3.5533e-02,  9.5022e-02,  9.5327e-02, -7.8994e-02,
          3.9903e-03,  1.4356e-01,  1.8291e-01, -1.0546e-01, -5.5641e-02,
          8.7561e-02, -6.1832e-02, -1.2932e-01, -1.2810e-01,  2.5768e-02,
         -7.0707e-02,  7.8855e-03,  6.8845e-03,  6.8331e-02,  2.2085e-02,
         -2.1410e-01,  1.9063e-01,  1.1544e-01,  1.6201e-01, -9.8083e-02,
          6.2491e-02, -1.5877e-02,  7.8713e-02, -1.1693e-01,  1.7985e-02,
         -7.8234e-02, -7.3148e-04,  4.0283e-02,  1.3685e-02,  1.1536e-01,
          1.9765e-01, -5.3646e-02, -1.1219e-01]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.8252, 0.8946], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.weight | Size: torch.Size([2, 128]) | Values : tensor([[ 0.0152, -0.0253, -0.0224,  0.0493, -0.0286, -0.0384, -0.0318, -0.0106,
          0.0193,  0.0402,  0.0453,  0.0124, -0.0159,  0.0513,  0.0616,  0.0393,
          0.0080,  0.0319, -0.0093, -0.0468, -0.0538, -0.0035,  0.0093,  0.0792,
          0.0773,  0.0030, -0.0156,  0.0208,  0.0729, -0.0068, -0.0307,  0.0604,
          0.0632, -0.0118, -0.0444, -0.0458, -0.0848,  0.0291, -0.0016, -0.0365,
          0.0735, -0.0733, -0.0384, -0.0709,  0.0348,  0.0470,  0.0386, -0.0613,
          0.0473, -0.0013, -0.0057, -0.0101, -0.0698,  0.0490, -0.0791, -0.0376,
         -0.0366,  0.0178,  0.0575, -0.0189, -0.0004,  0.0388, -0.0168,  0.0579,
          0.0215,  0.0126, -0.0563,  0.0062,  0.0268, -0.0045,  0.0066, -0.0207,
         -0.0463, -0.0068,  0.0255, -0.0851, -0.0184,  0.0195, -0.0242, -0.0174,
         -0.0151,  0.0253,  0.0165, -0.0097,  0.0546,  0.0226, -0.0413,  0.0186,
         -0.0426, -0.0517,  0.0159, -0.0389,  0.0342, -0.0630, -0.0097,  0.0012,
         -0.0179, -0.0007, -0.0296, -0.0472, -0.0045, -0.0056,  0.0359,  0.0271,
         -0.0712,  0.0127,  0.0354, -0.0260,  0.0086,  0.0225, -0.0245,  0.0163,
         -0.0167, -0.0248, -0.0405,  0.0153,  0.0648, -0.0597, -0.0233, -0.0165,
          0.0274, -0.0269, -0.0513,  0.0589,  0.0108,  0.0294,  0.0032,  0.0069],
        [ 0.0243, -0.0264, -0.0242,  0.0483, -0.0308, -0.0421, -0.0327,  0.0128,
          0.0352,  0.0422,  0.0443,  0.0023, -0.0257,  0.0525,  0.0582, -0.0129,
          0.0049,  0.0309, -0.0054, -0.0438, -0.0527, -0.0064,  0.0049,  0.0751,
          0.0755, -0.0082, -0.0136,  0.0257,  0.0657, -0.0040, -0.0541,  0.0615,
          0.0593, -0.0013, -0.0428, -0.0446, -0.0860,  0.0037, -0.0029, -0.0343,
          0.0477, -0.0715, -0.0303, -0.0696,  0.0384,  0.0487,  0.0417, -0.0489,
          0.0524, -0.0084, -0.0001, -0.0138, -0.0727,  0.0492, -0.0760, -0.0373,
         -0.0297,  0.0166,  0.0456, -0.0196, -0.0063,  0.0405, -0.0219,  0.0617,
         -0.0277, -0.0140, -0.0543,  0.0115,  0.0436, -0.0032,  0.0061, -0.0124,
         -0.0419, -0.0129,  0.0216, -0.0787, -0.0201,  0.0085, -0.0231, -0.0109,
          0.0021,  0.0217,  0.0112,  0.0032,  0.0552,  0.0223, -0.0444,  0.0163,
         -0.0354, -0.0353,  0.0188, -0.0467,  0.0054, -0.0643, -0.0054,  0.0024,
         -0.0155, -0.0183, -0.0214, -0.0480, -0.0029,  0.0092,  0.0323,  0.0262,
         -0.0690,  0.0076,  0.0449, -0.0262,  0.0159,  0.0211, -0.0190,  0.0090,
         -0.0146, -0.0229, -0.0385,  0.0166,  0.0608, -0.0627, -0.0165, -0.0218,
          0.0199, -0.0229, -0.0586,  0.0538,  0.0099,  0.0176,  0.0047,  0.0053]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.bias | Size: torch.Size([2]) | Values : tensor([-0.1197, -0.0159], device='cuda:0', grad_fn=<SliceBackward0>)
