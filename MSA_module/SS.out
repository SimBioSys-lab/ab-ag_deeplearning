{'batch_size': 16, 'sequence_file': 'preprocessed_seq_ab_train_1200.npz', 'ss_file': 'ss_train_data.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 1, 'num_classes': 8, 'num_epochs': 1000, 'learning_rate': 0.003, 'max_grad_norm': 0.5, 'validation_split': 0.1, 'early_stop_patience': 30}
Number of GPUs available: 4
Initializing SecondaryStructureModel with 1 repeated layers...
Using 4 GPUs with DataParallel.
Epoch [1/1000], Training Loss: 4.7940
Validation Loss: 1.4658
New best model found with validation loss: 1.4658
Epoch [2/1000], Training Loss: 1.4492
Validation Loss: 1.4478
New best model found with validation loss: 1.4478
Epoch [3/1000], Training Loss: 1.4461
Validation Loss: 1.4403
New best model found with validation loss: 1.4403
Epoch [4/1000], Training Loss: 1.4254
Validation Loss: 1.4189
New best model found with validation loss: 1.4189
Epoch [5/1000], Training Loss: 1.4084
Validation Loss: 1.4153
New best model found with validation loss: 1.4153
Epoch [6/1000], Training Loss: 1.3995
Validation Loss: 1.3993
New best model found with validation loss: 1.3993
Epoch [7/1000], Training Loss: 1.3909
Validation Loss: 1.3814
New best model found with validation loss: 1.3814
Epoch [8/1000], Training Loss: 1.3848
Validation Loss: 1.4268
No improvement in validation loss. Early stop counter: 1/30
Epoch [9/1000], Training Loss: 1.3800
Validation Loss: 1.3807
New best model found with validation loss: 1.3807
Epoch [10/1000], Training Loss: 1.3740
Validation Loss: 1.3715
New best model found with validation loss: 1.3715
Epoch [11/1000], Training Loss: 1.3612
Validation Loss: 1.3593
New best model found with validation loss: 1.3593
Epoch [12/1000], Training Loss: 1.3519
Validation Loss: 1.3558
New best model found with validation loss: 1.3558
Epoch [13/1000], Training Loss: 1.3499
Validation Loss: 1.3546
New best model found with validation loss: 1.3546
Epoch [14/1000], Training Loss: 1.3434
Validation Loss: 1.3697
No improvement in validation loss. Early stop counter: 1/30
Epoch [15/1000], Training Loss: 1.3399
Validation Loss: 1.3465
New best model found with validation loss: 1.3465
Epoch [16/1000], Training Loss: 1.3338
Validation Loss: 1.3429
New best model found with validation loss: 1.3429
Epoch [17/1000], Training Loss: 1.3285
Validation Loss: 1.3389
New best model found with validation loss: 1.3389
Epoch [18/1000], Training Loss: 1.3243
Validation Loss: 1.3406
No improvement in validation loss. Early stop counter: 1/30
Epoch [19/1000], Training Loss: 1.3168
Validation Loss: 1.3454
No improvement in validation loss. Early stop counter: 2/30
Epoch [20/1000], Training Loss: 1.3136
Validation Loss: 1.3416
No improvement in validation loss. Early stop counter: 3/30
Epoch [21/1000], Training Loss: 1.3041
Validation Loss: 1.3296
New best model found with validation loss: 1.3296
Epoch [22/1000], Training Loss: 1.2938
Validation Loss: 1.3414
No improvement in validation loss. Early stop counter: 1/30
Epoch [23/1000], Training Loss: 1.2909
Validation Loss: 1.3301
No improvement in validation loss. Early stop counter: 2/30
Epoch [24/1000], Training Loss: 1.2832
Validation Loss: 1.3310
No improvement in validation loss. Early stop counter: 3/30
Epoch [25/1000], Training Loss: 1.2789
Validation Loss: 1.3326
No improvement in validation loss. Early stop counter: 4/30
Epoch [26/1000], Training Loss: 1.2733
Validation Loss: 1.3315
No improvement in validation loss. Early stop counter: 5/30
Epoch [27/1000], Training Loss: 1.2683
Validation Loss: 1.3225
New best model found with validation loss: 1.3225
Epoch [28/1000], Training Loss: 1.2615
Validation Loss: 1.3360
No improvement in validation loss. Early stop counter: 1/30
Epoch [29/1000], Training Loss: 1.2569
Validation Loss: 1.3423
No improvement in validation loss. Early stop counter: 2/30
Epoch [30/1000], Training Loss: 1.2519
Validation Loss: 1.3353
No improvement in validation loss. Early stop counter: 3/30
Epoch [31/1000], Training Loss: 1.2340
Validation Loss: 1.3404
No improvement in validation loss. Early stop counter: 4/30
Epoch [32/1000], Training Loss: 1.2266
Validation Loss: 1.3449
No improvement in validation loss. Early stop counter: 5/30
Epoch [33/1000], Training Loss: 1.2216
Validation Loss: 1.3454
No improvement in validation loss. Early stop counter: 6/30
Epoch [34/1000], Training Loss: 1.2111
Validation Loss: 1.3463
No improvement in validation loss. Early stop counter: 7/30
Epoch [35/1000], Training Loss: 1.2058
Validation Loss: 1.3626
No improvement in validation loss. Early stop counter: 8/30
Epoch [36/1000], Training Loss: 1.1984
Validation Loss: 1.3615
No improvement in validation loss. Early stop counter: 9/30
Epoch [37/1000], Training Loss: 1.1896
Validation Loss: 1.3655
No improvement in validation loss. Early stop counter: 10/30
Epoch [38/1000], Training Loss: 1.1797
Validation Loss: 1.3760
No improvement in validation loss. Early stop counter: 11/30
Epoch [39/1000], Training Loss: 1.1735
Validation Loss: 1.3885
No improvement in validation loss. Early stop counter: 12/30
Epoch [40/1000], Training Loss: 1.1662
Validation Loss: 1.3818
No improvement in validation loss. Early stop counter: 13/30
Epoch [41/1000], Training Loss: 1.1478
Validation Loss: 1.3944
No improvement in validation loss. Early stop counter: 14/30
Epoch [42/1000], Training Loss: 1.1393
Validation Loss: 1.3997
No improvement in validation loss. Early stop counter: 15/30
Epoch [43/1000], Training Loss: 1.1303
Validation Loss: 1.4192
No improvement in validation loss. Early stop counter: 16/30
Epoch [44/1000], Training Loss: 1.1226
Validation Loss: 1.4157
No improvement in validation loss. Early stop counter: 17/30
Epoch [45/1000], Training Loss: 1.1144
Validation Loss: 1.4290
No improvement in validation loss. Early stop counter: 18/30
Epoch [46/1000], Training Loss: 1.1064
Validation Loss: 1.4323
No improvement in validation loss. Early stop counter: 19/30
Epoch [47/1000], Training Loss: 1.0980
Validation Loss: 1.4562
No improvement in validation loss. Early stop counter: 20/30
Epoch [48/1000], Training Loss: 1.0916
Validation Loss: 1.4542
No improvement in validation loss. Early stop counter: 21/30
Epoch [49/1000], Training Loss: 1.0827
Validation Loss: 1.4751
No improvement in validation loss. Early stop counter: 22/30
Epoch [50/1000], Training Loss: 1.0759
Validation Loss: 1.4836
No improvement in validation loss. Early stop counter: 23/30
Epoch [51/1000], Training Loss: 1.0557
Validation Loss: 1.5021
No improvement in validation loss. Early stop counter: 24/30
Epoch [52/1000], Training Loss: 1.0440
Validation Loss: 1.5128
No improvement in validation loss. Early stop counter: 25/30
Epoch [53/1000], Training Loss: 1.0376
Validation Loss: 1.5268
No improvement in validation loss. Early stop counter: 26/30
Epoch [54/1000], Training Loss: 1.0325
Validation Loss: 1.5314
No improvement in validation loss. Early stop counter: 27/30
Epoch [55/1000], Training Loss: 1.0244
Validation Loss: 1.5495
No improvement in validation loss. Early stop counter: 28/30
Epoch [56/1000], Training Loss: 1.0178
Validation Loss: 1.5531
No improvement in validation loss. Early stop counter: 29/30
Epoch [57/1000], Training Loss: 1.0079
Validation Loss: 1.5739
No improvement in validation loss. Early stop counter: 30/30
Early stopping triggered after 57 epochs.
Best model saved successfully.
Layer: module.embedding.weight | Size: torch.Size([22, 128]) | Values : tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 4.0241e-01, -1.6049e-01, -3.6464e-01, -2.3872e-01, -2.9907e-01,
         -6.6262e-02, -5.4812e-01, -1.7255e+00, -2.7705e-01, -8.6290e-01,
          8.2123e-01, -4.4162e-01, -3.5186e-01, -2.2151e+00, -1.4974e+00,
         -1.0509e+00, -9.0302e-01,  5.5117e-01,  1.0964e+00,  7.8007e-01,
          5.0205e-01,  2.0078e+00, -6.4045e-01,  3.8539e-02, -8.2753e-01,
         -1.8786e-01, -4.5587e-02,  4.7563e-02,  3.1579e-02, -5.1861e-02,
         -5.2508e-01, -5.1065e-01, -2.9010e-01, -2.5538e-01, -1.8630e+00,
          1.5548e+00,  1.3871e+00,  2.3723e-01, -3.1067e-01,  3.8688e-01,
          6.0962e-01,  1.9350e-01, -5.7961e-01, -2.8023e-01,  1.9390e-01,
          2.3677e-01, -3.0958e-01, -1.9101e+00, -1.1931e+00,  6.7024e-01,
         -2.1018e-01,  1.2130e+00, -1.6126e+00, -2.0635e-02,  1.3441e-02,
         -8.0351e-01,  9.4658e-01,  1.0405e+00, -2.1196e+00,  1.2594e+00,
          7.0778e-01,  8.0429e-01, -1.5603e+00,  7.1264e-01, -2.8891e+00,
         -1.3694e+00, -7.6592e-01,  1.4196e+00, -7.1575e-01, -7.5095e-01,
         -8.9268e-01,  6.2404e-01, -7.5898e-01, -8.1896e-01,  9.0360e-01,
          6.8408e-01, -2.5433e-01,  1.2455e+00,  4.2249e-01,  8.1796e-01,
         -8.6388e-01, -4.2872e-01, -7.4844e-01,  5.8596e-02,  1.2486e+00,
         -8.4071e-01,  6.3000e-01,  5.2690e-01,  2.1241e-02, -1.6274e+00,
         -5.7046e-01, -9.9947e-01,  6.1638e-01,  1.6253e+00,  1.3611e-03,
         -3.1182e-01, -4.6185e-01, -3.6936e-01, -1.7584e+00, -7.5307e-01,
          1.1832e+00,  1.2148e+00, -1.2592e+00,  1.2630e+00, -4.9455e-01,
         -1.4516e+00, -3.2129e-01, -7.6363e-01, -5.2676e-02,  1.9987e-01,
         -9.7586e-02,  2.0424e-01, -2.2329e-01, -4.4277e-01,  1.8667e+00,
          1.0988e+00,  2.8007e-01,  2.8039e+00, -1.7114e+00, -4.0328e-02,
         -9.4435e-01, -1.2510e+00,  1.5573e+00, -1.0755e+00,  3.2194e-03,
         -8.1486e-01,  1.0207e+00,  7.1039e-01]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.6244, 0.6805], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([ 0.0081, -0.0024], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 3.3070e-02, -4.6440e-03,  8.5611e-05,  3.4220e-02,  8.4069e-02,
         -2.6361e-02,  3.6902e-03,  7.5817e-02,  3.9855e-02, -2.6475e-02,
          1.3709e-02,  5.5555e-02, -8.0237e-02, -7.2120e-02,  6.1906e-02,
          3.5639e-02,  7.6933e-02,  3.4545e-02, -1.8279e-02, -6.3834e-02,
          4.5662e-02,  5.6430e-02, -7.7065e-02, -3.1078e-02, -2.6305e-02,
          8.7541e-02, -3.5801e-02, -7.6102e-02, -7.5431e-02, -8.6723e-02,
         -3.0166e-02, -1.1404e-02, -7.1052e-02, -1.2040e-03,  6.8979e-02,
          3.1744e-02,  5.8287e-03,  1.9870e-02,  2.8304e-02,  7.4965e-02,
         -7.9539e-02, -3.5722e-02,  2.8289e-02,  7.2334e-02,  5.2903e-02,
         -3.7803e-02, -4.7105e-02, -4.3333e-02, -1.4296e-02, -4.2254e-02,
         -5.8432e-02,  4.3527e-02, -3.0248e-02,  6.6633e-02,  3.0622e-02,
          4.9856e-02,  1.7167e-02, -5.2639e-02, -1.1793e-02, -5.6459e-02,
         -3.6121e-02, -4.9989e-02, -5.9381e-02,  4.8525e-02,  5.9207e-02,
          4.9246e-02,  4.2749e-02, -7.0427e-03,  7.7257e-02, -1.3418e-02,
         -9.5238e-03,  6.8890e-02, -7.8921e-02, -4.6207e-02,  6.7380e-02,
          7.1016e-02,  8.2353e-02,  3.7914e-02, -4.7607e-02,  3.1135e-02,
         -8.4498e-02,  3.4647e-02,  4.3001e-02,  7.7443e-02, -3.4247e-02,
         -7.4152e-02,  4.3223e-02,  4.6245e-02, -8.6555e-02, -7.0452e-02,
         -7.7973e-02,  3.6093e-02,  3.3006e-02,  6.0147e-02, -1.1992e-02,
          5.4339e-02, -4.6346e-02,  5.4178e-02, -4.6815e-02, -4.1736e-02,
          2.6967e-02, -2.8699e-02, -1.4706e-02,  6.8692e-02,  7.3470e-02,
         -2.9597e-02, -5.9624e-02,  1.3057e-02, -8.3809e-02, -4.6403e-02,
          6.7014e-02, -5.3173e-02,  1.2774e-02,  8.7120e-02, -1.9573e-02,
         -7.4023e-02,  2.5590e-02, -8.0984e-02, -3.7774e-02, -2.3182e-02,
          2.6483e-02, -8.7414e-02, -8.3109e-02, -2.4350e-02,  5.8602e-02,
         -8.2188e-02,  3.3773e-02,  2.7481e-02],
        [-6.9715e-02, -1.2396e-03, -7.2672e-02,  4.5386e-02, -6.9227e-02,
          8.7066e-02,  6.5662e-02,  2.4202e-02, -3.1143e-02,  2.2568e-02,
         -7.9418e-02, -1.5290e-04, -1.3647e-02,  6.0391e-02,  4.4467e-02,
         -1.7881e-02,  3.4320e-02,  5.4172e-02, -1.8111e-02, -4.5497e-02,
         -8.6794e-02,  2.8886e-02,  7.2615e-02,  2.5317e-02, -2.6440e-02,
          2.3184e-04,  2.8300e-02, -4.1159e-02, -6.3942e-02, -6.3693e-02,
         -7.2477e-02,  6.2376e-02, -5.2114e-03, -7.9671e-02,  2.9068e-02,
          6.0502e-02,  5.1661e-02,  7.7006e-02, -5.1907e-02, -1.8501e-02,
          8.6857e-02,  4.4576e-02,  5.2923e-02, -3.0286e-02, -4.3195e-02,
          6.9156e-02,  7.1391e-02,  4.1720e-02, -4.3868e-02, -5.2254e-02,
         -7.6560e-02,  5.7163e-02, -5.9446e-02, -2.9749e-02,  4.2862e-02,
         -6.8652e-02,  3.5241e-02,  7.6343e-02, -1.2702e-02,  3.3445e-02,
         -6.9462e-02,  3.6535e-02,  7.3535e-02,  4.2370e-02,  8.4578e-02,
          7.5266e-02, -1.5909e-02, -1.2384e-02, -3.8947e-02,  1.7525e-02,
         -7.3436e-02, -8.8192e-02,  2.7891e-02, -5.1390e-02, -3.1851e-02,
          7.0766e-02, -3.0189e-04,  6.2445e-02, -2.3548e-02,  7.4490e-03,
          7.2938e-02, -7.9681e-02, -8.8245e-02,  5.8536e-03,  6.5643e-02,
          8.2953e-02, -5.5753e-02, -5.5397e-02,  3.6165e-02, -1.0660e-02,
         -2.4847e-02, -8.9868e-03, -6.3561e-02, -5.7580e-02, -7.9547e-02,
          5.1589e-02,  5.7869e-02,  9.2253e-03,  4.7276e-03,  6.3207e-02,
          5.5282e-02, -5.5713e-02,  8.7332e-02,  3.3468e-02,  1.7888e-02,
          8.6822e-02, -1.2622e-03,  1.2506e-02, -1.9444e-02,  3.1089e-02,
          3.5725e-02,  2.1220e-02, -6.6706e-02, -6.6008e-04,  7.3262e-02,
          5.4619e-02, -1.9349e-02, -5.1426e-02, -7.9525e-02, -3.7581e-02,
          3.5201e-02,  6.1347e-02, -2.9163e-02,  4.6552e-02, -7.8654e-02,
          1.7458e-02,  4.4694e-02, -7.3085e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[ 0.0539, -0.0871,  0.0119, -0.0444, -0.0773,  0.0779, -0.0750,  0.0688,
         -0.0592, -0.0335, -0.0850,  0.0067, -0.0542, -0.0111,  0.0671, -0.0598,
         -0.0029,  0.0684,  0.0625, -0.0605,  0.0266,  0.0422,  0.0509,  0.0345,
         -0.0493,  0.0486, -0.0726,  0.0593,  0.0709,  0.0801,  0.0083,  0.0478,
          0.0518, -0.0447,  0.0043, -0.0628, -0.0618,  0.0216,  0.0304,  0.0099,
          0.0421, -0.0070,  0.0321, -0.0221,  0.0472, -0.0322, -0.0722,  0.0358,
          0.0174, -0.0726, -0.0342,  0.0435,  0.0279, -0.0083,  0.0125,  0.0060,
         -0.0432,  0.0428, -0.0594,  0.0084,  0.0644,  0.0584, -0.0832,  0.0093,
          0.0539, -0.0601,  0.0170,  0.0354,  0.0851, -0.0308, -0.0555, -0.0805,
         -0.0488, -0.0273,  0.0596, -0.0142,  0.0244, -0.0607, -0.0365,  0.0210,
          0.0783, -0.0321, -0.0522,  0.0822, -0.0118,  0.0013, -0.0536, -0.0801,
         -0.0076,  0.0324,  0.0266,  0.0364, -0.0036, -0.0149, -0.0251,  0.0159,
         -0.0848, -0.0173, -0.0283,  0.0406,  0.0082, -0.0373, -0.0454, -0.0335,
         -0.0553,  0.0469, -0.0738, -0.0348,  0.0248,  0.0091,  0.0086,  0.0569,
         -0.0520,  0.0002,  0.0717,  0.0598,  0.0789,  0.0217,  0.0499,  0.0750,
         -0.0217, -0.0746,  0.0215, -0.0264, -0.0431, -0.0239, -0.0486, -0.0640],
        [-0.0634,  0.0816, -0.0039, -0.0387,  0.0392, -0.0169, -0.0576,  0.0478,
         -0.0185, -0.0375,  0.0246, -0.0074,  0.0048, -0.0033, -0.0732, -0.0494,
          0.0795, -0.0684,  0.0318, -0.0515, -0.0105,  0.0120,  0.0231,  0.0169,
          0.0448, -0.0185,  0.0309, -0.0348,  0.0796,  0.0008, -0.0618,  0.0874,
          0.0817,  0.0506,  0.0191, -0.0322, -0.0091,  0.0827, -0.0550, -0.0761,
          0.0576, -0.0815,  0.0683,  0.0163, -0.0047, -0.0590, -0.0482,  0.0283,
          0.0784, -0.0298,  0.0776,  0.0070, -0.0020, -0.0525,  0.0077,  0.0095,
         -0.0186, -0.0665, -0.0508,  0.0200, -0.0085,  0.0834,  0.0665,  0.0372,
          0.0057, -0.0360, -0.0194, -0.0375,  0.0425,  0.0436,  0.0131, -0.0694,
         -0.0418,  0.0502, -0.0320, -0.0718,  0.0490, -0.0433,  0.0870, -0.0121,
         -0.0879, -0.0821,  0.0279, -0.0799, -0.0713,  0.0658, -0.0737,  0.0722,
         -0.0266, -0.0265, -0.0543, -0.0505, -0.0791, -0.0775,  0.0296,  0.0711,
          0.0667,  0.0360, -0.0494, -0.0174, -0.0030,  0.0165, -0.0692, -0.0191,
          0.0570, -0.0259,  0.0458,  0.0664,  0.0066,  0.0759,  0.0316,  0.0681,
          0.0268, -0.0506,  0.0145,  0.0371, -0.0103, -0.0871,  0.0818, -0.0473,
         -0.0341, -0.0860,  0.0146,  0.0536, -0.0404, -0.0089,  0.0451,  0.0446]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[-0.1500, -0.0300,  0.0426,  ..., -0.0145,  0.0292, -0.0128],
        [-0.0204, -0.0090, -0.0169,  ..., -0.0006, -0.0157, -0.0285]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.4192, -0.0594], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-4.7071e-03,  1.0271e-01,  2.8267e-01,  2.2773e-01,  3.6595e-01,
         -4.2207e-01, -8.6866e-02, -1.4308e-01, -3.0704e-01,  1.2905e-01,
         -2.1370e-01, -3.8339e-01,  9.4869e-03,  8.3426e-02, -2.3077e-01,
          4.0952e-01, -4.1607e-01, -2.7867e-01,  3.9097e-01, -2.1565e-01,
          2.3208e-01,  4.5391e-01,  3.0427e-01,  5.8923e-02,  4.5377e-01,
         -1.5006e-01, -1.7583e-01, -4.2396e-01,  7.8802e-02, -1.2418e-02,
         -2.2267e-02,  2.5464e-02,  1.3037e-01,  1.8839e-01,  6.6718e-02,
          7.6358e-02, -4.5654e-02, -5.4082e-01,  4.0210e-02,  1.4874e-01,
          2.6657e-01, -4.1091e-01, -4.5177e-02,  5.1736e-01, -2.8404e-01,
          1.4397e-01, -5.8074e-02,  2.4688e-01,  5.1494e-01, -1.6068e-01,
         -2.4285e-01, -3.3348e-01, -2.5546e-01,  5.4682e-03,  1.7568e-01,
          2.6993e-01, -6.1176e-01,  1.9498e-01,  3.0300e-01,  1.9285e-01,
         -4.8247e-01, -4.9922e-01, -4.3609e-01, -1.5368e-02, -1.6269e-01,
          1.3441e-01,  1.9234e-01, -3.1647e-01,  6.4442e-01, -2.9071e-01,
          1.3654e-01,  2.2715e-01,  2.9182e-01, -1.6015e-01,  6.2199e-02,
         -1.1119e-01, -2.2196e-01, -2.5171e-01,  3.6841e-01,  2.2683e-01,
          2.7214e-01,  4.8238e-02, -1.9847e-01, -2.2639e-01, -2.5037e-01,
          6.4416e-01, -2.0167e-01,  3.6626e-01, -4.1004e-01,  1.5938e-01,
         -7.1504e-04, -2.4554e-01,  3.0801e-01, -2.2022e-01,  4.0827e-01,
         -1.8744e-01,  1.0569e-01,  2.2610e-01,  4.0125e-01,  1.0004e-01,
         -2.3599e-01,  6.9093e-02,  3.1190e-01,  3.6879e-02,  2.0924e-01,
          5.3630e-01, -4.5915e-01, -6.6926e-02,  3.1984e-01,  7.2528e-01,
         -1.5063e-01, -6.2406e-01,  4.6611e-02, -1.9066e-01, -4.4435e-01,
          3.3034e-01, -1.4015e-01, -3.5015e-01,  2.4286e-03,  2.7891e-01,
         -2.5274e-01, -8.1959e-02, -1.1164e-02, -3.1010e-02,  1.0976e-01,
         -9.3126e-04,  2.3870e-01,  2.2064e-01],
        [ 2.4079e-02,  3.4957e-02,  2.0670e-01,  1.7901e-01,  3.2372e-01,
         -3.3666e-01,  2.9704e-02, -1.5567e-01, -2.6702e-01,  7.8039e-03,
         -2.2235e-01, -2.9893e-01, -4.7789e-02, -1.3509e-02, -2.0639e-01,
          4.7696e-01, -3.1676e-01, -2.6852e-01,  4.4341e-01, -2.0789e-01,
          1.6291e-01,  2.8106e-01,  1.3584e-01,  1.0646e-01,  4.5260e-01,
         -2.1847e-01, -1.4377e-01, -3.5777e-01, -4.6699e-02, -5.1094e-02,
          4.9780e-02, -1.3423e-01,  8.5088e-02,  3.5147e-01,  7.0912e-02,
          2.3158e-01,  1.5741e-02, -4.5209e-01, -6.3540e-02,  1.5765e-01,
          2.8520e-01, -4.4201e-01, -5.8231e-02,  5.2841e-01, -3.3686e-01,
          9.9168e-02, -1.6757e-01,  1.5476e-01,  3.6484e-01, -2.5704e-02,
         -7.0541e-02, -3.9631e-01, -3.2462e-01,  2.1194e-02,  7.9829e-02,
          5.9972e-02, -4.1291e-01,  1.3806e-01,  2.2008e-01,  1.8646e-01,
         -3.4312e-01, -4.4599e-01, -2.4308e-01,  6.0113e-02, -7.4408e-02,
          2.1973e-01,  1.3686e-01, -2.7792e-01,  3.7244e-01, -2.4428e-01,
          1.6694e-01,  1.3106e-01,  1.6404e-01, -1.1221e-01,  7.6876e-02,
         -3.4784e-02, -1.3668e-01, -2.0495e-01,  2.2146e-01,  9.8065e-02,
          1.9205e-01,  9.3204e-02, -2.2766e-01, -7.6956e-02, -1.5494e-01,
          3.6924e-01, -2.0593e-01,  2.9696e-01, -4.1211e-01,  2.1718e-01,
          1.4002e-01, -1.9448e-01,  8.3330e-02, -1.5596e-01,  3.0264e-01,
         -1.4819e-01,  2.1980e-01,  1.3789e-01,  3.1432e-01,  3.2474e-02,
         -2.0729e-01,  2.1510e-01,  3.3010e-01,  8.1521e-02,  6.5305e-02,
          3.3628e-01, -2.9000e-01,  4.1674e-02,  2.2066e-01,  5.7659e-01,
         -2.0672e-01, -4.9049e-01,  1.2450e-01, -2.4329e-01, -3.6981e-01,
          3.8033e-01, -1.5747e-01, -2.3966e-01, -1.0927e-01,  1.8024e-01,
         -1.2906e-01, -1.5301e-01, -1.4797e-02, -9.2704e-02,  7.0990e-02,
          1.6145e-02,  1.0633e-01,  1.5998e-01]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([-0.6116, -0.4544], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.5133, 0.8575], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([ 0.0166, -0.1324], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-2.1855e-01, -1.2243e-01,  3.0232e-02, -8.1446e-03, -1.0651e-01,
          1.8164e-01,  2.5609e-03,  1.3398e-01, -5.0850e-02, -5.4044e-02,
          4.5546e-02,  1.6525e-01, -1.5563e-02,  4.4792e-02,  5.5972e-02,
          1.2362e-01, -4.0136e-02, -1.0875e-01, -9.3732e-02,  2.0801e-01,
          1.7535e-01,  1.5731e-01, -6.3783e-02,  5.3235e-02, -1.4826e-01,
         -4.6104e-02,  3.5980e-02, -2.1700e-01,  3.7044e-03, -5.9049e-02,
         -1.6420e-01,  1.4045e-01,  5.2121e-02,  5.7786e-02,  1.0458e-01,
          2.7233e-02, -6.7352e-02,  1.1940e-01, -4.7180e-02, -1.1069e-01,
          1.8388e-02, -4.6236e-03, -7.0302e-02,  5.6004e-02,  1.0318e-01,
         -1.2953e-01,  2.9310e-01, -1.4343e-01,  8.9489e-02, -1.2629e-01,
         -2.3839e-01, -1.0627e-01, -5.8747e-02,  5.5585e-03, -7.5068e-02,
         -1.0688e-01, -7.6477e-02,  5.1225e-02,  1.2371e-02, -7.7062e-02,
         -1.3702e-01, -1.9010e-01, -7.4820e-02, -2.3144e-02,  1.6510e-02,
         -1.3670e-01,  3.4554e-02,  1.7049e-01, -1.1240e-01,  4.8065e-02,
         -5.5452e-02, -1.5827e-01, -4.6623e-02, -1.6411e-01,  6.4603e-02,
         -4.9118e-02,  4.2736e-03, -3.9667e-02, -1.9041e-01,  6.3567e-02,
         -8.3695e-02,  9.5761e-02, -1.2861e-01,  5.7561e-02,  4.5865e-02,
         -1.7177e-02, -5.1617e-03,  9.7478e-02,  9.4224e-02, -3.5030e-02,
         -9.3901e-02, -7.5576e-02,  7.7231e-02,  4.4309e-02,  3.0961e-02,
         -2.5819e-01,  9.7852e-02,  6.1609e-02, -7.7413e-02,  2.1869e-02,
          1.5692e-02,  1.7400e-01,  6.5088e-02, -3.9785e-02,  1.5201e-01,
         -1.1119e-01,  1.4664e-01,  6.4603e-02, -1.2623e-02, -1.0653e-01,
          1.4202e-01,  4.5459e-02,  1.0877e-01,  1.2372e-01, -5.5142e-02,
          1.2661e-01, -1.4810e-03,  7.4459e-02, -2.6079e-02,  1.0865e-01,
         -9.3618e-03,  2.9761e-02, -3.4726e-02, -1.4204e-01,  7.8698e-02,
         -5.7467e-02, -1.2335e-01,  1.0706e-01],
        [ 3.5154e-02,  1.8959e-01,  7.5460e-02, -1.0915e-01, -5.8943e-02,
         -1.6787e-01,  4.3271e-02, -9.4190e-02,  3.5905e-02, -4.8565e-02,
         -1.3385e-01,  8.2443e-02, -8.6634e-02,  2.0668e-01,  1.9631e-01,
         -4.7208e-02,  1.9883e-01, -1.8898e-03, -2.2803e-01, -2.3143e-01,
         -1.1076e-01, -5.8392e-03,  2.8719e-01,  8.1449e-03, -1.0028e-01,
          2.0848e-02, -1.2534e-01, -1.1050e-01,  2.1128e-01,  2.5159e-01,
         -1.9866e-01,  2.5012e-01,  1.6367e-01, -3.1625e-02, -5.2637e-02,
         -5.3720e-02,  1.4578e-02, -6.6977e-02, -4.5588e-02,  8.5094e-02,
          6.3162e-02,  8.1008e-02, -4.7719e-02, -1.3748e-01,  8.0726e-02,
         -1.7570e-01,  2.1715e-01, -5.5260e-02,  2.5506e-01, -2.4835e-01,
         -1.0510e-01,  7.3302e-02,  3.4277e-02, -2.3740e-01, -4.2059e-02,
         -1.9241e-02, -8.7036e-02, -5.4422e-02,  7.0904e-02, -1.2546e-01,
          7.9294e-02, -5.0049e-02, -1.7115e-02, -7.5587e-02,  8.8008e-03,
          7.9165e-02, -1.9269e-01,  2.8280e-01,  2.4106e-02,  8.6380e-02,
         -1.5703e-01,  1.9448e-03, -7.3950e-03, -1.4498e-01, -5.2155e-02,
          9.8651e-02, -1.0528e-01,  2.3696e-02, -1.3778e-01,  1.4557e-01,
          1.1487e-01, -1.0365e-01,  1.2480e-01, -1.2398e-01, -1.0264e-01,
         -4.5064e-02, -4.2678e-02,  1.9131e-01, -8.5848e-02, -4.3721e-02,
          1.2545e-01, -2.4786e-01, -1.3067e-01, -4.6148e-02, -1.1917e-01,
          1.5209e-01, -1.7634e-02,  9.9066e-02,  8.0436e-02, -1.8903e-02,
         -1.1860e-01, -1.1118e-01, -4.4035e-03,  1.6233e-02,  9.4739e-02,
         -1.2634e-01,  6.8106e-02,  2.6623e-02,  1.5054e-01, -2.2848e-03,
          1.3600e-01, -5.8692e-02,  4.1749e-02,  9.7868e-02, -1.8404e-01,
          1.8142e-01, -9.6754e-02,  2.4953e-04,  1.5243e-01,  7.2740e-02,
          4.0118e-02, -4.6071e-02,  1.6137e-01,  1.2279e-01,  3.2016e-02,
         -2.5671e-02, -1.2142e-01, -4.1041e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[ 0.0027, -0.0456, -0.0879,  0.0267, -0.0381,  0.0359, -0.0504,  0.0949,
          0.0979, -0.0997,  0.0452, -0.1278,  0.1082, -0.1565,  0.0097,  0.1768,
          0.0072, -0.0637, -0.1799, -0.0425,  0.1230,  0.0023,  0.1377,  0.1898,
          0.1235, -0.0428,  0.1450, -0.0530, -0.0158, -0.1443,  0.1697,  0.0211,
         -0.1201,  0.0384,  0.0872,  0.2394, -0.0402, -0.0405,  0.0419, -0.1226,
          0.1102,  0.0741, -0.1484, -0.0275,  0.0862,  0.0540, -0.0781,  0.2219,
         -0.1069, -0.0506,  0.1927,  0.0767,  0.2351, -0.0145,  0.0478, -0.0305,
          0.2223, -0.0729,  0.1297,  0.1167, -0.0323, -0.1837,  0.0630,  0.0703,
         -0.1234,  0.0083, -0.0178,  0.0266,  0.0322,  0.0139,  0.1555, -0.0930,
         -0.0554,  0.1181,  0.0663, -0.0324, -0.0119, -0.1044, -0.0998, -0.1254,
          0.1670,  0.2036,  0.0779, -0.1582,  0.0901,  0.2591, -0.0742,  0.0708,
          0.0814,  0.2518, -0.0477, -0.0175,  0.0739, -0.1420, -0.1920,  0.0500,
          0.1208,  0.0293,  0.0557, -0.1377, -0.0673,  0.1228, -0.0955, -0.0280,
          0.0033,  0.0789, -0.0688, -0.1435, -0.0888, -0.0990, -0.0389,  0.3570,
         -0.0384, -0.0877, -0.1298,  0.0259, -0.0265, -0.1208, -0.0176,  0.1913,
         -0.2384,  0.1079, -0.0298,  0.1292,  0.1054,  0.0948, -0.0696,  0.2687],
        [-0.0707, -0.0582,  0.1115,  0.0989,  0.0851, -0.0558, -0.1847,  0.2363,
          0.0751, -0.0957,  0.1014,  0.0700,  0.0334,  0.2009, -0.0156,  0.0924,
         -0.1044,  0.0037, -0.2714, -0.1346, -0.0346, -0.0332,  0.1530, -0.0387,
          0.2737, -0.2109,  0.1594, -0.0246, -0.0380, -0.0864,  0.0274,  0.0830,
          0.0042, -0.1188, -0.0040,  0.0020, -0.0239,  0.0700,  0.0213, -0.0134,
         -0.0069, -0.1418, -0.0482,  0.1359, -0.1349,  0.0220,  0.0973, -0.1450,
          0.1116, -0.0755,  0.0077,  0.1384, -0.0917,  0.2022,  0.0536, -0.2705,
          0.1382, -0.0907,  0.0232,  0.2989, -0.0722,  0.0077,  0.0130,  0.0632,
         -0.0456, -0.1234,  0.0640,  0.0659,  0.0087,  0.0112, -0.2304, -0.1891,
          0.1066, -0.0723,  0.0108, -0.1292,  0.1702, -0.3538,  0.1606, -0.0004,
          0.0514, -0.0206, -0.0904, -0.0596, -0.0431, -0.3923,  0.1306,  0.1011,
          0.1543, -0.1618, -0.1805,  0.1617,  0.0523, -0.3531,  0.0231, -0.0681,
          0.2925,  0.0259,  0.0546, -0.0706,  0.1215,  0.0175,  0.0540, -0.1062,
          0.1158, -0.1378,  0.0812, -0.1470, -0.0618,  0.1065,  0.0867,  0.0074,
         -0.2016,  0.1841, -0.0887,  0.1143,  0.0994,  0.0632, -0.0835,  0.2362,
         -0.4038, -0.1466, -0.2225, -0.0102,  0.1185, -0.2704,  0.2360,  0.0489]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.1094,  0.0936, -0.2202,  ..., -0.1258,  0.3677,  0.1349],
        [ 0.1042,  0.0376,  0.0578,  ...,  0.1376, -0.0859,  0.0395]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([0.0454, 0.0222], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-3.6606e-02, -9.2391e-02, -4.4911e-02, -2.1202e-02,  1.2720e-01,
          2.4261e-01, -2.3646e-02, -3.3049e-02,  6.5518e-01,  1.4347e-02,
         -3.2793e-02,  3.4554e-01,  7.5428e-02, -1.3859e-01,  1.1779e-01,
          1.7769e-01,  1.1327e-01,  2.6931e-02, -8.3694e-02,  2.0366e-02,
          4.6206e-01, -3.8873e-02,  2.8190e-01, -7.3545e-02, -1.5998e-01,
          6.7759e-02, -8.1742e-02, -1.0568e-01, -5.0322e-01, -3.8022e-01,
         -2.9079e-01, -2.2724e-02, -7.7926e-02,  3.4489e-02,  2.6850e-01,
          2.7201e-01,  1.4271e-01, -3.3666e-02, -2.9886e-03, -8.1532e-03,
          9.2674e-02,  2.9922e-01,  6.1481e-02, -2.4400e-01,  3.2891e-01,
          3.0599e-01,  8.9025e-02, -1.2941e-01,  2.7209e-03, -1.5117e-01,
         -4.4693e-02, -1.2859e-01, -1.5296e-01,  1.5369e-01, -3.8377e-01,
          1.0800e-01, -1.0296e-01,  1.9482e-01, -1.0501e-02,  2.3912e-01,
         -1.0735e-01,  3.0466e-02,  3.4472e-02,  1.2898e-01, -1.5837e-02,
         -2.0313e-01, -2.3309e-01, -3.4351e-02, -1.5707e-01,  2.6495e-01,
          5.4139e-02,  2.9833e-01, -3.6431e-01,  1.6088e-01,  2.8528e-02,
         -2.8934e-01, -5.2045e-03,  2.7366e-01,  7.3504e-02,  4.2970e-01,
         -5.4768e-02,  1.6616e-02, -1.3901e-01, -1.3893e-01, -2.2941e-01,
         -1.0571e-01, -1.6831e-01,  2.3631e-01,  3.7662e-01, -1.3389e-02,
         -2.7221e-02, -1.5089e-01, -6.4706e-01,  1.8143e-01, -1.2552e-01,
         -2.4250e-01,  2.3333e-01,  3.9110e-01,  2.5135e-01, -1.3964e-01,
          1.9132e-02,  8.6029e-03,  9.0671e-02,  2.2163e-01, -7.1661e-02,
          6.2568e-02, -3.4711e-01,  1.0081e-01,  2.6005e-02, -5.7754e-02,
          2.9272e-01,  4.2078e-02, -2.5656e-01, -9.2396e-02,  1.9272e-02,
          5.0170e-05,  2.5299e-01,  9.4588e-02, -2.3709e-01, -1.3040e-01,
          1.9597e-01, -2.0661e-01, -5.5441e-01, -1.7033e-01,  3.5992e-02,
         -2.0653e-01, -2.2210e-01, -1.2374e-01],
        [-9.2761e-02, -3.7130e-02,  1.6101e-01, -1.6519e-01,  4.5134e-04,
          9.1120e-03, -9.9612e-02,  1.6798e-02, -1.5544e-01,  6.8850e-02,
          5.0275e-02, -1.1522e-01,  1.1164e-01,  1.6337e-02, -9.2161e-02,
          7.1810e-02,  4.1980e-02,  8.9180e-03,  2.7751e-02,  2.7661e-01,
         -1.7755e-01, -3.5033e-02,  9.0074e-02, -7.4121e-02,  1.3362e-01,
         -1.0654e-01,  1.6679e-01,  1.7168e-02,  1.0795e-01,  9.5949e-03,
         -1.1783e-01,  1.0204e-01,  1.5029e-01, -2.2772e-01,  2.8880e-01,
         -5.3444e-02,  1.5684e-02, -5.2473e-02,  1.1445e-01,  1.7164e-02,
          1.5048e-01, -8.6085e-02,  2.1643e-01,  1.4104e-01, -1.4171e-01,
          5.0624e-02,  1.4176e-01, -1.2906e-01, -2.9265e-02, -1.5198e-01,
          6.3232e-02, -1.9214e-01, -3.2118e-01,  8.3859e-02, -5.4350e-02,
          6.9244e-02, -8.5985e-02,  1.1310e-01, -2.3391e-01, -1.1877e-01,
         -6.9890e-02, -1.6863e-01,  5.4732e-02, -2.6370e-01,  2.8649e-02,
          3.1531e-02, -1.2561e-01,  8.6012e-02, -6.7409e-02,  2.0384e-01,
         -2.3012e-02, -7.1424e-02,  1.0973e-02, -8.6175e-02, -1.2851e-01,
          5.6543e-02,  2.0509e-01, -3.0369e-01,  1.6749e-02, -3.6105e-02,
         -8.4339e-02, -1.2231e-01,  2.5240e-01,  3.7934e-03, -1.8758e-01,
         -1.9032e-01, -1.1665e-01, -2.7340e-02, -1.4138e-01, -6.0732e-02,
          1.0934e-01, -2.9244e-02, -3.0184e-02, -1.2817e-01,  6.3306e-02,
          1.0970e-01, -2.9325e-01,  9.5499e-02,  1.1027e-01,  4.6063e-02,
         -4.9697e-02, -1.1998e-02,  3.9955e-01, -2.2733e-02,  7.0945e-02,
          1.6022e-01,  1.6150e-01,  1.3009e-01,  1.7431e-01, -8.4583e-02,
          4.0466e-01, -1.1069e-01,  2.8345e-01,  1.7045e-01,  1.5695e-01,
         -1.4992e-01, -1.3510e-01,  6.6416e-02, -3.7562e-02,  4.2422e-02,
         -9.7064e-02, -7.8836e-02,  6.9776e-02,  9.7232e-02,  4.2634e-02,
          1.3925e-01, -2.0694e-01, -1.1968e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.9144, 0.8487], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.weight | Size: torch.Size([8, 128]) | Values : tensor([[-0.0279,  0.0205,  0.0140, -0.0124,  0.0085, -0.0155, -0.0024,  0.0588,
          0.0485,  0.0502, -0.0576, -0.0016, -0.0096, -0.0304,  0.0473,  0.0579,
         -0.0185, -0.0197, -0.0076, -0.0389,  0.0402, -0.0159,  0.0023,  0.0121,
         -0.0544, -0.0102, -0.0087,  0.0173,  0.0204, -0.0151,  0.0484,  0.0094,
         -0.0340, -0.0208, -0.0504, -0.0031,  0.0374,  0.0385, -0.0118,  0.0248,
          0.0268,  0.0373, -0.0275, -0.0656, -0.0125, -0.0255, -0.0183,  0.0448,
         -0.0092, -0.0094,  0.0135, -0.0193,  0.0369,  0.0090,  0.0188,  0.0016,
          0.0306, -0.0242,  0.0131,  0.0065, -0.0299,  0.0212, -0.0008,  0.0613,
         -0.0246,  0.0449, -0.0107, -0.0477, -0.0065,  0.0155,  0.0370,  0.0198,
          0.0130,  0.0024,  0.0339, -0.0012, -0.0071,  0.0976,  0.0302,  0.0109,
         -0.0181, -0.0285, -0.0599,  0.0132,  0.0266,  0.0557, -0.0297, -0.0002,
         -0.0100,  0.0112,  0.0237,  0.0102,  0.0480, -0.0037, -0.0248,  0.0025,
          0.0374, -0.0496, -0.0290,  0.0258, -0.0143, -0.0366, -0.0591, -0.0233,
          0.0260, -0.0143, -0.0678, -0.0021,  0.0400,  0.0137,  0.0365, -0.0055,
         -0.1030, -0.0414, -0.0293, -0.0086,  0.0533,  0.0014,  0.0014, -0.0340,
          0.0053,  0.0273,  0.0441,  0.0423,  0.0523,  0.0283, -0.0156,  0.0307],
        [ 0.0070,  0.0627,  0.0004, -0.0111,  0.0002, -0.0888,  0.0030,  0.0227,
         -0.1035,  0.0206, -0.0562, -0.0481, -0.0014,  0.0331,  0.0341, -0.0040,
         -0.0430, -0.0313,  0.0231, -0.0580, -0.0183, -0.0164,  0.0199,  0.0299,
          0.1470, -0.0195, -0.0451,  0.0515,  0.0019,  0.0062, -0.0225, -0.0257,
          0.0598, -0.0452, -0.0943,  0.0354,  0.0116,  0.0372, -0.0236, -0.0206,
         -0.0245, -0.0255, -0.0134,  0.0527, -0.0093, -0.0615, -0.0075,  0.0345,
          0.0593,  0.0114, -0.0057,  0.0201, -0.0915,  0.0267,  0.0473, -0.0325,
          0.0418, -0.0045, -0.0152, -0.0006, -0.0344, -0.0839, -0.0771,  0.0583,
         -0.0295,  0.1168,  0.0584, -0.0484, -0.0060, -0.0853,  0.0093,  0.0478,
          0.0567, -0.0129,  0.0398,  0.0279, -0.0375, -0.0266, -0.0036,  0.0172,
          0.0151, -0.0102, -0.0459,  0.0685,  0.0013,  0.0506, -0.0041, -0.0174,
         -0.0495, -0.0081,  0.0518,  0.0597,  0.0586,  0.0319,  0.0580,  0.0093,
         -0.0049, -0.1215, -0.0368, -0.0028, -0.0014, -0.0576,  0.0506, -0.0171,
          0.0083,  0.0208, -0.0151, -0.0499,  0.1240,  0.0393,  0.0524,  0.0429,
          0.1442, -0.0206, -0.0687,  0.0410,  0.0622, -0.0146, -0.0272, -0.0505,
         -0.0443,  0.1346,  0.0445,  0.0264,  0.0505, -0.0356,  0.0427, -0.0113]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.bias | Size: torch.Size([8]) | Values : tensor([ 0.1062, -0.2746], device='cuda:0', grad_fn=<SliceBackward0>)
