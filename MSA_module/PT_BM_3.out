{'batch_size': 16, 'sequence_file': 'preprocessed_seq_ab_1200.npz', 'pt_file': 'cdrs_output.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 1, 'num_classes': 2, 'num_epochs': 20, 'learning_rate': 0.003, 'n_splits': 3, 'max_grad_norm': 0.5}
Number of GPUs available: 3
Starting fold 1/3
Initializing ParatopeModel with 1 repeated layers...
Using 3 GPUs with DataParallel.
Fold 1, Epoch [1/20], Training Loss: 6.8425
Fold 1, Epoch [2/20], Training Loss: 0.6787
Fold 1, Epoch [3/20], Training Loss: 0.6547
Fold 1, Epoch [4/20], Training Loss: 0.6355
Fold 1, Epoch [5/20], Training Loss: 0.6083
Fold 1, Epoch [6/20], Training Loss: 0.5722
Fold 1, Epoch [7/20], Training Loss: 0.5449
Fold 1, Epoch [8/20], Training Loss: 0.5198
Fold 1, Epoch [9/20], Training Loss: 0.4996
Fold 1, Epoch [10/20], Training Loss: 0.4819
Fold 1, Epoch [11/20], Training Loss: 0.4474
Fold 1, Epoch [12/20], Training Loss: 0.4321
Fold 1, Epoch [13/20], Training Loss: 0.4186
Fold 1, Epoch [14/20], Training Loss: 0.4122
Fold 1, Epoch [15/20], Training Loss: 0.4065
Fold 1, Epoch [16/20], Training Loss: 0.3955
Fold 1, Epoch [17/20], Training Loss: 0.3877
Fold 1, Epoch [18/20], Training Loss: 0.3810
Fold 1, Epoch [19/20], Training Loss: 0.3733
Fold 1, Epoch [20/20], Training Loss: 0.3648
Fold 1 Validation Loss: 0.4405
New best model found at fold 1 with validation loss: 0.4405
Starting fold 2/3
Initializing ParatopeModel with 1 repeated layers...
Using 3 GPUs with DataParallel.
Fold 2, Epoch [1/20], Training Loss: 5.5090
Fold 2, Epoch [2/20], Training Loss: 0.6665
Fold 2, Epoch [3/20], Training Loss: 0.6561
Fold 2, Epoch [4/20], Training Loss: 0.6418
Fold 2, Epoch [5/20], Training Loss: 0.6221
Fold 2, Epoch [6/20], Training Loss: 0.6030
Fold 2, Epoch [7/20], Training Loss: 0.5938
Fold 2, Epoch [8/20], Training Loss: 0.5738
Fold 2, Epoch [9/20], Training Loss: 0.5613
Fold 2, Epoch [10/20], Training Loss: 0.5414
Fold 2, Epoch [11/20], Training Loss: 0.5016
Fold 2, Epoch [12/20], Training Loss: 0.4798
Fold 2, Epoch [13/20], Training Loss: 0.4636
Fold 2, Epoch [14/20], Training Loss: 0.4495
Fold 2, Epoch [15/20], Training Loss: 0.4391
Fold 2, Epoch [16/20], Training Loss: 0.4282
Fold 2, Epoch [17/20], Training Loss: 0.4199
Fold 2, Epoch [18/20], Training Loss: 0.4108
Fold 2, Epoch [19/20], Training Loss: 0.4020
Fold 2, Epoch [20/20], Training Loss: 0.3937
Fold 2 Validation Loss: 0.4498
Starting fold 3/3
Initializing ParatopeModel with 1 repeated layers...
Using 3 GPUs with DataParallel.
Fold 3, Epoch [1/20], Training Loss: 5.2198
Fold 3, Epoch [2/20], Training Loss: 0.6787
Fold 3, Epoch [3/20], Training Loss: 0.6564
Fold 3, Epoch [4/20], Training Loss: 0.6450
Fold 3, Epoch [5/20], Training Loss: 0.6208
Fold 3, Epoch [6/20], Training Loss: 0.6045
Fold 3, Epoch [7/20], Training Loss: 0.5923
Fold 3, Epoch [8/20], Training Loss: 0.5826
Fold 3, Epoch [9/20], Training Loss: 0.5695
Fold 3, Epoch [10/20], Training Loss: 0.5652
Fold 3, Epoch [11/20], Training Loss: 0.5324
Fold 3, Epoch [12/20], Training Loss: 0.5137
Fold 3, Epoch [13/20], Training Loss: 0.5008
Fold 3, Epoch [14/20], Training Loss: 0.4854
Fold 3, Epoch [15/20], Training Loss: 0.4725
Fold 3, Epoch [16/20], Training Loss: 0.4633
Fold 3, Epoch [17/20], Training Loss: 0.4535
Fold 3, Epoch [18/20], Training Loss: 0.4423
Fold 3, Epoch [19/20], Training Loss: 0.4335
Fold 3, Epoch [20/20], Training Loss: 0.4222
Fold 3 Validation Loss: 0.4616
Average Validation Loss across 3 folds: 0.4506
Best model saved successfully.
Layer: module.embedding.weight | Size: torch.Size([22, 128]) | Values : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.5913,  1.9332, -0.8353,  2.6790,  0.3793,  0.3451, -1.1396,  0.3335,
         -3.6069,  0.0902,  0.9357, -0.7595,  0.1490,  1.5120, -0.5550, -0.5545,
         -0.4473, -1.5678, -0.3828,  0.1789, -0.4653,  0.5266,  0.9199,  0.7723,
          1.6838, -0.9006, -1.1990,  1.6015, -0.0843, -0.0448,  1.7626, -1.4950,
         -0.1744, -0.8960, -1.1081, -1.8170,  2.1274,  0.0383, -0.0223,  1.0660,
         -0.9585,  1.1097, -1.4172,  0.1144,  0.2052,  0.6908, -0.9067,  1.0933,
         -0.4587,  0.2902, -2.3358,  0.0865, -1.9091, -0.1741, -2.5487, -0.6089,
         -0.7054, -0.4187,  0.3533, -0.3177,  2.4268,  0.0392,  1.1676,  0.4385,
          1.0592, -0.3774, -0.4638, -1.7207,  0.9432,  1.4703,  1.1255,  0.7623,
          1.0626, -0.5053, -0.8427, -0.3814, -0.8164, -1.3836, -1.4302, -2.6064,
          0.3827,  0.0536, -1.0043, -0.3011,  0.1547,  0.0760, -0.8221, -0.1711,
         -0.0963, -0.7727,  0.3395, -0.0979,  1.7014,  0.7302, -1.2328,  0.2068,
          0.6568,  0.3607, -0.2190,  1.5948,  1.1158, -0.7379,  0.6182,  0.5986,
          0.5810,  1.7728, -1.0294, -2.4596, -0.2560,  1.5852,  0.0933, -0.3833,
          1.1166,  2.8953,  0.3055, -1.4792, -1.0229,  0.2331, -1.5534, -0.4835,
         -0.3471, -0.6608, -1.7940, -0.4588, -0.2483,  2.6697,  0.2020, -1.4964]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.7707, 0.9523], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([-0.0152, -0.0433], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 1.3475e-02,  3.8430e-03,  5.2725e-02,  1.7762e-02,  2.6383e-02,
         -8.0204e-02, -3.5234e-02, -4.8205e-02, -1.7402e-02, -1.9482e-02,
         -8.0226e-02,  1.2254e-02,  1.3405e-02,  4.3441e-02, -2.4569e-02,
         -8.5368e-02, -8.2047e-02, -1.9972e-02, -7.3064e-02, -2.8361e-02,
         -5.4749e-02,  1.3051e-02, -1.2240e-02,  5.2380e-02, -4.9402e-02,
         -1.7210e-02, -5.3413e-02,  6.3517e-03, -1.3810e-02,  5.6619e-02,
         -6.9865e-03,  8.4482e-02,  4.5992e-02,  8.9034e-03,  2.3402e-02,
         -8.0086e-02,  3.4178e-02, -3.1835e-02,  1.5802e-02, -6.8684e-02,
          8.3965e-03,  6.5011e-02,  6.6973e-02,  4.8399e-02, -3.2263e-02,
         -1.8794e-02, -3.7091e-02, -9.2149e-03,  4.7465e-02, -1.5264e-02,
         -7.1104e-02, -3.7530e-02,  4.3061e-02,  7.4941e-02, -5.4367e-03,
         -2.7923e-02,  7.2342e-02, -2.0853e-02, -2.1589e-02, -8.0294e-02,
          7.5981e-02,  1.5844e-02, -6.4390e-02,  1.8920e-02,  3.0705e-02,
          8.2648e-02, -3.7741e-02,  1.6380e-02,  2.4272e-02, -6.5778e-02,
          9.4421e-03,  1.8268e-02,  5.1997e-03,  7.3759e-02,  8.0242e-02,
         -7.3784e-02,  2.2699e-02,  3.7208e-02,  1.8997e-02,  6.1578e-03,
         -6.5605e-02,  6.1673e-02, -5.5759e-02, -3.3871e-02, -1.7884e-02,
         -5.2603e-02,  6.0459e-03,  2.6174e-02,  2.5705e-02,  1.4438e-02,
          5.5836e-02, -4.5937e-04,  2.6453e-03, -3.5909e-03,  8.8018e-02,
         -8.7619e-03, -4.4366e-02,  4.0161e-02, -5.4133e-02,  8.6714e-02,
          2.3712e-03,  3.7907e-02, -8.6095e-03, -5.6492e-02, -3.1322e-02,
          7.4586e-02,  5.4138e-03,  2.2918e-02,  1.2977e-02, -8.6946e-02,
          8.4453e-02, -3.3412e-02, -3.1590e-02,  3.2591e-03, -4.4909e-02,
         -1.1014e-02,  6.7591e-02,  6.2571e-02, -7.3818e-02, -6.5375e-02,
         -4.7809e-02, -5.2395e-02,  4.0798e-02, -2.8217e-02, -7.8782e-02,
         -4.3880e-02, -8.8025e-02, -2.0989e-02],
        [-5.1369e-03, -5.9840e-02, -7.9980e-02, -1.5784e-03,  7.0470e-02,
          9.2619e-03, -7.4602e-02, -3.2492e-02, -1.0221e-02, -7.1946e-02,
          6.3141e-02,  8.2440e-03, -3.7140e-02,  1.2103e-03,  2.5861e-02,
          8.2663e-02,  5.6138e-02,  7.8008e-03,  3.6030e-02, -1.2143e-02,
         -7.5256e-02, -5.8408e-02, -7.0366e-02, -3.1798e-02,  2.4578e-02,
         -8.2673e-02, -2.5592e-02, -1.8119e-02, -4.4744e-02, -5.6466e-02,
         -4.8716e-02,  3.4234e-02, -4.4419e-02, -8.0037e-03,  6.3108e-03,
          4.5108e-02,  7.9573e-02, -3.5207e-02, -1.1851e-02, -2.4886e-02,
         -4.5214e-02, -2.0227e-03, -1.2973e-02,  3.4120e-02,  8.7993e-02,
         -7.0826e-02, -7.8120e-02, -5.3852e-02,  8.1213e-02, -8.8041e-02,
         -6.3899e-02,  1.6427e-02,  5.7946e-02,  5.4182e-02,  3.2356e-02,
         -2.2261e-02, -6.0943e-02,  2.2045e-03,  4.7934e-02, -4.7375e-02,
          2.2139e-02, -7.5362e-02, -1.6461e-02, -1.2429e-02,  6.1373e-02,
         -2.4612e-02, -1.8968e-02,  6.2414e-02,  2.2444e-02, -3.5614e-03,
          2.9044e-02, -5.3522e-02,  6.4301e-02, -4.0458e-02,  6.1818e-02,
         -3.7868e-02,  1.6318e-02,  3.8302e-02, -8.8335e-02, -1.5269e-02,
         -5.5775e-02, -7.7279e-02,  6.6266e-02, -3.5602e-02,  3.6374e-03,
         -5.2052e-02,  7.2339e-02,  3.4557e-02, -4.0948e-02,  3.8109e-03,
         -8.2602e-02, -2.7684e-02, -6.9404e-02,  6.1263e-02,  6.5827e-02,
         -5.4018e-03,  2.3943e-02, -2.6283e-02, -1.3232e-03, -2.9932e-02,
         -2.8574e-02, -8.6295e-02,  6.4700e-02, -4.3924e-02,  3.2647e-02,
          2.1734e-02, -2.1653e-02,  8.7025e-02,  2.7772e-02, -2.7528e-02,
          5.5387e-02, -5.0369e-02, -8.1318e-02, -6.5831e-02,  7.3571e-02,
          8.6744e-02, -5.9489e-02,  7.1438e-02,  6.6280e-02,  8.8930e-05,
         -7.6237e-02, -1.5133e-02,  1.5578e-02, -7.9009e-02,  3.5181e-02,
         -4.8508e-02,  7.0537e-02,  6.1739e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[-0.0072, -0.0511,  0.0835, -0.0234, -0.0497,  0.0390,  0.0873, -0.0794,
         -0.0707, -0.0415, -0.0588,  0.0748,  0.0690,  0.0242, -0.0868,  0.0315,
          0.0622, -0.0625, -0.0611, -0.0873,  0.0145,  0.0851, -0.0723, -0.0307,
          0.0244, -0.0440, -0.0347, -0.0841,  0.0088,  0.0601,  0.0824,  0.0601,
          0.0881, -0.0781,  0.0221, -0.0395,  0.0872, -0.0856,  0.0314,  0.0626,
          0.0590,  0.0619,  0.0374,  0.0312,  0.0116,  0.0099, -0.0422,  0.0339,
         -0.0351, -0.0818,  0.0053, -0.0613, -0.0841,  0.0498,  0.0228,  0.0117,
         -0.0386, -0.0751, -0.0825, -0.0325, -0.0709,  0.0751,  0.0476, -0.0359,
          0.0311, -0.0592,  0.0618,  0.0379,  0.0833,  0.0637,  0.0496,  0.0701,
          0.0307, -0.0346,  0.0244, -0.0370,  0.0828,  0.0104,  0.0464,  0.0031,
         -0.0416, -0.0313, -0.0319, -0.0349, -0.0035,  0.0623, -0.0629, -0.0804,
          0.0552,  0.0126,  0.0598,  0.0873,  0.0798,  0.0216, -0.0332,  0.0684,
          0.0509,  0.0244,  0.0318, -0.0875, -0.0676,  0.0112,  0.0366,  0.0444,
         -0.0454,  0.0267, -0.0594,  0.0740,  0.0349,  0.0855,  0.0594, -0.0639,
          0.0108, -0.0182, -0.0767,  0.0497,  0.0831, -0.0703,  0.0562,  0.0447,
         -0.0659,  0.0123,  0.0089,  0.0067, -0.0801,  0.0313, -0.0443,  0.0508],
        [ 0.0706,  0.0754,  0.0298,  0.0318,  0.0153,  0.0782, -0.0687,  0.0266,
         -0.0071, -0.0717, -0.0380, -0.0295,  0.0521, -0.0489, -0.0112,  0.0659,
          0.0594,  0.0056, -0.0525, -0.0080, -0.0442, -0.0276, -0.0225,  0.0591,
         -0.0404,  0.0178, -0.0133, -0.0381, -0.0486,  0.0025,  0.0830,  0.0551,
         -0.0421, -0.0184, -0.0243,  0.0629, -0.0533,  0.0362,  0.0051, -0.0523,
         -0.0362,  0.0444, -0.0393, -0.0737,  0.0417, -0.0497,  0.0140, -0.0493,
          0.0690, -0.0596,  0.0327, -0.0217, -0.0701, -0.0465,  0.0617,  0.0222,
          0.0665, -0.0340,  0.0649, -0.0138, -0.0327,  0.0140, -0.0198,  0.0054,
         -0.0455,  0.0051,  0.0812,  0.0226,  0.0429, -0.0796,  0.0382, -0.0880,
         -0.0105, -0.0726, -0.0636, -0.0375, -0.0250, -0.0708, -0.0021,  0.0069,
         -0.0632,  0.0451, -0.0004,  0.0844, -0.0388, -0.0758, -0.0449,  0.0825,
         -0.0712,  0.0729,  0.0771, -0.0206, -0.0717, -0.0784, -0.0358, -0.0797,
          0.0024, -0.0153, -0.0549,  0.0012,  0.0485, -0.0586, -0.0849,  0.0102,
         -0.0785,  0.0092,  0.0747, -0.0302, -0.0473, -0.0354,  0.0754,  0.0846,
         -0.0499,  0.0742, -0.0081,  0.0388, -0.0355,  0.0451,  0.0029, -0.0496,
          0.0241, -0.0182, -0.0849,  0.0832,  0.0666,  0.0043, -0.0588,  0.0715]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.0034, -0.0325,  0.0244,  ...,  0.0398,  0.0033,  0.0488],
        [-0.0019, -0.0202, -0.0011,  ...,  0.0159,  0.0130,  0.0172]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.0572, -0.0781], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-3.7304e-01, -7.3805e-02, -5.5100e-02,  1.9466e-01, -2.1735e-01,
         -1.2804e-01, -3.3470e-01, -1.1616e-01,  1.8033e-01, -1.5855e-01,
         -1.0084e-01,  3.2620e-02,  1.7770e-02, -3.9574e-01,  1.4538e-01,
          1.3318e-02, -3.1343e-01,  1.1447e-01,  8.3779e-02,  5.8465e-02,
          1.7824e-01,  2.3714e-01, -2.4161e-01,  3.5096e-02, -1.7256e-01,
         -3.1436e-01,  3.4412e-01, -1.7456e-02, -1.5216e-01, -2.6554e-01,
         -1.7014e-01,  2.4124e-01,  1.7041e-01,  4.7189e-02,  2.6334e-01,
          2.5265e-02, -2.8895e-01,  1.7982e-01, -4.0198e-02, -8.1422e-02,
          1.8380e-01,  2.6687e-02,  1.5738e-01, -1.6998e-02, -7.8332e-03,
         -3.9537e-02, -1.8313e-01, -1.2312e-01,  2.0722e-01, -7.0656e-02,
         -2.1707e-01,  8.7072e-02,  4.8156e-02,  2.0179e-02,  1.3994e-01,
          2.5173e-01,  3.6727e-01,  1.2274e-01,  4.3985e-02, -3.7002e-02,
         -1.5518e-01,  7.6246e-02,  1.8603e-01,  1.4483e-02,  1.3348e-01,
          1.0638e-01, -3.8059e-01,  3.9597e-02,  9.4102e-02, -2.6087e-01,
          1.3427e-01, -1.0870e-01, -3.3565e-01,  2.7288e-01,  3.5586e-01,
         -1.2270e-01,  1.2885e-01,  1.8375e-01, -1.6885e-03, -1.8066e-01,
         -3.7371e-01,  1.7852e-01, -1.2969e-02, -2.6449e-01, -2.2981e-01,
          1.9221e-01, -2.0332e-02, -1.7330e-01,  9.3969e-02,  3.3454e-01,
          3.4604e-01, -3.7279e-02, -3.4438e-01, -2.4251e-01,  2.0985e-01,
         -2.9089e-01, -1.7389e-01,  2.8889e-01,  1.3078e-01, -1.3641e-01,
          2.7926e-01, -1.8258e-01, -9.4453e-02, -3.0460e-01,  1.3222e-01,
          3.6542e-03,  9.7009e-02,  2.0881e-01, -2.4231e-02, -3.6612e-02,
          2.1795e-01, -3.0968e-02,  7.3398e-02,  9.8269e-02, -9.3341e-02,
          1.4649e-01,  3.5497e-04, -3.1850e-01, -3.8406e-02,  1.9798e-01,
          3.4563e-01, -8.1329e-02, -2.8976e-01,  4.0443e-01, -7.3853e-02,
         -3.7305e-02, -1.6732e-01, -7.9279e-02],
        [-7.4493e-02, -2.4728e-01, -3.2053e-01,  3.2343e-02, -1.6490e-01,
         -2.1151e-01, -1.6147e-01, -8.9505e-04,  2.8655e-01, -2.1612e-01,
          2.4379e-02,  1.5063e-01, -1.2715e-01, -2.1418e-01,  1.0736e-01,
         -7.7335e-02, -4.0838e-01,  2.9425e-01, -2.3639e-02,  5.0777e-02,
          4.1699e-01,  2.2940e-02, -1.7050e-01, -6.6649e-02,  5.5958e-02,
         -3.3237e-01,  3.3664e-01,  4.6494e-02, -6.4984e-02, -4.4487e-01,
         -2.4760e-01,  3.1855e-01, -2.0092e-02, -1.0695e-01,  1.1491e-01,
          1.4690e-01, -2.1626e-01,  2.2786e-01, -6.9921e-02, -1.0956e-01,
          1.7394e-01, -1.0162e-02,  2.2776e-01, -2.8714e-02,  1.0171e-01,
          1.5393e-02, -9.6710e-02,  2.1838e-03,  3.9747e-01, -1.8675e-01,
          1.8758e-02,  2.0707e-01,  1.9522e-01, -2.4551e-01,  1.4186e-01,
          3.2522e-01,  2.5679e-01,  5.6057e-02,  1.9524e-01, -1.5582e-01,
         -3.0562e-01,  5.2598e-02,  3.0358e-01, -2.4754e-02,  1.3898e-01,
          2.2217e-01, -2.8521e-01,  8.8429e-02, -6.5626e-02, -1.6097e-01,
          1.8260e-01, -1.6550e-01, -1.4837e-01,  1.5097e-01,  8.7997e-02,
         -1.5285e-01, -3.1090e-02, -6.3111e-02,  1.4547e-01, -2.3308e-02,
         -2.1511e-01,  7.3777e-02,  9.8510e-03, -6.3997e-02, -2.7766e-01,
          1.7838e-01,  4.3261e-02, -1.0260e-01,  7.9001e-02, -1.1958e-01,
          2.4802e-01,  1.5166e-01, -1.3470e-01, -3.8917e-01, -6.8879e-02,
         -3.4842e-01, -1.7810e-01,  8.9419e-02,  2.4672e-01, -3.9622e-01,
          4.2504e-01, -2.6814e-01, -3.3694e-02, -1.6523e-01,  9.6681e-02,
          1.7371e-02,  1.1500e-01,  1.7436e-01, -4.0572e-02,  1.0380e-03,
          2.2958e-01,  5.5204e-02, -9.3754e-02, -1.4502e-01, -2.6078e-01,
          1.1638e-01,  7.0645e-02, -2.6692e-01,  1.0557e-02,  9.8599e-02,
          4.8211e-01, -8.0469e-02, -1.1791e-01,  3.7459e-01, -1.4075e-01,
          2.6989e-01, -2.3734e-01, -3.2136e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.5369, 0.6216], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.7885, 0.8138], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([-0.0590, -0.0597], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-9.4831e-02, -7.6877e-03, -9.6956e-03, -4.8487e-02, -1.1061e-03,
         -2.7510e-02, -3.0857e-02,  1.1667e-01,  1.4188e-02, -1.8193e-02,
         -2.2035e-03,  1.2291e-01, -1.0434e-01,  1.3371e-01, -1.3479e-01,
         -8.9902e-02,  1.0924e-01, -8.3423e-02, -5.3063e-02,  1.2175e-01,
          1.1784e-01,  3.3906e-03, -6.3427e-02,  2.8876e-02,  4.8366e-02,
          2.0799e-02, -2.2861e-02, -5.0604e-03, -7.5250e-02, -2.9723e-02,
          6.8489e-02,  5.3856e-02, -3.5534e-02, -3.3777e-02, -2.8016e-02,
          1.5150e-01, -3.2826e-02,  1.0196e-01, -3.3395e-02,  7.9984e-02,
          5.9555e-02,  8.2480e-02, -1.2885e-01,  5.9417e-02,  2.5584e-02,
          5.9275e-02, -7.4444e-02, -3.0162e-02,  6.0922e-02,  4.2509e-02,
          4.0876e-02,  1.2186e-01, -5.9684e-02,  5.7563e-02,  1.0007e-01,
          1.5343e-01, -8.8423e-03, -6.3567e-02, -3.0204e-02, -6.6325e-02,
          9.6290e-03,  4.0124e-02,  2.5327e-02, -1.6880e-02, -9.1774e-02,
          1.1233e-01, -1.3588e-01, -2.0446e-01,  1.1809e-02, -8.5134e-02,
         -1.4488e-02, -3.8273e-03,  4.5991e-02,  4.6828e-03,  1.1849e-02,
         -8.4739e-02, -1.0205e-01,  2.1523e-03, -9.1124e-02, -5.6177e-02,
          2.3281e-02,  1.7515e-01, -7.5084e-03,  6.1663e-02,  1.0611e-02,
          1.7765e-02, -3.2775e-02, -1.0214e-01,  1.1736e-01, -9.3006e-02,
         -1.2968e-01,  3.2573e-02,  6.0242e-02,  8.4747e-02, -8.7150e-02,
         -4.5088e-03, -3.5833e-02, -3.9223e-03,  3.8850e-02, -2.4057e-02,
         -4.3944e-02, -8.0487e-02,  1.2090e-01, -5.4234e-02,  1.3527e-01,
         -2.2363e-02,  1.0559e-01,  2.0922e-02, -5.8068e-02,  1.8861e-02,
         -1.3111e-01,  9.5875e-03,  9.2579e-02, -4.1520e-02,  1.2136e-01,
         -3.0157e-03,  9.5515e-02, -1.2562e-02,  3.8928e-02, -5.8423e-02,
         -6.1407e-02, -9.2799e-02, -1.1375e-01,  8.6725e-02, -6.3731e-02,
          1.3162e-01, -1.6033e-01,  4.7609e-02],
        [ 6.3462e-02, -1.0148e-01, -9.2738e-02, -4.5702e-02, -1.5206e-01,
         -1.1594e-01, -1.6522e-01, -2.3672e-02, -1.1065e-02,  6.3177e-03,
          2.0814e-01, -6.1451e-02,  1.3654e-01,  1.1302e-01, -3.4737e-03,
         -1.9120e-02,  5.4741e-02, -1.3454e-01,  1.5443e-01, -3.6443e-02,
          2.9516e-01, -9.9605e-02,  1.6691e-01,  1.0319e-01,  9.6414e-02,
          1.7593e-01, -4.2561e-03,  1.7271e-01, -1.8971e-01, -7.6559e-02,
         -4.0738e-02,  1.4158e-02, -8.5551e-02, -1.7003e-03,  8.0778e-02,
         -9.9041e-02,  2.5629e-02,  7.8431e-02, -2.7168e-02,  2.1613e-01,
          1.3214e-01, -4.2867e-02,  3.1537e-02,  1.2813e-02,  5.0280e-02,
         -4.5882e-02, -8.1219e-02,  9.6583e-02, -4.6343e-02,  3.9191e-01,
         -1.5267e-01,  1.5841e-02, -1.5138e-02, -1.2393e-02, -4.1865e-02,
          1.3339e-01,  7.1963e-02, -3.7604e-03, -6.2330e-02,  1.1552e-01,
          3.3764e-02, -1.7471e-01,  3.4276e-02,  1.5822e-02,  2.8313e-04,
         -1.1858e-01,  1.5985e-01, -1.1580e-01,  1.5470e-01,  1.7464e-01,
         -4.3282e-02,  9.6406e-02,  1.2772e-01, -2.7448e-02, -7.4700e-02,
         -5.7985e-02,  7.7270e-02, -3.4974e-02, -1.2475e-01, -1.3261e-01,
          1.7285e-02, -8.4642e-02, -6.9410e-02, -3.9510e-02, -1.0016e-01,
          7.1314e-02, -1.6536e-01,  3.9364e-02, -9.0801e-02, -8.2406e-02,
         -2.6156e-01,  5.4572e-02, -3.9407e-02,  1.5170e-01,  1.0355e-01,
          6.6277e-02, -3.1892e-02, -9.9000e-03,  1.2838e-01,  7.4838e-02,
          7.3959e-02, -1.0023e-01, -3.1126e-02, -3.3140e-02, -3.8908e-02,
          5.3481e-02, -1.0441e-02, -4.4768e-03, -1.7795e-01,  5.6489e-02,
          5.1274e-02, -1.3600e-01,  1.1137e-02,  1.1899e-01, -2.6418e-03,
          1.1999e-01,  7.0245e-02, -3.4926e-02, -5.6297e-02, -3.2255e-03,
          7.8242e-02,  4.9198e-02, -6.8630e-02, -1.2150e-01, -1.2517e-01,
          2.5210e-02, -2.0193e-02, -1.3132e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[ 1.5789e-02,  7.4580e-03,  7.5492e-02, -1.1448e-01,  1.1376e-01,
          7.5029e-02, -9.3596e-02, -8.4086e-02, -1.3815e-01, -3.3993e-02,
          1.0989e-01, -4.1985e-02,  7.4641e-02,  4.3131e-02, -3.1475e-02,
          1.8903e-01, -3.5537e-02, -6.1746e-02,  2.4280e-03,  1.0251e-01,
         -1.0151e-01,  9.8531e-02, -7.0457e-02, -3.3968e-02,  2.6020e-02,
          5.9757e-02,  4.0043e-02,  5.5517e-02,  1.3322e-02,  1.9649e-01,
          9.9674e-02, -3.7077e-02, -2.3351e-02,  8.2062e-02,  1.2373e-01,
         -8.7256e-02,  5.5430e-02,  3.7591e-02, -5.0631e-03, -2.6644e-02,
          4.1145e-02,  3.4165e-02,  1.4298e-01, -1.7876e-01,  6.9023e-02,
         -5.8768e-02, -7.0483e-02,  8.1067e-02,  5.7996e-02,  8.8432e-02,
         -6.2319e-02,  2.5855e-02,  6.9190e-02, -1.1729e-03,  3.2427e-02,
         -6.6644e-02, -3.4606e-02, -3.9499e-02,  1.0577e-01, -3.1642e-02,
          1.0371e-01, -9.9325e-03,  5.5128e-04,  2.7900e-02, -9.0856e-02,
          1.0586e-01, -1.3027e-01,  1.4509e-01, -7.0518e-02,  3.1807e-02,
         -5.2472e-02, -1.2404e-01,  7.2835e-02,  6.2580e-02,  1.9224e-02,
          5.0600e-02, -2.6280e-02, -1.2809e-01, -6.2252e-02,  8.5534e-02,
         -9.4664e-02, -2.6787e-02,  7.5963e-02,  1.9004e-02, -1.2473e-01,
         -1.4740e-01,  2.2730e-01, -5.0692e-02, -6.9836e-02, -9.2304e-02,
          5.7060e-02,  1.3584e-01,  1.1078e-02,  9.7673e-02,  1.6763e-01,
         -2.2324e-02, -7.7339e-02, -1.8302e-02, -1.3679e-01, -5.8461e-03,
          3.2714e-02, -1.8589e-02,  3.2516e-02,  6.1823e-02, -1.7809e-02,
          6.3028e-02, -8.5296e-02,  9.7465e-02,  4.1581e-02,  1.3828e-01,
          5.1729e-02, -1.0571e-01, -2.1697e-01,  6.4270e-02, -2.8172e-02,
          2.2282e-02,  5.5737e-02, -3.0030e-02, -3.7509e-02,  1.1123e-01,
         -7.1983e-02, -6.4321e-02, -2.3981e-01, -6.5731e-02,  5.2351e-02,
         -1.3295e-01, -7.5517e-02, -5.8548e-02],
        [ 1.1501e-01, -8.6850e-02,  1.3138e-01,  3.1751e-02,  5.2813e-02,
         -1.8802e-02, -4.9434e-02, -5.1363e-02,  2.0690e-02,  7.2066e-02,
          9.3752e-02,  2.2661e-02, -9.3762e-02,  2.3607e-02,  1.1219e-01,
         -6.9815e-02, -1.9485e-01,  5.7778e-02,  4.8912e-02,  1.1242e-01,
         -2.4831e-02, -2.9434e-02, -1.4425e-01, -8.8303e-02,  1.5340e-04,
         -1.6788e-01, -1.7206e-01, -1.3542e-02, -5.0030e-02,  9.2094e-02,
         -1.0023e-01, -3.9824e-02, -1.1510e-01, -4.5844e-02, -7.2178e-02,
         -2.4919e-01,  7.1007e-03, -7.6221e-02, -3.6996e-02,  9.4029e-02,
          1.1172e-01, -6.4492e-03, -5.2795e-02, -9.4136e-02, -1.7055e-01,
          5.0206e-02, -7.4352e-02,  3.5487e-02,  1.2630e-01, -3.2206e-02,
         -6.6772e-02,  2.8022e-03, -2.1982e-02, -1.0659e-01, -2.6485e-02,
         -7.8068e-03, -3.9770e-02,  1.7213e-01,  1.8433e-03,  9.3893e-02,
         -3.5235e-02,  1.0335e-01, -5.4160e-03, -2.2777e-02, -4.8698e-02,
         -1.8768e-01,  7.4063e-02,  1.2043e-01,  4.3527e-02, -9.5791e-02,
          9.7111e-02,  5.9853e-02,  9.8324e-02, -3.4332e-02,  1.5820e-01,
         -1.3698e-02,  8.5564e-02, -1.9440e-01,  7.5617e-02,  4.4094e-02,
         -1.5073e-03, -1.7571e-01,  1.5106e-01, -3.5779e-02,  3.0764e-02,
          1.9642e-01, -8.7151e-02,  6.9151e-02, -1.7821e-01,  1.4047e-01,
          2.0384e-01,  6.5329e-03, -1.0231e-01,  1.1298e-01,  2.9277e-02,
          4.8765e-02, -1.3055e-01,  1.5725e-01,  1.1591e-01, -1.7227e-01,
          2.1083e-01, -3.6894e-02, -9.8169e-03,  4.3178e-03, -1.3156e-01,
         -7.7978e-02, -5.0912e-02,  7.2735e-02, -4.7367e-02, -2.7852e-01,
          1.9441e-02, -1.0576e-01, -6.5515e-02,  7.7478e-02,  2.8265e-03,
         -1.0963e-01, -7.8179e-02, -6.6596e-02,  1.8513e-02, -1.0153e-01,
         -1.1863e-01,  8.5781e-02, -4.0638e-02,  5.3724e-02, -1.7047e-01,
          4.6286e-02,  1.4580e-01, -9.0446e-03]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[-0.0666, -0.0378,  0.0238,  ...,  0.0543, -0.0362,  0.0243],
        [ 0.0077,  0.0037, -0.0118,  ...,  0.0433, -0.0412,  0.0271]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([ 0.0382, -0.0180], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 6.8224e-02,  1.3197e-02,  6.0822e-03,  2.9515e-02,  5.7166e-02,
          3.4290e-02, -3.3806e-01,  2.0415e-02, -2.4483e-01,  4.3157e-02,
         -1.0152e-01, -1.9920e-01,  2.2139e-01,  8.9000e-02, -1.0351e-01,
         -1.8473e-01,  6.0044e-02,  1.2577e-01,  4.8681e-02,  8.4016e-02,
         -8.7012e-02, -1.9389e-01, -1.3641e-01, -3.8971e-02,  6.6833e-02,
          1.2782e-01, -4.1633e-02,  2.4186e-01, -8.0648e-02,  1.9252e-02,
          2.5055e-01, -6.8631e-02, -1.2784e-03, -2.1693e-01, -1.0006e-01,
         -2.5183e-02,  1.8935e-01,  1.9265e-02,  1.3766e-01,  1.4753e-01,
         -1.2731e-01,  3.5671e-01, -2.9550e-01, -7.1547e-02,  1.1893e-01,
         -7.0179e-02, -1.4929e-01, -6.7867e-02,  1.3197e-01,  1.3496e-03,
          9.1722e-02,  5.9385e-02, -4.4928e-02,  1.1943e-01,  7.1666e-02,
         -4.3429e-02, -7.2848e-02,  8.7803e-02, -6.8329e-02, -1.9077e-01,
          2.3628e-01,  9.1811e-02,  1.2365e-02,  1.0114e-01,  7.0005e-02,
          2.1966e-01, -8.3049e-02,  8.3026e-03, -2.1998e-01, -1.7461e-01,
          2.9586e-01, -6.4314e-02,  2.5198e-01,  1.9728e-02, -1.5483e-01,
         -1.2786e-01,  3.5651e-02,  1.7808e-01, -4.2692e-02, -1.4532e-01,
          3.1375e-02, -8.0315e-03, -6.8113e-02,  2.4642e-01, -2.4808e-01,
         -3.9995e-01,  4.7777e-02,  7.5543e-03,  5.3271e-02,  9.9455e-02,
          7.2715e-02, -2.0420e-01, -8.9937e-02,  5.9216e-02, -6.1541e-02,
          2.2367e-03, -1.7054e-01,  2.5114e-01,  1.7049e-01,  1.7830e-01,
          3.8147e-01, -3.6316e-01,  6.9895e-03, -2.2713e-01,  1.9369e-01,
          1.8519e-01, -1.1615e-02, -3.1672e-02,  8.4101e-02,  5.8400e-02,
         -8.0509e-02,  2.9142e-02, -3.4133e-03,  1.8003e-01, -1.9532e-01,
         -1.3830e-01,  1.6119e-01, -2.2175e-01,  1.8696e-01,  3.3246e-01,
         -1.6479e-01, -1.5924e-01, -2.0210e-01,  1.3525e-02,  9.3415e-02,
         -8.0249e-02, -8.0964e-02, -6.1053e-02],
        [ 9.7791e-02, -1.2588e-01,  1.1006e-02,  1.4305e-01,  1.8364e-01,
         -3.5030e-02, -1.4528e-01,  2.7053e-02, -2.9410e-02,  4.5598e-02,
         -6.9340e-02,  7.8482e-03, -7.3840e-03, -2.4547e-01,  7.8904e-02,
          9.1600e-03,  1.9406e-01, -8.4951e-02,  6.6565e-02, -3.5096e-03,
         -2.7389e-02,  2.5423e-02, -1.3781e-02, -5.9876e-02, -7.1116e-02,
         -1.0562e-01,  6.4907e-02,  9.5163e-02, -7.7976e-02, -4.1333e-02,
          3.4940e-02,  1.1068e-02,  7.2183e-03, -1.8475e-02,  1.4121e-01,
         -2.2397e-02, -1.5681e-01,  2.8187e-02, -1.9333e-02,  5.7340e-02,
          3.8066e-02, -6.0152e-02,  2.2845e-02, -1.3094e-01,  5.8560e-02,
          3.0896e-02,  1.0975e-02, -1.2306e-01,  4.1820e-03, -1.2056e-01,
         -1.7585e-02,  1.0141e-02, -1.9074e-02,  9.5285e-03, -1.0712e-02,
          3.5001e-03, -2.5626e-02, -5.1669e-02, -1.8640e-02, -4.5499e-02,
         -1.3541e-01, -6.6067e-02, -2.9293e-02,  1.2360e-01, -8.6106e-03,
          1.5865e-01, -1.1956e-01, -8.4221e-03,  2.4599e-02,  2.7058e-02,
          3.9259e-02,  3.0380e-02,  2.9225e-02, -2.0716e-02, -1.9872e-03,
         -6.1807e-02,  3.5881e-02, -3.1781e-02, -8.1861e-02, -2.3989e-01,
          6.0783e-02,  1.4435e-01, -2.1077e-02,  9.8876e-03, -1.0352e-01,
          5.6444e-02,  7.3962e-02, -6.1579e-02,  6.1706e-02,  6.7095e-02,
          1.6880e-02, -3.6039e-02, -9.7860e-02,  7.8306e-02,  1.4750e-01,
         -7.9075e-02,  5.6740e-02,  8.8166e-02,  6.3325e-02,  1.0657e-01,
          9.7523e-03, -4.7376e-02, -7.5149e-02,  9.0679e-02,  7.4575e-02,
         -5.2480e-02, -8.8115e-02,  7.9532e-02, -7.6172e-02,  5.7341e-02,
          6.2511e-02,  3.1878e-02, -9.1110e-02,  5.0828e-02, -7.3732e-02,
          1.7046e-01,  4.6247e-03, -9.1628e-02,  3.1150e-02, -8.1045e-03,
         -9.8748e-03, -5.6045e-02,  1.5451e-06,  1.9021e-02, -1.6737e-02,
          1.7835e-01, -8.0177e-02, -7.8112e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.6243, 0.8138], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.weight | Size: torch.Size([2, 128]) | Values : tensor([[-2.4058e-02,  1.3307e-02,  1.9634e-02, -1.4120e-03, -8.9407e-02,
         -5.6848e-03, -3.8132e-02, -3.3139e-02, -3.2785e-02,  1.5525e-02,
          7.6331e-02,  1.9402e-02, -2.1548e-05,  8.3261e-03,  1.9141e-02,
          6.9022e-02,  1.3283e-02, -5.9026e-02, -1.9697e-02,  7.5089e-02,
         -4.9711e-02,  2.6004e-02,  1.7955e-02,  7.7988e-02,  1.7325e-02,
         -7.8421e-03, -4.1426e-03, -1.4307e-02,  7.5484e-02,  6.5414e-02,
          1.8603e-02, -7.0743e-03, -4.3099e-02, -6.2948e-02, -1.6272e-02,
          6.7200e-03, -1.4881e-02, -7.9738e-03, -4.4983e-03, -5.2487e-02,
         -2.9649e-02, -2.0910e-02,  5.4665e-02, -1.0421e-02,  3.1190e-02,
          4.0353e-02,  6.0910e-02,  8.9930e-03, -1.1356e-02,  4.7868e-02,
          1.8164e-02, -2.6579e-02, -9.4912e-03,  2.4268e-02,  1.3656e-02,
          1.4841e-02,  1.7667e-02, -3.4576e-02, -3.4239e-02,  4.1865e-02,
          6.0628e-02, -2.4658e-02, -5.8172e-02,  1.9232e-02, -5.3502e-02,
          1.4925e-02, -6.5477e-03,  4.2252e-02,  6.3579e-03, -1.0725e-02,
          2.5578e-02,  4.1183e-02,  4.1682e-02,  2.7043e-02,  2.7044e-02,
         -2.4170e-02,  4.0195e-02,  2.4194e-02, -4.4765e-02, -3.0619e-03,
         -2.6990e-02,  1.6141e-02,  3.7997e-03, -2.0235e-02,  3.5220e-02,
         -1.4120e-02,  2.2621e-02,  6.3399e-02,  7.6456e-02,  3.8421e-02,
          9.2634e-04, -5.1958e-02,  5.7064e-02,  9.3277e-03,  4.2658e-03,
          6.9447e-02, -7.3284e-04,  6.1031e-02,  5.0776e-02,  1.1031e-02,
          5.0635e-03,  5.9959e-02,  8.3452e-03, -1.0875e-02,  3.0092e-02,
         -6.0737e-02,  4.3419e-02, -8.8749e-03, -1.9540e-02,  5.9039e-02,
         -2.8236e-02, -5.6435e-02,  3.1881e-02, -9.2249e-03, -1.4220e-03,
         -2.0477e-02,  1.4877e-02, -4.8777e-02,  6.0216e-02,  5.2781e-02,
          1.8999e-02,  8.2005e-02, -2.4289e-02, -5.4022e-03,  4.0830e-02,
          7.8306e-03, -1.0573e-02, -7.8326e-02],
        [-2.3553e-02,  9.6470e-03,  1.8208e-02, -5.3945e-03, -8.5062e-02,
         -1.9817e-03,  5.0265e-02, -3.3033e-02, -3.4667e-02,  1.4658e-02,
          7.4479e-02,  2.3405e-02,  4.0018e-03,  6.9166e-03,  1.7985e-02,
          6.9588e-02,  1.4667e-02, -5.4611e-02, -1.8068e-02,  7.6080e-02,
         -5.1142e-02,  2.4472e-02,  1.4110e-02,  7.4222e-02,  2.7051e-02,
         -1.1871e-03, -7.4201e-03, -2.2517e-02,  7.3700e-02,  6.7778e-02,
          2.4639e-02, -1.1215e-03, -3.9544e-02, -6.1677e-02, -1.3471e-02,
          1.1440e-02, -1.0845e-02, -7.2416e-03,  4.5634e-03, -4.9549e-02,
         -3.0628e-02, -1.4828e-02,  5.1865e-02, -1.7179e-02,  3.3785e-02,
          4.2359e-02,  5.8933e-02,  6.3512e-03, -7.5488e-03,  1.7629e-02,
          1.9318e-02, -8.7166e-03, -8.4332e-03,  2.8391e-02,  2.3792e-02,
          1.1768e-02,  2.1265e-02, -3.2775e-02, -4.7522e-02,  3.8434e-02,
          7.1904e-02, -1.3563e-02, -5.3994e-02,  1.7531e-02, -5.6346e-02,
          1.9202e-02, -1.3590e-02,  3.9415e-02,  4.3174e-03, -2.7962e-02,
          2.3534e-02,  3.2702e-02,  4.0528e-02,  2.5211e-02,  2.9094e-02,
         -2.6622e-02,  3.5394e-02,  3.1078e-02, -4.4707e-02, -2.9162e-03,
         -3.2795e-02,  1.3871e-02,  5.6810e-03, -1.9541e-02,  3.0320e-02,
         -2.3984e-02,  2.2177e-02,  6.4770e-02,  7.7681e-02,  3.7263e-02,
         -6.0864e-04, -4.9620e-02,  5.8262e-02,  1.0981e-02, -1.7436e-02,
          6.1988e-02, -5.1615e-03,  6.6017e-02,  5.2304e-02,  1.3541e-02,
          1.4490e-02,  5.4084e-02,  8.7988e-03, -2.0435e-02,  2.8562e-02,
         -5.9611e-02,  3.8787e-02, -4.5830e-03, -1.7748e-02,  5.9171e-02,
         -2.6214e-02, -5.8965e-02,  3.8284e-02, -8.5802e-03, -3.3380e-03,
         -2.2308e-02,  1.2283e-02, -5.3326e-02,  6.0772e-02,  5.0573e-02,
          2.1079e-02,  7.6679e-02, -3.7683e-03, -5.4017e-03,  4.0351e-02,
          2.9670e-03, -1.5352e-02, -7.8523e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.fc.bias | Size: torch.Size([2]) | Values : tensor([-0.0142,  0.1251], device='cuda:0', grad_fn=<SliceBackward0>)
