{'batch_size': 8, 'sequence_file': 'preprocessed_seq_ab_train_1200.npz', 'pt_file': 'pt_train_data.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 1, 'num_classes': 2, 'num_epochs': 1000, 'learning_rate': 0.0001, 'max_grad_norm': 0.1, 'validation_split': 0.1, 'early_stop_patience': 30, 'gradient_accumulation_steps': 4, 'gradient_noise_std': 0.01}
Number of GPUs available: 1
Initializing ParatopeModel with 1 self-attention layers...
Epoch [1/1000], Training Loss: 0.9609
Validation Loss: 0.7842
New best model found with Validation Loss: 0.7842
Epoch [2/1000], Training Loss: 0.7103
Validation Loss: 0.6708
New best model found with Validation Loss: 0.6708
Epoch [3/1000], Training Loss: 0.6658
Validation Loss: 0.6470
New best model found with Validation Loss: 0.6470
Epoch [4/1000], Training Loss: 0.6488
Validation Loss: 0.6359
New best model found with Validation Loss: 0.6359
Epoch [5/1000], Training Loss: 0.6461
Validation Loss: 0.6307
New best model found with Validation Loss: 0.6307
Epoch [6/1000], Training Loss: 0.6360
Validation Loss: 0.6546
No improvement in validation loss. Early stop counter: 1/30
Epoch [7/1000], Training Loss: 0.6477
Validation Loss: 0.6300
New best model found with Validation Loss: 0.6300
Epoch [8/1000], Training Loss: 0.6311
Validation Loss: 0.6307
No improvement in validation loss. Early stop counter: 1/30
Epoch [9/1000], Training Loss: 0.6311
Validation Loss: 0.6237
New best model found with Validation Loss: 0.6237
Epoch [10/1000], Training Loss: 0.6293
Validation Loss: 0.6314
No improvement in validation loss. Early stop counter: 1/30
Epoch [11/1000], Training Loss: 0.6215
Validation Loss: 0.6157
New best model found with Validation Loss: 0.6157
Epoch [12/1000], Training Loss: 0.6208
Validation Loss: 0.6147
New best model found with Validation Loss: 0.6147
Epoch [13/1000], Training Loss: 0.6182
Validation Loss: 0.6189
No improvement in validation loss. Early stop counter: 1/30
Epoch [14/1000], Training Loss: 0.6181
Validation Loss: 0.6155
No improvement in validation loss. Early stop counter: 2/30
Epoch [15/1000], Training Loss: 0.6185
Validation Loss: 0.6116
New best model found with Validation Loss: 0.6116
Epoch [16/1000], Training Loss: 0.6166
Validation Loss: 0.6207
No improvement in validation loss. Early stop counter: 1/30
Epoch [17/1000], Training Loss: 0.6170
Validation Loss: 0.6120
No improvement in validation loss. Early stop counter: 2/30
Epoch [18/1000], Training Loss: 0.6155
Validation Loss: 0.6096
New best model found with Validation Loss: 0.6096
Epoch [19/1000], Training Loss: 0.6154
Validation Loss: 0.6102
No improvement in validation loss. Early stop counter: 1/30
Epoch [20/1000], Training Loss: 0.6157
Validation Loss: 0.6112
No improvement in validation loss. Early stop counter: 2/30
Epoch [21/1000], Training Loss: 0.6118
Validation Loss: 0.6085
New best model found with Validation Loss: 0.6085
Epoch [22/1000], Training Loss: 0.6126
Validation Loss: 0.6076
New best model found with Validation Loss: 0.6076
Epoch [23/1000], Training Loss: 0.6120
Validation Loss: 0.6079
No improvement in validation loss. Early stop counter: 1/30
Epoch [24/1000], Training Loss: 0.6112
Validation Loss: 0.6078
No improvement in validation loss. Early stop counter: 2/30
Epoch [25/1000], Training Loss: 0.6116
Validation Loss: 0.6083
No improvement in validation loss. Early stop counter: 3/30
Epoch [26/1000], Training Loss: 0.6109
Validation Loss: 0.6068
New best model found with Validation Loss: 0.6068
Epoch [27/1000], Training Loss: 0.6102
Validation Loss: 0.6092
No improvement in validation loss. Early stop counter: 1/30
Epoch [28/1000], Training Loss: 0.6100
Validation Loss: 0.6090
No improvement in validation loss. Early stop counter: 2/30
Epoch [29/1000], Training Loss: 0.6108
Validation Loss: 0.6062
New best model found with Validation Loss: 0.6062
Epoch [30/1000], Training Loss: 0.6094
Validation Loss: 0.6058
New best model found with Validation Loss: 0.6058
Epoch [31/1000], Training Loss: 0.6089
Validation Loss: 0.6057
New best model found with Validation Loss: 0.6057
Epoch [32/1000], Training Loss: 0.6085
Validation Loss: 0.6060
No improvement in validation loss. Early stop counter: 1/30
Epoch [33/1000], Training Loss: 0.6086
Validation Loss: 0.6053
New best model found with Validation Loss: 0.6053
Epoch [34/1000], Training Loss: 0.6082
Validation Loss: 0.6055
No improvement in validation loss. Early stop counter: 1/30
Epoch [35/1000], Training Loss: 0.6086
Validation Loss: 0.6050
New best model found with Validation Loss: 0.6050
Epoch [36/1000], Training Loss: 0.6084
Validation Loss: 0.6054
No improvement in validation loss. Early stop counter: 1/30
Epoch [37/1000], Training Loss: 0.6085
Validation Loss: 0.6060
No improvement in validation loss. Early stop counter: 2/30
Epoch [38/1000], Training Loss: 0.6079
Validation Loss: 0.6048
New best model found with Validation Loss: 0.6048
Epoch [39/1000], Training Loss: 0.6077
Validation Loss: 0.6072
No improvement in validation loss. Early stop counter: 1/30
Epoch [40/1000], Training Loss: 0.6081
Validation Loss: 0.6046
New best model found with Validation Loss: 0.6046
Epoch [41/1000], Training Loss: 0.6074
Validation Loss: 0.6050
No improvement in validation loss. Early stop counter: 1/30
Epoch [42/1000], Training Loss: 0.6071
Validation Loss: 0.6050
No improvement in validation loss. Early stop counter: 2/30
Epoch [43/1000], Training Loss: 0.6074
Validation Loss: 0.6042
New best model found with Validation Loss: 0.6042
Epoch [44/1000], Training Loss: 0.6071
Validation Loss: 0.6045
No improvement in validation loss. Early stop counter: 1/30
Epoch [45/1000], Training Loss: 0.6070
Validation Loss: 0.6040
New best model found with Validation Loss: 0.6040
Epoch [46/1000], Training Loss: 0.6071
Validation Loss: 0.6041
No improvement in validation loss. Early stop counter: 1/30
Epoch [47/1000], Training Loss: 0.6068
Validation Loss: 0.6043
No improvement in validation loss. Early stop counter: 2/30
Epoch [48/1000], Training Loss: 0.6078
Validation Loss: 0.6039
New best model found with Validation Loss: 0.6039
Epoch [49/1000], Training Loss: 0.6066
Validation Loss: 0.6039
No improvement in validation loss. Early stop counter: 1/30
Epoch [50/1000], Training Loss: 0.6068
Validation Loss: 0.6041
No improvement in validation loss. Early stop counter: 2/30
Epoch [51/1000], Training Loss: 0.6063
Validation Loss: 0.6037
New best model found with Validation Loss: 0.6037
Epoch [52/1000], Training Loss: 0.6067
Validation Loss: 0.6037
New best model found with Validation Loss: 0.6037
Epoch [53/1000], Training Loss: 0.6065
Validation Loss: 0.6036
New best model found with Validation Loss: 0.6036
Epoch [54/1000], Training Loss: 0.6062
Validation Loss: 0.6038
No improvement in validation loss. Early stop counter: 1/30
Epoch [55/1000], Training Loss: 0.6063
Validation Loss: 0.6044
No improvement in validation loss. Early stop counter: 2/30
Epoch [56/1000], Training Loss: 0.6064
Validation Loss: 0.6035
New best model found with Validation Loss: 0.6035
Epoch [57/1000], Training Loss: 0.6067
Validation Loss: 0.6037
No improvement in validation loss. Early stop counter: 1/30
Epoch [58/1000], Training Loss: 0.6061
Validation Loss: 0.6036
No improvement in validation loss. Early stop counter: 2/30
Epoch [59/1000], Training Loss: 0.6059
Validation Loss: 0.6036
No improvement in validation loss. Early stop counter: 3/30
Epoch [60/1000], Training Loss: 0.6062
Validation Loss: 0.6035
New best model found with Validation Loss: 0.6035
Epoch [61/1000], Training Loss: 0.6058
Validation Loss: 0.6036
No improvement in validation loss. Early stop counter: 1/30
Epoch [62/1000], Training Loss: 0.6060
Validation Loss: 0.6033
New best model found with Validation Loss: 0.6033
Epoch [63/1000], Training Loss: 0.6063
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 1/30
Epoch [64/1000], Training Loss: 0.6066
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 2/30
Epoch [65/1000], Training Loss: 0.6059
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 3/30
Epoch [66/1000], Training Loss: 0.6061
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 4/30
Epoch [67/1000], Training Loss: 0.6058
Validation Loss: 0.6035
No improvement in validation loss. Early stop counter: 5/30
Epoch [68/1000], Training Loss: 0.6058
Validation Loss: 0.6034
No improvement in validation loss. Early stop counter: 6/30
Epoch [69/1000], Training Loss: 0.6058
Validation Loss: 0.6032
New best model found with Validation Loss: 0.6032
Epoch [70/1000], Training Loss: 0.6061
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 1/30
Epoch [71/1000], Training Loss: 0.6059
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 2/30
Epoch [72/1000], Training Loss: 0.6057
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 3/30
Epoch [73/1000], Training Loss: 0.6057
Validation Loss: 0.6032
New best model found with Validation Loss: 0.6032
Epoch [74/1000], Training Loss: 0.6056
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [75/1000], Training Loss: 0.6057
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 1/30
Epoch [76/1000], Training Loss: 0.6058
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 2/30
Epoch [77/1000], Training Loss: 0.6059
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 3/30
Epoch [78/1000], Training Loss: 0.6059
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 4/30
Epoch [79/1000], Training Loss: 0.6057
Validation Loss: 0.6033
No improvement in validation loss. Early stop counter: 5/30
Epoch [80/1000], Training Loss: 0.6056
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [81/1000], Training Loss: 0.6057
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 1/30
Epoch [82/1000], Training Loss: 0.6053
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [83/1000], Training Loss: 0.6054
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [84/1000], Training Loss: 0.6056
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 2/30
Epoch [85/1000], Training Loss: 0.6056
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 3/30
Epoch [86/1000], Training Loss: 0.6057
Validation Loss: 0.6032
No improvement in validation loss. Early stop counter: 4/30
Epoch [87/1000], Training Loss: 0.6058
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 5/30
Epoch [88/1000], Training Loss: 0.6057
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [89/1000], Training Loss: 0.6059
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [90/1000], Training Loss: 0.6057
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 2/30
Epoch [91/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 3/30
Epoch [92/1000], Training Loss: 0.6059
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [93/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [94/1000], Training Loss: 0.6058
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 2/30
Epoch [95/1000], Training Loss: 0.6056
Validation Loss: 0.6031
New best model found with Validation Loss: 0.6031
Epoch [96/1000], Training Loss: 0.6057
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [97/1000], Training Loss: 0.6054
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 2/30
Epoch [98/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 3/30
Epoch [99/1000], Training Loss: 0.6055
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [100/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [101/1000], Training Loss: 0.6056
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [102/1000], Training Loss: 0.6058
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 1/30
Epoch [103/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 2/30
Epoch [104/1000], Training Loss: 0.6055
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [105/1000], Training Loss: 0.6055
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 1/30
Epoch [106/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 2/30
Epoch [107/1000], Training Loss: 0.6054
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 3/30
Epoch [108/1000], Training Loss: 0.6054
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 4/30
Epoch [109/1000], Training Loss: 0.6054
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 5/30
Epoch [110/1000], Training Loss: 0.6057
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 6/30
Epoch [111/1000], Training Loss: 0.6055
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 7/30
Epoch [112/1000], Training Loss: 0.6057
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 8/30
Epoch [113/1000], Training Loss: 0.6057
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 9/30
Epoch [114/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 10/30
Epoch [115/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 11/30
Epoch [116/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 12/30
Epoch [117/1000], Training Loss: 0.6054
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [118/1000], Training Loss: 0.6056
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [119/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 1/30
Epoch [120/1000], Training Loss: 0.6058
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 2/30
Epoch [121/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 3/30
Epoch [122/1000], Training Loss: 0.6056
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [123/1000], Training Loss: 0.6055
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 1/30
Epoch [124/1000], Training Loss: 0.6055
Validation Loss: 0.6030
New best model found with Validation Loss: 0.6030
Epoch [125/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 1/30
Epoch [126/1000], Training Loss: 0.6055
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 2/30
Epoch [127/1000], Training Loss: 0.6055
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 3/30
Epoch [128/1000], Training Loss: 0.6057
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 4/30
Epoch [129/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 5/30
Epoch [130/1000], Training Loss: 0.6055
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 6/30
Epoch [131/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 7/30
Epoch [132/1000], Training Loss: 0.6059
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 8/30
Epoch [133/1000], Training Loss: 0.6053
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 9/30
Epoch [134/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 10/30
Epoch [135/1000], Training Loss: 0.6059
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 11/30
Epoch [136/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 12/30
Epoch [137/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 13/30
Epoch [138/1000], Training Loss: 0.6058
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 14/30
Epoch [139/1000], Training Loss: 0.6060
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 15/30
Epoch [140/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 16/30
Epoch [141/1000], Training Loss: 0.6055
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 17/30
Epoch [142/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 18/30
Epoch [143/1000], Training Loss: 0.6055
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 19/30
Epoch [144/1000], Training Loss: 0.6053
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 20/30
Epoch [145/1000], Training Loss: 0.6053
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 21/30
Epoch [146/1000], Training Loss: 0.6056
Validation Loss: 0.6031
No improvement in validation loss. Early stop counter: 22/30
Epoch [147/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 23/30
Epoch [148/1000], Training Loss: 0.6053
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 24/30
Epoch [149/1000], Training Loss: 0.6057
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 25/30
Epoch [150/1000], Training Loss: 0.6056
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 26/30
Epoch [151/1000], Training Loss: 0.6055
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 27/30
Epoch [152/1000], Training Loss: 0.6054
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 28/30
Epoch [153/1000], Training Loss: 0.6057
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 29/30
Epoch [154/1000], Training Loss: 0.6057
Validation Loss: 0.6030
No improvement in validation loss. Early stop counter: 30/30
Early stopping triggered after 154 epochs.
Best model saved successfully.
Layer: embedding.weight | Size: torch.Size([22, 128]) | Values : tensor([[ 2.6640e-04, -3.4898e-03, -7.9325e-04, -4.8446e-03,  3.8078e-03,
         -4.6667e-03,  3.1634e-04, -2.9629e-03, -2.1276e-03,  7.1324e-04,
         -2.4303e-03,  2.7180e-03,  4.0522e-03,  2.5120e-03, -4.4909e-03,
          1.9707e-04,  5.7339e-04,  8.2431e-04, -2.2569e-03,  4.1209e-05,
          8.2283e-04, -6.2180e-04,  2.0869e-03,  1.5698e-03,  7.0709e-04,
         -1.1170e-03, -2.0320e-04, -6.8287e-03,  1.7617e-03,  7.6005e-04,
          1.4631e-03,  5.2987e-04, -1.5989e-04, -2.2794e-03, -9.5569e-04,
         -3.4621e-04, -7.3971e-04,  5.1808e-04, -1.1306e-03, -4.5257e-03,
         -3.8149e-04, -4.7667e-03, -5.4404e-03,  3.7181e-03, -3.4204e-03,
         -1.1809e-03, -1.7527e-03,  2.6659e-03,  8.7032e-04,  1.5877e-03,
         -3.5768e-05,  2.3230e-03, -4.6235e-04,  2.3893e-04, -1.6016e-03,
          1.6163e-04,  1.8993e-03,  5.0065e-03,  2.3314e-03, -5.1459e-03,
         -1.7388e-03,  2.5851e-05,  2.1079e-03, -9.8370e-04, -1.6931e-03,
          6.7022e-03,  1.3522e-04, -2.1695e-03, -7.4037e-04, -2.4542e-04,
          1.8596e-03,  3.2444e-03, -1.6127e-03,  3.9252e-03, -2.6436e-04,
          2.2251e-03,  2.2185e-03,  7.3606e-04,  3.1651e-03,  1.9783e-03,
          2.3541e-03, -3.5443e-03,  5.5627e-03, -2.3479e-03, -3.2282e-04,
         -3.9191e-04,  6.5201e-04,  1.9607e-03,  1.2914e-03,  2.1719e-03,
         -7.5753e-04,  4.0967e-03,  1.5921e-03, -5.9561e-04, -2.0261e-03,
          3.3703e-03, -6.7313e-04,  2.9904e-03, -4.1577e-03,  5.2931e-04,
          6.3297e-04, -2.8836e-03,  1.7451e-03,  2.6310e-03,  8.7460e-04,
          3.5854e-03, -3.7948e-03,  2.4055e-03,  4.5987e-03, -2.4925e-03,
         -2.3659e-03,  3.5158e-03, -2.4069e-03, -1.7001e-03,  4.6003e-03,
         -2.9781e-04, -2.9244e-03,  1.6833e-03,  1.8022e-03, -2.7688e-03,
          6.6109e-04,  2.3772e-03,  3.5720e-03, -5.5708e-04,  3.1241e-03,
         -1.1102e-03,  1.7913e-04, -3.2945e-03],
        [-7.9952e-01,  9.5429e-01, -8.2313e-01, -2.7751e-01,  4.1633e-02,
          1.3250e+00, -1.1576e+00,  2.2313e+00, -2.1749e+00, -1.3551e+00,
          2.3807e+00, -1.8972e-01,  1.6474e+00, -1.0590e+00, -4.4242e-01,
          2.6432e+00, -3.5060e-02,  7.3318e-01, -4.1457e-01,  5.4517e-01,
         -2.2337e-01,  2.0253e+00, -1.2721e+00, -4.2120e-01,  4.0601e-01,
         -6.0988e-01, -1.2375e-01,  5.6977e-01, -9.5041e-01, -1.4520e-01,
         -2.2511e+00,  1.7462e-01, -1.7134e+00,  2.0182e+00, -1.1850e+00,
          1.1439e+00,  9.2499e-01,  3.1563e-01,  1.6111e+00,  8.6428e-03,
         -3.3195e-01,  3.6227e-01, -6.4493e-01, -4.0906e-01,  1.0073e+00,
          4.6669e-01, -1.1457e+00, -5.3093e-01,  7.3106e-01, -3.7446e-01,
          1.7564e+00,  8.1443e-01, -1.2668e+00, -7.0761e-01,  2.5784e-01,
          4.8110e-01,  2.1783e+00,  1.0385e+00, -1.0295e+00, -1.9432e+00,
          6.2548e-01,  1.9975e+00, -1.3150e+00,  6.1858e-01, -3.1144e-01,
          8.3200e-01, -1.6363e+00, -1.7020e+00, -1.5511e+00, -1.2308e-01,
          4.2901e-01, -1.7347e-01, -1.5913e-01, -1.9705e+00, -1.2616e+00,
          4.3643e-01, -1.1108e+00,  3.8556e-01, -7.9599e-01, -1.7386e+00,
          7.7533e-01, -7.3658e-01,  1.5853e+00,  2.4178e+00,  2.5289e-01,
         -1.6335e-01, -2.0187e+00, -1.2537e+00,  6.4115e-01,  1.1513e+00,
          8.2462e-01, -4.5871e-01,  4.4483e-02, -5.8107e-01,  1.2299e+00,
         -4.7884e-01,  5.6603e-01,  2.7991e-01, -2.6660e-01, -2.4809e+00,
          2.0021e+00, -5.7953e-01,  2.4024e-01,  3.5871e-01,  5.1891e-01,
         -1.0023e+00, -2.1925e-02, -2.1755e-04, -1.4075e+00, -1.8942e-01,
          4.2865e-01,  7.1046e-01,  7.9857e-02,  1.1541e+00,  4.1990e-01,
         -1.3306e+00, -1.6274e+00,  3.2006e-01,  1.0008e+00,  5.1991e-01,
         -7.7455e-01, -2.1587e-01,  3.1292e-01, -7.8622e-01,  1.1471e+00,
         -3.2865e-01,  6.7682e-01,  6.9092e-01]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.9976, 1.0018], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([1.6858e-03, 1.8656e-05], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-4.0226e-02,  4.4888e-03, -6.7512e-02, -2.6263e-02, -6.0830e-02,
         -3.1398e-02, -4.0737e-02, -4.0670e-02, -5.6207e-02,  7.6738e-02,
          6.8022e-02, -5.6380e-02, -6.5500e-02,  2.2033e-02, -7.0437e-02,
          5.4246e-02,  3.1571e-02,  2.8724e-02, -7.6821e-02,  4.1211e-02,
          4.0294e-02, -7.1981e-03, -8.0833e-02, -3.9064e-02, -3.8412e-02,
         -2.7011e-02, -5.2150e-02,  6.3182e-02,  1.4826e-03, -3.9327e-02,
         -5.3634e-02, -9.4888e-03,  1.8717e-02,  3.6380e-02,  4.6337e-02,
          2.7252e-02,  5.2810e-02, -5.3984e-02,  6.6694e-02,  4.2385e-02,
         -8.6004e-02, -1.7400e-02,  6.8409e-02, -3.9898e-02, -8.8081e-02,
         -8.7022e-02, -6.3604e-02, -4.7428e-02, -1.5067e-02,  7.9363e-03,
         -2.8779e-02, -2.7119e-02,  1.6472e-02, -7.0138e-02, -8.6293e-02,
         -3.2967e-02, -6.1148e-02,  3.9954e-02, -2.4471e-02, -4.8012e-02,
          4.2559e-02,  5.9042e-02, -7.2340e-02,  3.6234e-02, -2.3057e-02,
          9.4714e-03, -1.4984e-02,  4.8751e-02,  8.2104e-03,  3.4179e-02,
         -6.8067e-02,  4.5194e-02,  7.8287e-02, -1.1909e-02,  7.2110e-02,
         -8.0806e-02, -1.7658e-02, -2.9593e-02, -4.9739e-02, -6.9695e-02,
         -3.3466e-02, -7.5205e-02, -6.6843e-02,  2.4319e-02,  7.6553e-02,
         -6.5171e-02,  1.0124e-02, -4.1986e-02,  6.5111e-02,  2.1569e-03,
         -7.9767e-02,  6.8995e-02, -4.9245e-02,  5.1503e-02, -6.6507e-02,
         -9.7396e-03, -4.2668e-02, -3.9260e-02,  7.7327e-02,  1.6355e-02,
         -6.0832e-02, -9.8887e-03,  5.8915e-02, -5.5581e-02,  7.0607e-02,
         -8.1778e-02,  3.6436e-02, -8.5563e-02,  5.4366e-02, -7.7592e-02,
         -3.0532e-02, -5.9554e-02, -3.9038e-02,  5.5563e-03,  1.5127e-02,
         -7.1730e-02, -6.9519e-02, -2.4637e-02,  7.4393e-02, -7.0522e-02,
          3.5615e-02,  3.8565e-02,  3.2262e-02,  4.4407e-02, -5.6273e-02,
          5.4353e-02, -2.5835e-02, -1.1536e-02],
        [ 7.0602e-02, -3.3237e-02,  2.7885e-02,  3.9298e-02,  1.0823e-02,
         -8.1943e-02,  4.7907e-02, -3.3482e-02,  6.6413e-02, -7.8146e-02,
         -4.9802e-02,  6.7115e-02, -7.7468e-02, -8.4990e-02,  2.9022e-02,
         -6.4828e-02, -2.4506e-02, -5.1858e-02,  2.0342e-02, -8.3126e-02,
          3.3728e-02, -2.4372e-02,  6.1498e-02,  5.8392e-02,  5.9637e-02,
         -5.5918e-03, -4.1196e-02, -5.8883e-02, -6.6844e-02, -8.3339e-02,
         -3.9605e-02,  7.9972e-02, -8.1670e-02,  4.3741e-02,  1.9759e-02,
         -6.8700e-02, -6.2979e-02,  8.2980e-03, -5.1832e-02, -5.0201e-02,
         -1.4043e-02, -7.2019e-02,  4.3169e-02, -3.1863e-02, -6.9637e-02,
          8.2261e-02, -4.4738e-03, -5.7208e-02, -7.2905e-02, -4.0748e-02,
         -8.3336e-02,  3.1483e-02, -6.4531e-02,  3.7305e-02, -8.2151e-02,
          2.9108e-02,  7.1357e-02,  5.8370e-03,  4.6350e-02, -7.7895e-02,
         -8.4243e-03,  1.8536e-03, -5.8370e-02,  4.7284e-03, -6.8474e-03,
          6.9730e-02, -7.8963e-02, -3.3219e-02, -2.7733e-02, -2.6691e-02,
          3.8965e-02,  6.2563e-03,  2.7020e-02,  8.4217e-02,  8.3260e-02,
         -5.2487e-02, -2.9945e-02,  9.0931e-03, -9.4182e-03,  7.5363e-02,
          3.4146e-02, -7.7693e-02,  3.6698e-02,  5.3453e-02,  6.0189e-03,
          6.7506e-02, -7.4426e-02,  2.6231e-02, -4.1116e-02, -1.5072e-02,
         -3.6684e-02, -2.9365e-02,  8.4674e-02,  4.6411e-03,  6.9059e-02,
         -2.5586e-02,  6.6033e-02, -8.6462e-02,  2.4636e-02, -8.5066e-02,
         -4.2798e-02,  6.5839e-02, -4.3014e-02,  3.5828e-02, -7.9254e-02,
          6.0687e-02,  8.4886e-02,  3.6419e-02,  4.7211e-02,  2.5811e-02,
         -7.4172e-02, -1.1470e-02,  2.1101e-02,  4.1344e-02, -1.9313e-05,
         -1.5776e-02, -9.4072e-03, -1.3352e-02,  7.8717e-02,  9.6334e-03,
          2.4341e-02, -6.9274e-02,  6.0158e-04, -3.6013e-02,  5.7211e-02,
          3.1416e-02,  1.2649e-02, -3.0400e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[-0.0108, -0.0444,  0.0080, -0.0848, -0.0601, -0.0827, -0.0205,  0.0217,
         -0.0603, -0.0151, -0.0097,  0.0051,  0.0787, -0.0349,  0.0080,  0.0532,
         -0.0301,  0.0773,  0.0374,  0.0123,  0.0356,  0.0225,  0.0771, -0.0254,
          0.0430,  0.0378, -0.0490, -0.0842,  0.0673, -0.0059, -0.0736,  0.0041,
         -0.0280, -0.0066,  0.0238,  0.0518, -0.0815,  0.0451,  0.0719,  0.0219,
          0.0265,  0.0843, -0.0688,  0.0702, -0.0461,  0.0861,  0.0840, -0.0347,
          0.0800, -0.0002, -0.0181,  0.0241, -0.0134, -0.0093, -0.0622,  0.0527,
         -0.0019,  0.0378, -0.0580, -0.0279,  0.0713, -0.0100, -0.0587, -0.0060,
          0.0261,  0.0337, -0.0341, -0.0715,  0.0784, -0.0673,  0.0328, -0.0772,
         -0.0146, -0.0129, -0.0733, -0.0908,  0.0246,  0.0322, -0.0022,  0.0774,
          0.0554, -0.0790, -0.0335, -0.0118, -0.0276, -0.0150,  0.0895,  0.0112,
         -0.0023,  0.0681,  0.0399, -0.0910,  0.0541, -0.0538, -0.0432, -0.0893,
          0.0685,  0.0872, -0.0587,  0.0359,  0.0323, -0.0812, -0.0662,  0.0206,
          0.0101, -0.0471,  0.0388, -0.0110, -0.0295, -0.0704, -0.0381,  0.0247,
         -0.0534, -0.0780, -0.0030, -0.0407,  0.0824, -0.0011,  0.0422, -0.0298,
          0.0904,  0.0736,  0.0206, -0.0561, -0.0207,  0.0115,  0.0469,  0.0611],
        [ 0.0717, -0.0503,  0.0158,  0.0758, -0.0623,  0.0383, -0.0173,  0.0714,
          0.0679,  0.0558,  0.0088, -0.0744,  0.0770, -0.0827, -0.0363, -0.0121,
          0.0679,  0.0254, -0.0267,  0.0251, -0.0113, -0.0732, -0.0068,  0.0077,
         -0.0129, -0.0539, -0.0103,  0.0779,  0.0501,  0.0159, -0.0832, -0.0200,
         -0.0185, -0.0083, -0.0352,  0.0052, -0.0075, -0.0251, -0.0036, -0.0836,
          0.0645, -0.0572, -0.0791, -0.0557, -0.0292, -0.0176,  0.0085,  0.0794,
          0.0520, -0.0376, -0.0213, -0.0731, -0.0379,  0.0037, -0.0425, -0.0754,
          0.0207, -0.0116,  0.0622,  0.0882, -0.0232, -0.0278,  0.0319,  0.0163,
         -0.0164,  0.0202, -0.0795, -0.0524,  0.0787,  0.0097,  0.0389, -0.0835,
         -0.0098, -0.0865, -0.0699, -0.0810,  0.0750, -0.0337, -0.0072, -0.0273,
         -0.0806, -0.0619, -0.0239, -0.0816, -0.0369,  0.0452, -0.0597, -0.0804,
         -0.0535, -0.0137,  0.0562,  0.0110,  0.0053,  0.0480, -0.0736,  0.0743,
         -0.0316, -0.0581, -0.0786,  0.0610,  0.0333,  0.0831, -0.0449, -0.0037,
          0.0478,  0.0328, -0.0448,  0.0467, -0.0899,  0.0209, -0.0701, -0.0474,
         -0.0863,  0.0173, -0.0609, -0.0667,  0.0812,  0.0071,  0.0085,  0.0588,
         -0.0026,  0.0597,  0.0339, -0.0144, -0.0580,  0.0060, -0.0375, -0.0520]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.0396, -0.0266,  0.0035,  ..., -0.0399,  0.0377, -0.0223],
        [ 0.0060, -0.0124, -0.0172,  ...,  0.0198,  0.0081,  0.0350]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([0.0270, 0.0028], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 4.1251e-03, -7.6219e-03,  2.4756e-03,  1.7767e-03,  7.0303e-03,
         -4.9840e-03, -4.3160e-03,  7.3225e-03,  6.6824e-03, -6.8842e-03,
          7.9529e-03,  6.7748e-03,  6.3705e-03,  2.8789e-03,  8.6441e-03,
         -8.1336e-03,  6.6184e-03,  4.3327e-03,  1.7486e-03, -7.6966e-03,
          2.3291e-03,  5.5213e-03, -7.9155e-03,  8.3421e-03,  2.9336e-03,
         -4.2028e-03,  6.4899e-03,  7.2127e-03,  6.4396e-03, -6.3883e-04,
          6.2858e-03, -7.2084e-03, -6.6410e-03,  5.7175e-03,  4.2186e-03,
         -6.3373e-03, -5.5039e-03,  6.1001e-03,  7.3942e-03, -7.3850e-03,
          6.4237e-03, -3.7859e-03,  6.1876e-03, -1.2788e-03, -6.9520e-03,
          2.6053e-03, -6.0954e-03, -7.4420e-03,  5.1197e-03,  6.9100e-03,
          4.3497e-03,  6.8769e-03,  4.5087e-03,  7.7149e-04, -5.1928e-03,
          6.4866e-03, -5.0932e-03, -5.0848e-03, -5.4275e-03, -4.6387e-03,
          7.3980e-03,  3.9708e-03,  3.7300e-03, -7.5098e-03, -5.7860e-03,
         -5.6189e-03, -8.0508e-03,  6.8126e-03,  7.8251e-03, -6.9156e-03,
         -2.1710e-03, -7.5687e-03, -7.7649e-03,  5.8659e-03,  6.4131e-03,
          4.9503e-03, -7.6415e-03, -6.0865e-03,  3.9228e-03,  6.0578e-03,
         -6.3156e-03,  3.2740e-03, -2.9465e-03,  5.9284e-03, -6.0898e-03,
         -7.2779e-03,  6.2836e-03, -3.0327e-03, -1.1977e-03, -5.6344e-03,
         -5.2745e-03, -3.0327e-03,  1.3970e-03, -6.7389e-03, -2.0914e-03,
         -7.4161e-04,  9.2554e-03,  7.2390e-03, -5.8855e-03, -7.6569e-03,
         -4.7516e-03, -1.0169e-02,  6.9606e-03,  7.1109e-03,  3.9544e-03,
          5.2749e-03,  6.0520e-03,  8.6793e-03,  4.2908e-03, -2.3113e-03,
         -6.2597e-03, -7.6137e-03,  6.5686e-03, -6.8544e-03, -9.0379e-03,
         -7.2580e-03,  1.4689e-03, -5.0970e-03,  4.0800e-03, -6.7651e-03,
          8.5417e-03,  5.8312e-03,  1.4612e-03,  7.4624e-03, -7.7449e-03,
          1.9772e-03, -9.3737e-04,  6.9828e-03],
        [-3.1901e-03, -1.4746e-02, -3.5968e-03, -2.7366e-03,  1.6737e-02,
         -5.8790e-03,  4.3459e-04,  1.5327e-03,  5.2670e-03, -4.1931e-03,
         -1.8953e-03,  7.3056e-03,  6.8549e-03,  8.5892e-03,  8.4433e-03,
         -1.2475e-02,  8.4217e-03,  7.2369e-03,  4.1105e-03, -1.1395e-02,
          4.2762e-03,  6.6075e-03, -1.2437e-02,  1.5031e-02,  6.7188e-03,
         -6.3332e-03,  9.8447e-03,  6.0380e-03,  9.8122e-03,  5.0698e-03,
          6.8977e-03, -1.2217e-02, -7.1091e-03,  6.5089e-03,  4.7825e-03,
         -6.3323e-03, -1.3049e-02,  6.1655e-03,  1.7406e-03, -1.2929e-02,
          1.2492e-02, -5.3988e-03,  4.9771e-03,  8.2575e-03, -8.6315e-03,
         -2.2806e-03, -1.2240e-02, -1.2831e-02,  9.9297e-03,  1.4007e-02,
          9.8835e-03,  1.0722e-02,  1.1482e-02,  1.1124e-03, -4.3610e-03,
          9.7813e-03, -3.6051e-03,  1.3822e-03, -4.3931e-04, -1.7941e-03,
          3.6993e-03,  7.1042e-03, -1.4247e-03, -1.2958e-02, -1.3686e-02,
          2.4986e-03, -1.9765e-02,  8.5791e-03,  1.6639e-02, -1.6714e-02,
          1.6553e-03, -1.9683e-03, -1.6523e-02,  1.1761e-02,  1.0522e-02,
          3.9935e-03, -5.9983e-03, -6.0261e-03,  7.6363e-03,  1.0151e-02,
         -3.1174e-03, -5.9515e-03,  5.7426e-04,  1.1633e-02, -2.5855e-03,
         -1.6382e-02,  1.0704e-02, -1.1085e-04,  2.2174e-03, -9.7593e-03,
         -2.2524e-03,  2.2793e-03,  3.1893e-03, -1.0418e-02, -7.1213e-03,
          3.5897e-04,  1.8020e-02,  6.5817e-03, -1.4065e-02, -1.7178e-02,
         -9.2297e-03, -2.0788e-02,  8.6248e-03,  1.4490e-02,  1.1419e-02,
          6.8157e-03,  2.2913e-03,  1.3567e-02,  7.7046e-03, -9.8439e-03,
         -1.3029e-02, -7.1560e-03,  6.6011e-03, -6.1212e-03, -7.0496e-03,
         -1.3810e-02,  7.3192e-03, -1.3622e-03,  1.7768e-03, -9.4246e-03,
          1.6342e-02,  4.5582e-03,  9.6050e-04,  1.0593e-02, -1.3919e-02,
          7.8281e-05, -2.4088e-03, -2.0371e-03]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.9987, 0.9979], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.9952, 0.9991], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.row_attn.attn.norm.bias | Size: torch.Size([128]) | Values : tensor([0.0016, 0.0009], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([1.0056, 1.0083], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([-9.6038e-05,  2.5533e-03], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 0.0120,  0.0488, -0.0595,  0.0028,  0.0500, -0.0541,  0.0995,  0.0010,
         -0.0672, -0.0487,  0.0109, -0.0704,  0.0063,  0.0361, -0.0234,  0.0502,
         -0.0890, -0.0797,  0.0725, -0.0141,  0.0417, -0.0900, -0.0651,  0.0119,
         -0.0698, -0.0059, -0.0465, -0.0951,  0.0484, -0.0079,  0.0283,  0.0805,
          0.0984, -0.0256, -0.0841, -0.0422,  0.0443,  0.0647, -0.0743, -0.0750,
          0.0683, -0.0705, -0.0157,  0.0652, -0.0422,  0.0556, -0.0744,  0.0489,
         -0.0193,  0.0817, -0.0884,  0.0512, -0.0677, -0.0114, -0.0039, -0.0210,
          0.0885,  0.0130,  0.0159, -0.0117, -0.0291,  0.0098,  0.0070,  0.0535,
         -0.0652, -0.0261, -0.0238, -0.0755, -0.0125,  0.0684,  0.0337, -0.0065,
          0.0470,  0.0334,  0.0835, -0.0612,  0.0202, -0.0395,  0.0085,  0.0850,
         -0.0193, -0.0636,  0.0676,  0.0412, -0.0189,  0.0084, -0.0092,  0.0810,
          0.0581,  0.0581,  0.0217, -0.0267,  0.0288, -0.0474,  0.0771,  0.0477,
         -0.0587, -0.0237, -0.0677, -0.0106,  0.0659,  0.0556,  0.0411,  0.0802,
         -0.0176,  0.0581,  0.0188, -0.0512,  0.0437,  0.0694, -0.0780, -0.0142,
         -0.0166, -0.0029,  0.0945,  0.0703, -0.0033, -0.0173, -0.0337,  0.0069,
          0.0281,  0.0051, -0.0856, -0.0814,  0.0820,  0.0805,  0.0032, -0.0175],
        [ 0.0367, -0.0998,  0.0024, -0.0918, -0.0508, -0.1098,  0.0457,  0.1090,
         -0.0310,  0.0032,  0.0302, -0.0416,  0.0169, -0.0069, -0.0761,  0.0308,
          0.0816, -0.0368, -0.0555, -0.0090,  0.0275,  0.0912, -0.0610,  0.0510,
          0.0501, -0.0773,  0.0979,  0.0643,  0.0641, -0.0367,  0.0170, -0.1186,
          0.0222, -0.0065,  0.0677, -0.0115,  0.0882,  0.0961,  0.0417, -0.0834,
         -0.0550, -0.0726, -0.0178, -0.0394,  0.0127,  0.0280,  0.0264, -0.0366,
          0.0750,  0.0865, -0.0330,  0.0718, -0.0240, -0.0685, -0.0802, -0.0447,
         -0.0819, -0.0300, -0.0212, -0.0189,  0.0873,  0.0142, -0.0208,  0.0595,
          0.1100, -0.0710, -0.0282,  0.0343, -0.0111,  0.0774, -0.0379, -0.0620,
         -0.0475, -0.0415, -0.0564, -0.0303,  0.0591,  0.0515,  0.0534, -0.0509,
         -0.1157,  0.0664,  0.0055, -0.0843, -0.0247,  0.0074, -0.0125, -0.0477,
          0.0530, -0.0328, -0.0644, -0.0742, -0.0797,  0.0669,  0.0937, -0.0015,
         -0.0135, -0.0635, -0.1057,  0.0156, -0.0404,  0.0100,  0.0131, -0.0473,
          0.0898,  0.0468,  0.0017,  0.0029,  0.0772, -0.0702,  0.0033, -0.0685,
          0.0592, -0.0931, -0.0480,  0.1040, -0.0697,  0.0180,  0.0555, -0.0440,
         -0.0745, -0.0129,  0.0002, -0.0613, -0.0731,  0.0583,  0.0325,  0.0349]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[-4.6155e-03,  1.3339e-02, -5.6577e-02, -5.7668e-02, -2.1415e-02,
         -7.0870e-02,  3.7421e-02,  6.0071e-02, -6.8507e-03, -1.7435e-02,
          2.5255e-02, -6.0313e-02, -1.6741e-02, -4.6124e-02,  4.2038e-02,
         -3.1106e-02, -2.7921e-02,  1.2673e-02, -4.4602e-02, -5.3377e-02,
         -1.4386e-02, -1.7572e-02, -5.9145e-02, -3.8752e-02,  2.2090e-02,
          7.9732e-02,  4.3302e-02, -1.2420e-02, -2.9413e-02, -4.2160e-02,
          5.5486e-02, -1.1460e-02,  6.3258e-02, -7.9568e-02, -9.8048e-03,
          3.4722e-02, -3.1061e-02,  7.1324e-02,  3.1608e-02,  5.8054e-03,
         -4.3695e-03,  3.1504e-02, -9.5698e-02,  5.8760e-03, -8.5593e-02,
          1.3519e-02,  2.9456e-02, -3.0562e-02, -7.3108e-02, -4.5305e-02,
         -6.6684e-02,  4.3652e-02, -6.3334e-02,  3.2231e-02,  9.0790e-04,
         -5.2947e-02, -1.2686e-02,  3.6486e-03, -3.2245e-03,  2.6633e-02,
         -6.4580e-02,  6.6153e-03, -1.5788e-02,  3.9950e-02,  1.0867e-01,
          8.0114e-02, -2.0847e-02, -5.5400e-02,  5.9868e-02,  4.6047e-02,
         -7.5501e-02, -7.3512e-02, -1.0411e-01,  2.7141e-03, -5.0250e-02,
         -7.0976e-02,  2.6922e-02, -7.8427e-02, -2.3864e-02, -1.4218e-03,
         -5.9371e-02,  8.7432e-02, -1.6223e-02,  6.6607e-02,  1.4763e-02,
         -6.2082e-02, -3.3441e-03,  2.3634e-02, -1.7763e-02, -3.5324e-02,
          6.4006e-03,  5.0969e-02, -2.0943e-02, -5.9214e-02,  5.8415e-02,
          6.3409e-02, -5.8542e-02, -4.1167e-02, -5.5491e-02,  6.4616e-02,
         -5.5285e-02, -5.3968e-02, -7.6208e-02, -1.9544e-02, -1.7198e-02,
         -5.0728e-02,  4.1062e-02, -1.1226e-02,  9.8721e-02,  1.3097e-02,
          5.5279e-02, -6.4838e-02, -3.3532e-02, -3.4090e-02,  7.6819e-02,
          3.0561e-02, -7.5219e-02, -5.1546e-02,  7.7357e-02,  8.1973e-02,
         -7.4670e-02,  3.1441e-03, -4.5492e-02,  4.6265e-02, -8.5300e-02,
          3.8952e-02,  4.3933e-02,  3.9668e-02],
        [-5.6709e-02, -7.9838e-03,  3.5453e-02,  1.1103e-02,  7.4283e-02,
          2.7494e-02, -7.4223e-02,  6.5523e-02,  4.7301e-02, -4.4241e-02,
         -2.1410e-02,  9.5178e-02,  9.5372e-03, -8.6220e-02, -6.2455e-02,
          8.4553e-02, -4.1431e-02, -3.3750e-02,  9.6009e-02,  7.7896e-02,
         -3.4822e-02, -1.4847e-02,  4.9332e-02,  6.3805e-02, -7.4828e-02,
          7.5241e-02, -5.3534e-02,  6.3419e-02, -5.6956e-02, -1.9432e-02,
         -2.5205e-02, -4.8113e-02,  5.8867e-02, -5.8415e-02, -6.3090e-02,
         -8.1742e-02,  5.4219e-02, -2.9186e-02, -2.9231e-02, -7.3902e-02,
          2.1580e-02,  7.7004e-03,  2.6295e-02, -3.3124e-02, -7.0892e-02,
         -3.2724e-02, -9.7047e-02, -1.1312e-02,  4.4479e-02,  3.9638e-03,
         -7.4130e-03, -7.1979e-02, -5.3540e-02,  8.8022e-02,  7.7093e-02,
         -8.4887e-02,  4.6553e-02, -4.2704e-02, -9.8865e-02,  1.0385e-02,
          4.0505e-02,  6.4519e-02, -2.8113e-02, -2.2051e-02,  1.7470e-02,
         -8.3261e-02, -1.4162e-02, -7.8055e-02,  1.3646e-02,  5.1637e-02,
          1.1703e-02,  4.1974e-02,  3.5533e-02,  5.8994e-02,  6.4450e-03,
          2.5137e-02,  7.4322e-02, -2.3777e-02, -5.1353e-02,  4.2850e-02,
          3.4645e-02,  3.3931e-02, -1.5973e-02,  7.4115e-02, -3.7751e-02,
          2.9748e-02,  5.8524e-02,  3.9316e-02,  8.0388e-02, -7.5143e-02,
          1.6050e-02, -2.1434e-02,  3.5708e-02, -1.9665e-02, -5.1167e-02,
          6.1436e-02, -9.0269e-02,  2.9502e-02,  1.0928e-02,  4.4671e-02,
          5.1978e-02, -8.8750e-02, -1.6857e-02,  5.5078e-02, -6.5453e-02,
          2.4568e-02,  9.2894e-03,  5.4576e-02,  1.4689e-02, -1.2479e-01,
         -3.2969e-02, -2.7519e-02,  8.2552e-02,  5.8521e-02,  6.6735e-02,
         -2.7970e-02,  6.1187e-02, -9.9484e-05, -3.7994e-03,  5.4781e-02,
          1.9610e-02, -3.6273e-02, -2.9399e-03,  1.8848e-02,  5.1932e-03,
         -5.1859e-02, -3.3263e-02,  7.7529e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.0243, -0.0356,  0.0305,  ...,  0.0292, -0.0257, -0.0054],
        [-0.0051, -0.0492, -0.0063,  ..., -0.0327,  0.0188,  0.0171]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([0.0088, 0.0015], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 2.7950e-03,  2.0040e-03, -2.9530e-03, -8.8400e-03, -1.5567e-03,
         -1.3772e-03,  1.3937e-03, -6.1119e-04,  9.3042e-03,  2.7596e-03,
          3.4895e-03,  2.0763e-03,  1.0243e-03,  6.3191e-03,  5.1434e-03,
          4.4047e-03, -2.8321e-03,  2.1961e-04, -3.7998e-03,  7.3870e-03,
         -5.4547e-03, -4.1487e-03,  8.9316e-04,  2.2279e-03, -5.4355e-03,
          1.3936e-03, -9.6474e-05, -2.3468e-03, -4.2779e-03,  1.3545e-04,
          2.6341e-03,  1.7076e-03,  2.8025e-03, -8.5046e-04, -4.3748e-04,
         -2.4269e-03,  4.0044e-03, -3.1542e-03, -6.3787e-03, -5.5015e-04,
          5.1468e-03,  9.7297e-03,  6.0383e-03, -1.0308e-02, -8.2101e-03,
         -3.2322e-03,  6.2881e-03, -1.2700e-04, -2.4385e-03, -5.1122e-03,
         -8.3750e-03,  1.0628e-03,  1.0398e-03,  3.2567e-03, -9.6056e-04,
         -3.6613e-03, -6.7370e-03,  1.6758e-04,  4.0234e-03, -1.0027e-04,
         -4.8548e-03, -3.8618e-03,  5.8990e-03,  3.9580e-03, -1.0657e-02,
          3.8209e-03,  3.8540e-04, -1.8337e-03, -3.6768e-03,  2.0785e-03,
          5.6943e-03,  7.0939e-03,  1.6780e-03, -5.6549e-04, -3.2727e-03,
          1.9590e-03, -4.2803e-04,  2.6432e-03,  2.9940e-03, -9.7276e-03,
         -2.7884e-03, -4.9069e-03, -2.0807e-03,  3.5305e-03, -3.4721e-03,
          2.3946e-03, -8.9259e-04, -1.4759e-03,  6.6093e-04, -2.0865e-03,
         -2.6183e-03,  4.6243e-03,  3.0082e-03,  3.0582e-03,  2.8664e-03,
          9.7919e-04, -7.1695e-03,  3.7189e-03,  5.5697e-03, -1.5637e-03,
          3.2196e-03,  1.6366e-03, -3.2423e-03, -5.4213e-03,  5.3377e-04,
         -7.1128e-03,  5.0025e-03, -3.0833e-03, -8.3431e-04,  3.2137e-03,
         -3.0822e-03,  1.4663e-02, -4.3368e-03,  2.9092e-03, -6.8962e-04,
          2.6443e-03, -3.9006e-03, -4.8406e-03, -6.3891e-03,  5.9119e-03,
         -3.5307e-03, -1.6232e-03,  1.4272e-02, -7.4098e-03,  1.5773e-03,
         -9.5467e-03,  2.4945e-04,  4.4921e-03],
        [-4.5931e-03, -1.2269e-02, -7.0989e-03,  3.7633e-03,  7.0084e-03,
         -7.0297e-03, -1.2384e-03, -1.8244e-04,  7.2826e-03, -1.0751e-02,
         -2.6995e-03,  5.5145e-03,  2.5997e-03,  1.6231e-02,  1.0733e-03,
         -2.4918e-02,  4.3442e-03,  7.1034e-03, -4.2917e-03, -1.7276e-02,
          8.6490e-03, -1.8634e-03, -1.5303e-03,  7.4591e-03,  2.9769e-03,
          1.0267e-02, -1.5092e-03,  2.4213e-03,  9.0889e-03, -1.3198e-02,
          5.9593e-03, -4.9676e-03,  1.6395e-02, -1.3814e-02, -7.5719e-03,
         -2.1991e-04, -1.0119e-02,  1.2870e-02,  1.4243e-02, -1.3367e-03,
         -8.0771e-03,  3.0150e-03, -8.3431e-03, -6.3791e-03,  4.4588e-03,
         -1.1894e-03,  5.8154e-03,  1.0141e-02,  5.2008e-03,  7.5639e-05,
          4.4283e-03,  4.2281e-04, -1.0471e-02, -7.1980e-03, -1.1007e-02,
          1.2951e-02,  1.0002e-02,  1.3503e-03, -4.8480e-03, -5.8438e-03,
         -3.5131e-03,  5.0018e-03, -8.9901e-03, -7.7718e-03,  2.5472e-02,
         -1.8006e-02,  5.6515e-03,  1.2248e-02,  1.3973e-02,  4.0927e-03,
         -3.7420e-03, -1.1577e-02, -1.1250e-02,  7.4335e-03,  1.4901e-02,
         -1.3612e-02, -9.9477e-03, -7.5320e-03, -1.3136e-02,  2.4889e-02,
         -5.8164e-03, -2.6879e-03,  3.5486e-03, -1.1495e-02,  1.6820e-02,
         -2.3299e-03,  6.8093e-03,  4.5378e-03,  4.8039e-03,  2.0413e-02,
          6.7302e-04, -9.0037e-03, -4.2737e-03, -1.3187e-03,  9.8020e-03,
         -2.6202e-03, -5.6544e-04, -1.1436e-03, -5.8583e-03, -2.3410e-03,
          8.0171e-03, -1.5074e-02,  1.1809e-03, -4.9390e-03,  4.6902e-04,
          3.5587e-03,  3.4127e-03,  1.8127e-02,  1.0859e-02,  1.1095e-02,
         -1.3185e-02, -1.5355e-02,  7.9065e-03, -9.1713e-03, -4.7886e-03,
          1.2905e-02,  1.2320e-02,  9.1153e-03,  1.3001e-02, -1.2261e-02,
          4.9200e-04,  1.2496e-03,  4.8602e-03,  2.1574e-03, -1.8340e-02,
          7.3001e-03, -2.0109e-04, -1.5077e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.9992, 1.0093], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.norm.weight | Size: torch.Size([128]) | Values : tensor([1.0103, 1.0164], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: self_attention_layers.0.col_attn.attn.norm.bias | Size: torch.Size([128]) | Values : tensor([0.0002, 0.0004], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: fc.weight | Size: torch.Size([2, 128]) | Values : tensor([[ 0.0493,  0.0091, -0.0198, -0.0126,  0.0818, -0.0462, -0.0264, -0.0036,
          0.0153, -0.0594,  0.0312,  0.0152, -0.0168, -0.0766,  0.0467,  0.0439,
          0.0021,  0.0276, -0.0361, -0.0080,  0.0664,  0.0010, -0.0632,  0.0332,
          0.0566,  0.0385, -0.0398,  0.0742,  0.0565,  0.0215, -0.0071, -0.0037,
         -0.0328,  0.0120, -0.0716, -0.0807, -0.0777, -0.0665, -0.0155,  0.0122,
          0.0005, -0.0225,  0.0434, -0.0314, -0.0730,  0.0030,  0.0147,  0.0266,
         -0.0346, -0.0391, -0.0161,  0.0385,  0.0557,  0.0227, -0.0006,  0.0215,
         -0.0552,  0.0143,  0.0531,  0.0361, -0.0409, -0.0584, -0.0821,  0.0523,
         -0.0515, -0.0264, -0.0442,  0.0508, -0.0216, -0.0489, -0.0484,  0.0769,
          0.0170,  0.0136,  0.0377,  0.0459, -0.0089, -0.0323,  0.0344, -0.0771,
         -0.0716,  0.0681, -0.0152, -0.0230, -0.0687, -0.0516,  0.0486, -0.0753,
          0.0236,  0.0002, -0.0422,  0.0510, -0.0461, -0.0794,  0.0212,  0.0229,
          0.0424,  0.0392, -0.0280, -0.0782, -0.0424, -0.0579,  0.0198, -0.0161,
         -0.0814, -0.0187,  0.0555,  0.0708,  0.0696,  0.0445, -0.0698,  0.0606,
         -0.0519, -0.0609, -0.0343,  0.0379, -0.0769,  0.0311, -0.0204, -0.0569,
         -0.0341,  0.0038,  0.0819, -0.0318, -0.0384, -0.0453, -0.0798, -0.0659],
        [ 0.0407, -0.0737, -0.0591, -0.0208,  0.0688, -0.0228, -0.0007, -0.0409,
         -0.0605,  0.0830,  0.0057,  0.0663, -0.0441,  0.0206,  0.0249, -0.0149,
          0.0711,  0.0565, -0.0018,  0.0636, -0.0007, -0.0771, -0.0555,  0.0830,
         -0.0387, -0.0033, -0.0566, -0.0748,  0.0802, -0.0025, -0.0730,  0.0151,
         -0.0992,  0.0106, -0.0734,  0.0364, -0.0210, -0.0347,  0.0578,  0.0525,
         -0.0363,  0.0425,  0.0054, -0.0421,  0.0738, -0.0213, -0.0530,  0.0422,
         -0.0394, -0.0327,  0.0184, -0.0346, -0.0031, -0.0877,  0.0338, -0.0210,
          0.0792,  0.0547,  0.0795,  0.0849,  0.0139, -0.0566, -0.0289,  0.0406,
         -0.0763,  0.0336,  0.0559, -0.0463, -0.0771,  0.0486,  0.0286,  0.0349,
         -0.0700,  0.0544, -0.0475,  0.0792, -0.0364,  0.0013, -0.0747,  0.0719,
         -0.0622, -0.0247, -0.0147, -0.0377,  0.0044,  0.0570, -0.0002,  0.0351,
         -0.0021,  0.0261, -0.0660, -0.0077, -0.0622,  0.0731, -0.0225, -0.0394,
         -0.0217, -0.0800, -0.0503, -0.0468,  0.0059, -0.0539, -0.0684,  0.0138,
         -0.0545,  0.0117, -0.0629,  0.0358, -0.0816,  0.0011,  0.0051,  0.0480,
          0.0687,  0.0334, -0.0240,  0.0809, -0.0238, -0.0403, -0.0745, -0.0931,
         -0.0252, -0.0582, -0.0554, -0.0109,  0.0252,  0.0875,  0.0470,  0.0467]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: fc.bias | Size: torch.Size([2]) | Values : tensor([-0.0198,  0.0079], device='cuda:0', grad_fn=<SliceBackward0>)
