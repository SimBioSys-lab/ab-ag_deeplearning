{'batch_size': 16, 'sequence_file': 'preprocessed_seq_ab_1200.npz', 'pt_file': 'cdrs_output.csv', 'seq_len': 1200, 'vocab_size': 22, 'embed_dim': 128, 'num_heads': 8, 'num_layers': 1, 'num_classes': 2, 'num_epochs': 20, 'learning_rate': 0.003, 'n_splits': 3, 'max_grad_norm': 0.5}
Number of GPUs available: 2
Starting fold 1/3
Initializing ParatopeModel with 1 repeated layers...
Using 2 GPUs with DataParallel.
Fold 1, Epoch [1/20], Training Loss: 5.6863
Fold 1, Epoch [2/20], Training Loss: 0.6699
Fold 1, Epoch [3/20], Training Loss: 0.6731
Fold 1, Epoch [4/20], Training Loss: 0.6302
Fold 1, Epoch [5/20], Training Loss: 0.6252
Fold 1, Epoch [6/20], Training Loss: 0.5820
Fold 1, Epoch [7/20], Training Loss: 0.5576
Fold 1, Epoch [8/20], Training Loss: 0.5420
Fold 1, Epoch [9/20], Training Loss: 0.5104
Fold 1, Epoch [10/20], Training Loss: 0.4912
Fold 1, Epoch [11/20], Training Loss: 0.4591
Fold 1, Epoch [12/20], Training Loss: 0.4421
Fold 1, Epoch [13/20], Training Loss: 0.4278
Fold 1, Epoch [14/20], Training Loss: 0.4190
Fold 1, Epoch [15/20], Training Loss: 0.4125
Fold 1, Epoch [16/20], Training Loss: 0.4020
Fold 1, Epoch [17/20], Training Loss: 0.3933
Fold 1, Epoch [18/20], Training Loss: 0.3856
Fold 1, Epoch [19/20], Training Loss: 0.3806
Fold 1, Epoch [20/20], Training Loss: 0.3744
Fold 1 Validation Loss: 0.4351
New best model found at fold 1 with validation loss: 0.4351
Starting fold 2/3
Initializing ParatopeModel with 1 repeated layers...
Using 2 GPUs with DataParallel.
Fold 2, Epoch [1/20], Training Loss: 5.7543
Fold 2, Epoch [2/20], Training Loss: 0.6675
Fold 2, Epoch [3/20], Training Loss: 0.6378
Fold 2, Epoch [4/20], Training Loss: 0.6256
Fold 2, Epoch [5/20], Training Loss: 0.5853
Fold 2, Epoch [6/20], Training Loss: 0.5510
Fold 2, Epoch [7/20], Training Loss: 0.5240
Fold 2, Epoch [8/20], Training Loss: 0.5104
Fold 2, Epoch [9/20], Training Loss: 0.4912
Fold 2, Epoch [10/20], Training Loss: 0.4786
Fold 2, Epoch [11/20], Training Loss: 0.4371
Fold 2, Epoch [12/20], Training Loss: 0.4263
Fold 2, Epoch [13/20], Training Loss: 0.4149
Fold 2, Epoch [14/20], Training Loss: 0.4058
Fold 2, Epoch [15/20], Training Loss: 0.3987
Fold 2, Epoch [16/20], Training Loss: 0.3914
Fold 2, Epoch [17/20], Training Loss: 0.3837
Fold 2, Epoch [18/20], Training Loss: 0.3749
Fold 2, Epoch [19/20], Training Loss: 0.3681
Fold 2, Epoch [20/20], Training Loss: 0.3632
Fold 2 Validation Loss: 0.4322
New best model found at fold 2 with validation loss: 0.4322
Starting fold 3/3
Initializing ParatopeModel with 1 repeated layers...
Using 2 GPUs with DataParallel.
Fold 3, Epoch [1/20], Training Loss: 4.5057
Fold 3, Epoch [2/20], Training Loss: 0.6684
Fold 3, Epoch [3/20], Training Loss: 0.6564
Fold 3, Epoch [4/20], Training Loss: 0.6538
Fold 3, Epoch [5/20], Training Loss: 0.6388
Fold 3, Epoch [6/20], Training Loss: 0.6194
Fold 3, Epoch [7/20], Training Loss: 0.6213
Fold 3, Epoch [8/20], Training Loss: 0.6011
Fold 3, Epoch [9/20], Training Loss: 0.5943
Fold 3, Epoch [10/20], Training Loss: 0.5797
Fold 3, Epoch [11/20], Training Loss: 0.5603
Fold 3, Epoch [12/20], Training Loss: 0.5423
Fold 3, Epoch [13/20], Training Loss: 0.5279
Fold 3, Epoch [14/20], Training Loss: 0.5109
Fold 3, Epoch [15/20], Training Loss: 0.4978
Fold 3, Epoch [16/20], Training Loss: 0.4839
Fold 3, Epoch [17/20], Training Loss: 0.4705
Fold 3, Epoch [18/20], Training Loss: 0.4646
Fold 3, Epoch [19/20], Training Loss: 0.4503
Fold 3, Epoch [20/20], Training Loss: 0.4436
Fold 3 Validation Loss: 0.4611
Average Validation Loss across 3 folds: 0.4428
Best model saved successfully.
Layer: module.embedding.weight | Size: torch.Size([22, 128]) | Values : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.5780,  0.8993, -0.1688, -0.0170,  0.6978, -0.9594, -2.0524, -0.3027,
          0.0051,  0.1036,  0.0608, -1.4935, -1.7396, -0.2953,  0.6699, -0.5520,
         -0.0919,  1.8830, -0.5864,  0.1043,  0.9506, -0.4425,  1.1977, -0.6745,
          0.2349,  0.5966, -0.2642, -0.5260,  1.4589, -1.6610, -0.4383, -0.0435,
         -0.8445, -0.4749, -0.0773,  0.8338, -0.5159, -0.0412,  0.0492,  0.3815,
         -1.5500,  0.0324,  0.3096,  0.4667, -0.1380, -1.3047, -0.4513,  1.5591,
         -1.3918,  1.5367, -1.4310, -0.4253,  0.4802,  0.2418,  0.0760, -0.2372,
         -2.4866, -0.1077,  0.0633,  1.1373, -0.8059,  0.4311, -0.5406,  0.1723,
          2.5857, -0.7500,  1.1879, -0.4542,  0.6885,  1.0084, -0.9672,  0.2375,
         -1.5144,  0.5560,  0.2997, -0.6496,  0.0715,  0.1512,  0.3589, -0.2110,
          0.7476, -0.6029,  0.6822, -0.3272,  0.4962,  0.0287, -0.1944,  1.0408,
         -0.4093, -0.5944,  0.5004, -0.1454,  0.5252,  1.0073, -0.3987, -1.0125,
         -1.1475,  0.2152, -0.1789, -0.8671, -0.3589,  0.1833,  0.1259, -0.4516,
          0.4447,  0.1041,  1.0279, -0.8057,  0.6531,  1.0814, -1.7476, -0.1729,
          1.0654, -1.3064, -1.8504, -0.0415,  0.4684,  0.5087,  0.0658, -0.3919,
          0.4020,  1.2864,  0.6433,  1.5612, -0.8165,  0.2589, -0.5874,  0.3877]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.7475, 0.9559], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([0.0003, 0.0416], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[ 0.0672, -0.0638, -0.0350,  0.0823, -0.0536,  0.0347,  0.0502,  0.0683,
         -0.0679,  0.0047,  0.0867, -0.0557,  0.0872, -0.0463,  0.0544,  0.0075,
         -0.0226,  0.0577, -0.0324, -0.0588, -0.0815,  0.0542, -0.0526, -0.0095,
         -0.0664,  0.0618, -0.0850,  0.0070, -0.0504, -0.0815, -0.0747, -0.0558,
          0.0324, -0.0062,  0.0845,  0.0726,  0.0829,  0.0039,  0.0425,  0.0019,
         -0.0142,  0.0800, -0.0565,  0.0238, -0.0602, -0.0854,  0.0318, -0.0461,
         -0.0128, -0.0294,  0.0696, -0.0594,  0.0603, -0.0360,  0.0267,  0.0224,
         -0.0035,  0.0859,  0.0492, -0.0155, -0.0178, -0.0402,  0.0520, -0.0507,
          0.0340,  0.0152,  0.0736,  0.0504, -0.0527,  0.0377,  0.0867,  0.0640,
          0.0603,  0.0654,  0.0281, -0.0586, -0.0550,  0.0642, -0.0006, -0.0260,
          0.0718, -0.0027, -0.0468, -0.0746,  0.0136, -0.0084, -0.0282, -0.0144,
          0.0176, -0.0403,  0.0777,  0.0557,  0.0590, -0.0089,  0.0089, -0.0571,
          0.0833, -0.0860,  0.0784,  0.0158,  0.0794, -0.0049, -0.0208,  0.0557,
         -0.0781, -0.0227, -0.0072, -0.0583, -0.0598, -0.0784, -0.0602,  0.0731,
         -0.0643, -0.0537,  0.0375, -0.0565, -0.0366, -0.0528, -0.0566, -0.0091,
         -0.0767,  0.0048,  0.0681,  0.0439,  0.0097,  0.0262,  0.0752, -0.0441],
        [ 0.0521, -0.0510,  0.0588, -0.0568, -0.0181,  0.0511,  0.0028,  0.0139,
          0.0850,  0.0409,  0.0186,  0.0143, -0.0644,  0.0443,  0.0476,  0.0882,
          0.0429, -0.0498,  0.0231, -0.0856,  0.0573,  0.0440,  0.0682,  0.0236,
         -0.0589, -0.0641, -0.0614,  0.0829,  0.0311,  0.0570,  0.0302, -0.0581,
          0.0541,  0.0179,  0.0366, -0.0174,  0.0118, -0.0701,  0.0125, -0.0222,
          0.0025,  0.0699, -0.0505,  0.0318, -0.0707, -0.0533, -0.0602, -0.0173,
          0.0667,  0.0302,  0.0813,  0.0338,  0.0293, -0.0849,  0.0498, -0.0462,
         -0.0400, -0.0184,  0.0693,  0.0625, -0.0384, -0.0204,  0.0303, -0.0647,
          0.0554,  0.0473,  0.0787,  0.0389, -0.0437, -0.0699,  0.0574,  0.0798,
         -0.0590,  0.0422, -0.0020,  0.0581, -0.0572,  0.0410, -0.0838, -0.0388,
         -0.0628, -0.0850, -0.0243, -0.0523,  0.0237, -0.0232,  0.0597,  0.0371,
          0.0082,  0.0321, -0.0265, -0.0595,  0.0276, -0.0492,  0.0329, -0.0124,
          0.0162,  0.0100, -0.0514,  0.0695, -0.0606,  0.0545,  0.0779, -0.0195,
          0.0172,  0.0333, -0.0844,  0.0512, -0.0629, -0.0198, -0.0098, -0.0856,
          0.0404, -0.0762, -0.0317, -0.0210,  0.0355,  0.0777,  0.0574, -0.0708,
          0.0623,  0.0491, -0.0142,  0.0393,  0.0246,  0.0645,  0.0633, -0.0610]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[-4.1863e-02, -2.7206e-02, -8.3335e-02, -8.8325e-03, -6.0888e-02,
         -2.0138e-02, -2.8417e-02, -4.4066e-02, -6.2218e-02, -4.2453e-02,
          2.9389e-02,  3.1754e-02, -5.9355e-02,  5.9794e-02,  4.3313e-03,
         -7.7322e-02,  3.8263e-02, -1.9122e-02, -2.1598e-03,  1.3967e-03,
          1.9697e-02, -5.1774e-02, -3.3956e-02,  5.7321e-02,  1.0005e-02,
          7.2002e-02,  1.1585e-02,  2.0164e-02,  5.1809e-02,  5.6056e-02,
         -6.1381e-02,  1.2251e-03, -7.2157e-02,  1.0424e-02, -5.0971e-02,
          7.0143e-02, -2.9988e-02,  4.8936e-03,  2.9426e-03, -3.6823e-02,
         -6.2350e-03,  6.7904e-02, -3.6829e-02,  2.7679e-02,  2.1871e-02,
          5.4083e-02, -1.6211e-02, -8.7154e-03,  1.6372e-02, -6.8773e-03,
          2.3908e-03,  6.2797e-02,  3.0403e-03,  4.3925e-02,  4.1098e-02,
         -8.6989e-02,  4.0925e-02,  2.6995e-02,  5.1957e-02, -7.3979e-02,
          3.1412e-02,  4.8083e-02,  2.7006e-02, -8.5484e-02, -5.4299e-02,
          7.8431e-02, -4.5574e-02,  6.4835e-02,  8.3226e-02,  8.4106e-02,
         -3.4821e-02, -4.3025e-02,  3.1469e-02, -5.4129e-02, -7.6918e-02,
         -4.6215e-02,  1.1725e-02, -9.7185e-03,  5.1245e-03, -7.6210e-02,
          3.6143e-02, -2.0456e-03,  8.8294e-02, -7.8245e-02, -1.9988e-02,
          2.2043e-02, -1.6676e-02, -6.0238e-02, -8.1405e-02,  7.3092e-02,
         -3.4247e-02, -5.2613e-02,  3.9799e-02, -4.1964e-03,  3.4897e-02,
         -2.7006e-02,  8.0246e-02,  2.3398e-02,  3.0926e-02,  4.5995e-02,
          8.7933e-02, -1.4036e-02, -6.5807e-02, -4.8935e-02,  3.6696e-02,
          1.7625e-02, -3.6268e-02,  7.0653e-02, -2.8356e-02, -6.3665e-03,
         -6.0971e-02,  3.7105e-02, -5.8167e-03,  1.4085e-02, -3.8213e-02,
         -2.7073e-02,  6.8871e-02,  4.2484e-02,  3.0764e-03,  8.7029e-02,
         -4.2255e-03, -7.9609e-02, -2.1701e-02, -8.1401e-02,  6.8291e-03,
          5.8527e-02, -6.1561e-02, -6.5735e-03],
        [ 4.2058e-02,  4.5561e-02, -1.2025e-02, -2.4105e-02,  1.8394e-02,
          2.1480e-02, -8.2267e-02,  9.3418e-05, -8.2663e-02,  5.8711e-04,
          2.1283e-02, -8.5682e-02, -4.4509e-02, -9.3992e-03, -6.9619e-02,
          7.8211e-02,  8.0829e-02,  5.1831e-02,  1.3413e-02, -7.9778e-02,
         -3.9144e-02, -5.1608e-02,  6.4808e-02, -6.8168e-02,  5.7575e-02,
         -8.6015e-02,  7.6150e-02,  3.3863e-02, -4.1572e-02, -5.1723e-02,
          2.0229e-02, -4.0639e-02, -5.7196e-02,  5.5543e-02,  1.1461e-02,
         -4.8844e-02,  8.3839e-02,  5.6160e-02,  8.0346e-02, -6.6367e-02,
          3.5109e-02,  1.3312e-02, -5.6805e-02,  5.8314e-02,  8.4886e-02,
          7.5667e-02, -7.8571e-02,  3.3395e-03, -1.9713e-02,  2.3552e-02,
          8.2062e-02, -4.9885e-02, -6.3005e-02, -2.9853e-02,  8.2420e-02,
         -4.6321e-02, -7.9117e-02,  7.1663e-02, -4.3667e-02, -4.7789e-02,
         -4.9854e-02,  7.8873e-02, -4.5824e-02,  5.9850e-02, -3.5087e-02,
         -5.9512e-02,  6.5025e-02,  8.7023e-02,  7.6297e-04, -7.8677e-02,
          3.4378e-03, -6.8546e-02,  7.4784e-02,  4.5611e-02,  4.3604e-02,
         -3.2250e-04, -2.5106e-02, -8.8230e-02, -2.0009e-03,  7.0499e-02,
         -4.2859e-03, -4.3050e-02, -3.6068e-02, -7.2308e-02,  3.2161e-02,
          1.0991e-02,  2.0755e-02, -2.3543e-02, -8.7297e-02,  5.4201e-02,
         -3.5848e-02,  7.7042e-02,  2.7879e-02, -6.9318e-02,  4.4295e-02,
         -5.1011e-02, -8.0975e-02,  1.8646e-02, -2.9209e-03,  5.0815e-02,
         -5.4759e-02,  7.4276e-02, -4.5558e-02, -2.1912e-02, -6.0063e-02,
          3.2263e-02,  8.8259e-02,  3.7554e-02, -6.4695e-02, -8.0869e-02,
         -2.9477e-02, -3.3374e-03,  4.2344e-02,  1.3140e-02, -2.0889e-02,
          8.3822e-02, -6.8067e-02,  6.0369e-02,  3.5069e-02, -1.5868e-02,
          7.2811e-02, -5.9414e-02, -1.6657e-02, -4.0015e-02, -4.7047e-02,
         -4.2719e-02, -2.6394e-02,  4.1282e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.0315, -0.0176, -0.0052,  ...,  0.0399,  0.0040, -0.0677],
        [-0.0561,  0.0118,  0.0351,  ..., -0.0182, -0.0040,  0.0235]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.0660,  0.0767], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-0.1016,  0.2911, -0.0244,  0.1123, -0.1704,  0.0390,  0.0485, -0.0519,
          0.2420, -0.0226,  0.0982,  0.1230, -0.2321, -0.0803,  0.1478,  0.1050,
          0.0572, -0.1492,  0.0827, -0.0807, -0.2681, -0.0963, -0.1202,  0.3054,
         -0.0118, -0.0264, -0.0642, -0.1024, -0.1773,  0.2232,  0.4013, -0.1047,
          0.2054,  0.2633,  0.0244,  0.1416,  0.2472,  0.1817, -0.0264,  0.0176,
         -0.0533, -0.4148, -0.3201, -0.1076,  0.2916, -0.0476, -0.0543, -0.0684,
          0.1847,  0.2647, -0.0007, -0.0054, -0.1037,  0.2062,  0.1346,  0.0608,
          0.1443,  0.2518, -0.1078, -0.0232, -0.3048, -0.0725, -0.1619,  0.0733,
         -0.0284, -0.0699, -0.3821,  0.1130, -0.2891, -0.1634,  0.0184, -0.3944,
          0.1261,  0.2069,  0.2169, -0.3142, -0.3501, -0.0491,  0.0662,  0.2108,
          0.2983,  0.1083, -0.1821,  0.1870, -0.0646, -0.1933, -0.0827, -0.3334,
          0.2511,  0.4188, -0.3341,  0.0530,  0.1627, -0.3217, -0.0392, -0.3207,
         -0.1024,  0.3599, -0.4011,  0.2036, -0.0536,  0.4191,  0.1147, -0.1468,
         -0.0061, -0.0569, -0.1808,  0.3279,  0.0392, -0.4153, -0.0894,  0.1532,
         -0.0065,  0.3895,  0.1751, -0.2562,  0.0691, -0.1651,  0.1855, -0.0137,
         -0.0197, -0.2801, -0.2546,  0.0433,  0.1541, -0.0764,  0.0805, -0.2272],
        [-0.0297,  0.1298, -0.0300,  0.1235,  0.0014, -0.1221, -0.0066, -0.0920,
          0.1289, -0.1338,  0.0773,  0.0356, -0.2069, -0.0213,  0.1488,  0.2971,
         -0.0099,  0.0257, -0.0088, -0.0527, -0.1445, -0.0291,  0.0051,  0.1376,
         -0.1094,  0.0122,  0.0999, -0.1178, -0.1925,  0.0951,  0.2273, -0.0574,
          0.0325,  0.1529, -0.0486,  0.1024,  0.0960,  0.0215, -0.0230, -0.0338,
         -0.1072, -0.1853, -0.1419,  0.0450,  0.2957,  0.0267, -0.1782, -0.0408,
         -0.0105,  0.1509,  0.0213, -0.0835, -0.0937,  0.1804,  0.1442,  0.0247,
          0.0410,  0.0312, -0.0987, -0.0404, -0.0109, -0.0104, -0.2150,  0.1110,
          0.0695, -0.0007, -0.2515,  0.0132, -0.0606, -0.0247, -0.0588, -0.2474,
          0.1556,  0.1952,  0.1188, -0.2761, -0.2071, -0.0493, -0.0076,  0.1806,
          0.2199,  0.0843, -0.2042,  0.0981, -0.0767, -0.2049, -0.1618, -0.2420,
          0.1785,  0.2126, -0.2198,  0.2360,  0.0890, -0.3363,  0.0432, -0.0148,
         -0.1391,  0.1524, -0.3645,  0.0769, -0.0640,  0.2439, -0.0630, -0.0897,
          0.0658,  0.0356, -0.2428,  0.0470,  0.2844, -0.3442, -0.0406,  0.1744,
          0.0809,  0.1888,  0.1984, -0.1458, -0.0119, -0.1426,  0.1218,  0.0023,
          0.0976, -0.1574, -0.1598,  0.1028,  0.0874, -0.0773,  0.0541, -0.0714]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.row_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.5014, 0.5677], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.weight | Size: torch.Size([128]) | Values : tensor([0.7666, 0.6783], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.norm.bias | Size: torch.Size([128]) | Values : tensor([0.0187, 0.1952], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_q.weight | Size: torch.Size([512, 128]) | Values : tensor([[-1.7941e-01,  8.4693e-02, -6.6674e-02, -4.7611e-03,  4.3922e-02,
         -3.3046e-02,  1.3707e-01,  4.0889e-02,  2.7739e-02, -3.3411e-02,
          1.4617e-02,  2.0158e-01,  1.0218e-01,  3.0758e-02, -3.6239e-02,
          1.1804e-01, -1.1295e-02,  1.4230e-01, -6.4282e-02,  1.3472e-01,
         -6.3563e-02, -1.7302e-01, -1.6701e-02,  8.1852e-02,  1.8909e-03,
          1.1469e-01, -1.1029e-01,  1.7687e-02, -7.9114e-02,  5.6066e-03,
          1.2878e-02, -7.3355e-02,  1.2204e-01,  1.3485e-01,  3.6985e-02,
         -3.3309e-02, -4.9138e-04,  2.6074e-01,  1.4602e-01,  6.5695e-02,
         -5.4123e-02,  2.6853e-03,  1.3385e-01,  7.9838e-02,  1.0743e-01,
          7.9919e-02, -1.4417e-01, -1.4731e-01, -8.4863e-02,  2.0073e-02,
          1.2359e-02,  3.4883e-03, -1.2915e-02, -5.1852e-02,  1.5306e-01,
         -5.1889e-03,  8.7182e-02, -3.9165e-02,  4.1070e-02, -1.5255e-01,
          9.7814e-02,  5.2650e-02,  7.3131e-02,  4.2924e-02, -3.5406e-02,
          5.3466e-02, -4.3355e-02,  1.3408e-01,  2.7690e-02, -1.1662e-03,
         -1.1744e-01, -1.0647e-01,  1.1376e-01,  3.7333e-02,  6.2866e-02,
          1.9490e-01, -1.1697e-01, -6.8490e-02, -1.8054e-01,  1.1755e-02,
         -4.6839e-02, -5.5170e-02, -5.5288e-02, -6.7138e-03,  1.8134e-01,
          3.6513e-02,  1.0426e-01, -4.6058e-02,  6.2053e-02, -2.0742e-02,
         -9.8865e-02, -1.8366e-01,  2.0035e-02,  6.0052e-02,  5.0121e-02,
         -3.2405e-02, -1.7423e-03, -1.8992e-01,  2.7667e-01,  1.7471e-01,
          6.9737e-02,  9.7869e-02, -6.1874e-02, -3.4589e-02, -6.9652e-02,
         -5.9728e-02, -4.1154e-02, -6.4491e-02, -7.4610e-02, -3.2860e-02,
         -6.8098e-03,  5.2260e-02, -3.0033e-02, -6.1833e-02,  1.6196e-01,
         -1.2390e-02, -3.5235e-02,  5.1252e-04, -5.4539e-02, -1.3280e-01,
          6.3204e-03, -2.9505e-02, -1.0815e-01, -1.0650e-01,  1.5822e-02,
          3.4269e-02,  1.1401e-01, -1.2734e-01],
        [ 1.5965e-02,  1.5645e-02,  5.1468e-02, -1.0517e-02, -2.0949e-01,
         -3.1801e-02, -2.0079e-02, -8.8143e-02, -1.0630e-01,  1.9700e-01,
          1.6606e-01,  4.3569e-02,  5.9411e-02,  1.1258e-01,  8.3384e-02,
          5.7594e-02, -2.4246e-03,  6.6311e-02, -2.2042e-01, -1.3121e-01,
         -1.4614e-01,  5.5147e-02,  2.8543e-02,  5.2169e-02,  2.1890e-01,
         -1.3839e-01, -1.5181e-01,  4.9132e-02, -1.3877e-01, -7.1102e-02,
         -6.9413e-02, -8.2117e-03,  9.7555e-02, -1.1031e-01, -1.0113e-01,
          5.8322e-02,  3.7706e-02,  1.7010e-03, -6.9279e-02,  9.8828e-03,
         -2.5062e-02,  1.2457e-01,  8.7365e-03, -3.2989e-02, -9.0154e-02,
          2.6607e-02, -7.1653e-02,  1.9196e-03,  9.6447e-02,  2.6865e-02,
         -1.3525e-01, -2.6945e-02, -4.6194e-02,  2.6395e-01, -6.4710e-02,
          1.0027e-01,  8.6118e-02, -2.0517e-03,  6.8447e-02,  1.6421e-01,
         -1.2976e-02, -3.0032e-02,  4.8914e-02, -6.4845e-02, -4.2968e-02,
         -2.3999e-02, -1.5713e-01, -5.5173e-02, -5.7875e-02,  9.6152e-02,
         -8.1783e-02,  7.5486e-02, -2.9955e-02, -5.5641e-02, -1.4043e-01,
         -1.5543e-01,  8.5433e-02, -5.9403e-02,  6.4008e-02,  5.0021e-02,
          7.3694e-02,  2.0397e-02,  1.2878e-01,  1.2456e-01, -1.6103e-01,
          5.3679e-02, -3.8517e-02, -1.5293e-01,  1.4827e-01,  2.1311e-02,
          1.1401e-01,  9.0816e-02, -4.7207e-02,  2.8980e-04, -3.2185e-02,
         -1.3276e-01, -1.1206e-01,  8.7684e-02, -9.2171e-02, -3.9774e-02,
         -2.2733e-01, -1.7583e-02,  1.1635e-01,  1.3605e-01, -8.7598e-03,
         -3.4008e-02, -2.4479e-01, -2.0604e-02, -1.5354e-01, -1.7136e-01,
         -1.5199e-03, -2.9136e-03,  8.4282e-03,  7.8713e-02,  1.6331e-02,
         -1.3751e-01,  4.6846e-02,  1.2224e-01, -3.6262e-02,  9.7375e-02,
          9.0467e-02, -2.9804e-01, -1.4970e-02,  8.3479e-02, -1.3840e-01,
          2.8138e-02,  1.7046e-01,  7.0376e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_kv.weight | Size: torch.Size([1024, 128]) | Values : tensor([[ 1.1451e-02,  1.0857e-01, -1.9557e-01,  5.2371e-02,  1.3323e-01,
         -2.8146e-02, -1.0457e-02,  2.2205e-01, -6.0981e-02, -6.4014e-03,
         -9.6834e-02, -2.3026e-01,  3.6731e-02,  1.9483e-01, -1.9484e-02,
          4.6590e-02,  5.5786e-02, -4.2763e-03, -2.9553e-01,  1.0470e-01,
         -2.0329e-02, -3.2968e-02,  8.6884e-02,  5.1540e-02, -1.8635e-01,
         -5.2439e-02, -4.1457e-02,  3.1759e-03,  1.4702e-01, -9.2509e-02,
         -1.6154e-01, -4.8480e-02, -7.7256e-02,  1.0111e-02, -1.2273e-01,
          6.2487e-02, -3.1105e-02, -1.1547e-01, -6.1335e-02, -4.0690e-02,
          6.6433e-02,  2.4696e-01, -3.2381e-02,  2.6087e-02,  5.8696e-02,
          1.9964e-02,  4.6658e-02,  3.6467e-02,  1.9617e-03,  1.6242e-02,
         -7.1439e-02, -7.6239e-03, -1.3660e-02,  3.3726e-02, -2.8534e-02,
         -2.6758e-02,  1.4344e-01,  3.1407e-02, -8.2952e-02, -8.9696e-02,
          1.1650e-01,  1.3638e-01, -4.6108e-02, -1.7310e-02, -8.2107e-02,
         -6.4057e-02, -7.6079e-02, -6.4359e-02,  2.2876e-02, -2.0403e-02,
         -1.6808e-01,  1.0712e-01,  1.9103e-01, -1.4680e-01, -1.4192e-01,
          9.2541e-02,  9.3562e-02,  6.7174e-02, -5.4211e-02, -2.0968e-01,
          1.8196e-01, -3.4381e-02, -7.8426e-02, -1.2271e-01,  1.2431e-01,
          1.1711e-01, -2.4987e-02,  6.2982e-02,  1.2604e-01, -1.0918e-02,
         -8.5732e-03,  1.8907e-01,  7.3832e-02,  1.5766e-01, -1.3135e-01,
          1.6080e-02, -2.3441e-01, -9.7019e-02, -2.3426e-02,  2.4153e-01,
          1.2852e-01,  1.7151e-02, -4.2932e-02, -2.7848e-02,  1.0593e-01,
         -1.1055e-02,  5.3743e-02,  4.6591e-02, -1.1505e-01, -5.2441e-03,
         -7.8431e-02, -8.3138e-02,  1.1664e-01, -3.9302e-02, -5.6112e-02,
         -5.3097e-05, -1.7828e-01, -1.6765e-02, -3.7759e-02,  1.5326e-01,
          9.2001e-02,  1.8315e-01,  2.0055e-01, -5.8563e-02,  4.4142e-02,
          8.3765e-02, -1.6922e-01,  7.4255e-03],
        [-9.0621e-02, -5.3833e-02, -8.9471e-02,  3.1319e-02,  2.4279e-02,
         -1.2918e-01,  5.9973e-02, -8.4631e-02,  2.0931e-02,  1.9921e-01,
         -2.6179e-02,  1.9062e-02,  3.3504e-02,  9.3771e-02,  3.4676e-02,
         -1.1182e-01, -8.3349e-02,  5.0364e-04, -1.1954e-01,  4.2623e-02,
          5.3194e-02, -1.5838e-01,  2.5143e-01, -6.2330e-02,  1.7903e-01,
          1.5526e-01,  1.8416e-02,  2.2346e-02, -1.2157e-01,  1.0576e-01,
          1.9383e-01, -2.0148e-02, -8.5401e-04, -1.2580e-01, -1.2189e-01,
         -1.5944e-01, -5.6198e-02,  9.7999e-02,  1.9468e-02,  6.4827e-02,
          8.3889e-03, -4.2438e-02,  9.2194e-02,  2.4463e-02,  9.2085e-02,
          8.7106e-02, -7.1104e-02, -1.3813e-01,  1.0244e-01, -4.4366e-02,
          4.3783e-02, -7.1510e-03, -7.7684e-02, -3.2950e-02, -6.0548e-03,
          1.0187e-01, -1.1581e-02, -1.5116e-01,  1.2380e-01, -9.1692e-02,
          5.5276e-02, -3.2949e-02,  5.9970e-02, -4.6775e-02,  1.6185e-02,
          1.8583e-02,  9.0339e-02,  5.3165e-02, -6.5999e-02, -5.2970e-02,
          2.5529e-02, -1.7596e-02, -7.7681e-02, -1.2506e-01,  2.1345e-02,
          2.6771e-02, -1.2613e-01, -1.0125e-01, -1.6729e-02,  6.1461e-02,
         -7.6876e-02, -1.0107e-01,  7.4572e-02,  7.7267e-02,  3.1597e-02,
          3.0652e-02, -3.1209e-02, -1.6383e-01,  1.4954e-01, -1.0396e-01,
          5.6112e-02, -1.8320e-01,  2.6844e-02, -7.5464e-03,  9.1881e-03,
          9.9565e-03,  1.3371e-01, -9.2029e-02,  2.6746e-02,  3.2998e-02,
         -2.5383e-01, -1.8138e-01, -1.5935e-01,  3.4264e-02, -4.1437e-02,
          4.2904e-02, -4.2907e-02,  3.8972e-02, -9.6771e-02,  5.4017e-03,
          1.2104e-02,  1.4041e-01, -6.2264e-02,  1.6898e-01,  2.0553e-01,
          1.1917e-01, -2.8059e-02,  1.0980e-02, -7.2592e-02, -1.2186e-02,
          1.3200e-01,  2.2034e-02, -1.1145e-01,  1.5121e-01, -1.0701e-01,
          1.9409e-01,  2.1849e-01, -5.8511e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.weight | Size: torch.Size([128, 512]) | Values : tensor([[ 0.0315,  0.0228,  0.0043,  ...,  0.0481,  0.0096,  0.0219],
        [ 0.0065, -0.1331,  0.1154,  ...,  0.0217,  0.0075,  0.0060]],
       device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.to_out.bias | Size: torch.Size([128]) | Values : tensor([-0.0362, -0.0087], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.weight | Size: torch.Size([512, 128]) | Values : tensor([[-2.0798e-03,  1.2352e-02, -4.6000e-02, -8.8497e-02, -3.0155e-02,
         -1.0158e-01,  1.7860e-01,  2.7609e-03,  1.5629e-01,  1.7364e-01,
         -4.3250e-02,  1.6460e-01,  2.3025e-01,  2.0100e-01, -1.9275e-02,
          5.9343e-02, -8.4664e-02,  1.1889e-01, -3.1554e-01,  2.6394e-02,
          3.3419e-01, -2.7075e-02, -3.2310e-02, -1.1467e-01,  8.1748e-02,
          1.8771e-01, -9.5724e-02,  2.1696e-01,  6.7143e-02, -6.0459e-02,
          5.1313e-03,  7.6724e-02, -8.4469e-02, -5.6611e-05,  1.0730e-01,
         -2.7965e-01, -1.1431e-01, -1.5319e-01, -4.0203e-02, -4.1539e-02,
         -1.3619e-01, -1.0111e-01, -1.6662e-01,  6.8795e-02, -1.4731e-01,
         -2.7003e-02,  9.6577e-02, -1.3671e-02, -1.3669e-01,  1.5688e-02,
         -9.4283e-03, -1.7230e-01,  6.9137e-03,  2.7144e-01,  1.8770e-01,
         -2.4217e-01, -9.6819e-03,  5.2911e-02, -4.4288e-02,  1.2241e-01,
          8.2019e-02,  4.4952e-02,  2.1543e-01, -9.5161e-02, -2.1715e-01,
          3.9153e-02,  1.5112e-02, -5.5224e-02, -1.6575e-01,  6.8627e-02,
         -3.8988e-02,  9.0908e-02, -1.1472e-01, -2.6562e-02,  7.2406e-02,
         -7.3305e-02, -1.2398e-01, -1.2299e-01,  2.4214e-01,  2.6133e-02,
          1.2128e-01,  6.3067e-02, -7.8540e-02, -5.0182e-02, -6.9334e-02,
         -1.4709e-01,  1.7413e-01,  2.1288e-01, -1.3487e-01,  2.1918e-01,
          1.2990e-01,  4.5311e-02, -6.9222e-02,  2.4336e-01,  1.8356e-02,
          1.4709e-02,  3.0734e-02, -1.5825e-02,  1.3336e-01, -2.2951e-01,
          4.0087e-02, -6.2942e-03,  2.2729e-02,  1.0865e-01,  3.1718e-01,
         -2.3030e-01, -1.8198e-01,  4.2370e-02, -1.8675e-01, -2.5026e-01,
         -6.5558e-02,  1.3630e-02, -1.1036e-01, -4.4700e-01,  5.2651e-03,
          1.3387e-02,  1.4723e-01,  8.8937e-02, -5.4597e-03, -2.8752e-02,
          1.7636e-01,  8.3184e-02, -1.8755e-02, -3.6389e-01,  3.1975e-02,
         -5.7540e-02, -1.2090e-01, -2.5617e-02],
        [-1.5425e-01,  9.2004e-02,  7.0839e-02, -5.9283e-03,  1.6335e-01,
         -1.3870e-01, -2.2715e-01, -1.1016e-01,  1.5708e-01,  7.5721e-02,
          1.8979e-02,  3.6278e-02,  5.0659e-02, -4.5067e-02,  4.9393e-02,
          1.3738e-01, -7.5080e-02,  1.4285e-01,  1.1181e-01,  2.5330e-02,
          4.6304e-02, -4.9632e-02,  8.2835e-02,  6.5582e-03,  1.6893e-03,
          1.8209e-01, -1.7172e-02,  6.5977e-02, -1.1318e-01,  5.6775e-02,
          7.1369e-02,  8.9854e-02,  2.9394e-02,  7.1533e-02,  9.3716e-02,
         -2.1812e-01,  3.1531e-02,  3.3261e-02, -7.2562e-02, -1.3705e-01,
          5.0357e-02, -2.5713e-01, -1.2128e-01, -2.3015e-02,  2.7215e-01,
          1.8061e-02,  4.7037e-02, -1.3863e-01, -4.7792e-02, -3.4058e-02,
         -1.7568e-01, -9.5793e-02,  1.5021e-01,  3.1042e-02,  5.8335e-02,
          9.6534e-02, -1.3261e-01, -1.1785e-01, -3.5565e-02,  1.9745e-02,
         -4.4644e-02, -3.1556e-02, -1.0575e-01,  1.2545e-01,  4.2586e-02,
          1.1529e-01,  1.2782e-01, -2.1039e-03, -2.0043e-01, -1.2756e-01,
         -6.9752e-03, -3.6496e-02,  7.6025e-02,  3.0201e-02,  1.6363e-01,
         -3.9889e-03, -1.5808e-01, -1.8966e-01, -1.9085e-01,  1.7327e-01,
          1.4968e-01,  9.6366e-02,  2.6951e-02,  7.3332e-02, -5.2992e-02,
         -2.1035e-01, -3.6353e-02,  9.3694e-03,  1.1363e-02,  6.2783e-02,
         -6.0720e-02,  2.2025e-01, -1.0607e-01, -1.3088e-01, -4.5549e-02,
         -3.9102e-02, -1.7662e-01, -1.1806e-01, -2.3815e-02, -1.5814e-02,
          4.0542e-02,  4.8886e-02, -3.2764e-02, -4.1652e-02,  7.1785e-02,
          1.2027e-02, -1.4377e-01,  6.5715e-03,  8.0498e-02, -2.8848e-01,
         -7.9253e-02,  1.6292e-01, -2.3573e-01, -1.2009e-01,  9.2969e-02,
         -1.0885e-01,  1.2541e-01, -2.7945e-02,  3.0321e-02,  2.7822e-02,
          1.8657e-02,  1.0760e-01, -1.1006e-01, -1.9193e-02,  8.5824e-02,
          8.7913e-02,  8.3552e-02, -2.0754e-01]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.self_attention.col_attn.attn.gating.bias | Size: torch.Size([512]) | Values : tensor([0.7885, 0.8581], device='cuda:0', grad_fn=<SliceBackward0>)
Layer: module.fc.weight | Size: torch.Size([2, 128]) | Values : tensor([[ 3.5486e-02,  1.1663e-03,  2.0631e-02,  2.2293e-02, -5.8119e-02,
          3.6540e-02, -4.0884e-02,  2.4996e-02,  3.8262e-02,  3.1219e-02,
         -2.2018e-03, -2.6686e-02,  2.3579e-02,  9.6021e-03,  7.4059e-02,
         -1.9057e-02,  7.5841e-02,  5.1838e-02,  1.3784e-02, -6.6324e-02,
          1.6180e-02, -3.6431e-02, -5.9532e-02, -5.6236e-02, -1.7014e-02,
          4.6595e-03, -2.6766e-02, -2.7567e-02,  6.2066e-02, -7.3484e-02,
         -3.2978e-02,  3.7890e-03, -5.0608e-02, -7.5312e-02,  5.9735e-02,
         -2.1750e-02,  2.2878e-03,  6.0507e-02, -3.3534e-03,  6.9463e-02,
         -5.3362e-02,  2.5406e-02,  5.2237e-02,  3.2621e-02, -2.8462e-02,
         -2.5126e-02,  8.1739e-02,  6.8570e-02,  1.2542e-03,  5.9394e-02,
          2.9259e-02, -3.9976e-02, -2.1590e-02,  3.7035e-02, -6.8389e-02,
         -3.0595e-02, -5.4274e-02, -2.8484e-02, -4.6998e-02,  3.1278e-02,
         -1.7475e-02, -2.0401e-02, -5.6226e-04,  5.5176e-04, -3.2790e-02,
          1.8641e-02, -3.5306e-02,  2.0837e-03, -2.6413e-02, -4.0889e-02,
         -7.7867e-03, -2.2228e-02,  2.7224e-02,  2.2337e-02, -3.6642e-02,
         -4.2997e-02, -3.4385e-02, -3.6260e-02,  5.8725e-02, -8.6662e-02,
         -6.9336e-02,  5.7246e-02, -6.1409e-02, -7.8741e-02, -4.5248e-02,
         -2.3551e-02,  3.3807e-02,  3.3930e-02, -4.6939e-02,  4.9508e-02,
         -7.8582e-03, -3.1512e-02, -1.3650e-02, -5.3279e-03,  2.3383e-03,
         -5.1414e-03, -7.4488e-02,  7.6637e-03,  2.8863e-02, -5.7378e-02,
          4.2959e-02,  3.5814e-02,  3.3182e-02,  1.7137e-02,  1.9471e-02,
          1.2046e-02, -3.6026e-02, -4.7370e-02, -3.2959e-02, -3.6160e-02,
          7.3090e-02, -6.2617e-02, -4.9340e-03, -1.0757e-02, -1.4811e-02,
         -2.3548e-02, -3.4289e-02, -5.0006e-02, -6.4007e-02, -2.2995e-03,
         -2.6926e-03, -1.7192e-02, -2.3998e-02, -1.9679e-02, -4.7656e-05,
         -3.4944e-02,  2.4699e-02,  2.9579e-02],
        [ 3.8578e-02,  2.1382e-03,  2.1634e-02,  2.1707e-02, -5.8156e-02,
          3.6700e-02, -3.6254e-02,  2.5902e-02,  4.4409e-02, -3.1997e-02,
         -5.7976e-03, -2.3873e-02,  2.3148e-02,  9.3518e-03,  6.4799e-02,
         -1.6150e-02,  7.3826e-02,  4.6203e-02,  1.4163e-02, -6.6069e-02,
          2.3193e-02, -3.5683e-02, -6.5124e-02, -5.6208e-02, -2.2445e-02,
          4.3955e-03, -2.1610e-02, -2.8091e-02,  7.1617e-02, -7.3902e-02,
         -2.8051e-02,  1.0320e-02, -4.9023e-02, -7.9271e-02,  6.0866e-02,
         -2.6034e-02,  7.7775e-04,  6.4310e-02,  6.4196e-04,  7.0571e-02,
         -5.7429e-02,  2.3643e-02,  5.1374e-02,  2.7953e-02, -2.6976e-02,
         -2.6690e-02,  8.3358e-02,  6.6007e-02,  3.1900e-03,  6.0540e-02,
          2.8090e-02, -3.5518e-02, -2.0494e-02,  4.1491e-02, -5.9427e-02,
         -3.5835e-02, -5.8487e-02, -2.7754e-02, -4.5706e-02,  3.2602e-02,
         -1.8986e-02, -1.3006e-02,  2.9553e-03,  3.3877e-03, -3.4862e-02,
          1.5890e-02, -3.4513e-02,  1.2963e-03, -2.4509e-02, -4.6397e-02,
         -1.8125e-03, -2.4382e-02,  2.7461e-02,  1.8523e-02, -3.5571e-02,
         -4.0257e-02, -4.1461e-02, -3.6673e-02,  6.0657e-02, -8.7406e-02,
         -6.6312e-02,  5.8832e-02, -6.3417e-02, -8.8680e-02, -4.3662e-02,
         -2.3791e-02,  3.7465e-02,  3.6405e-02, -4.9049e-02,  5.0128e-02,
         -8.4880e-03, -2.5608e-02, -9.5751e-03, -2.3156e-03,  1.1397e-02,
          1.0291e-03, -7.6438e-02,  4.7074e-03,  1.7054e-02, -5.6917e-02,
          4.6847e-02,  3.5885e-02,  3.0416e-02, -1.2976e-02,  2.5514e-02,
          1.5161e-02,  1.8461e-02, -4.8230e-02, -3.6106e-02, -4.1101e-02,
          7.5142e-02, -5.7566e-02, -5.4307e-03, -1.5128e-02, -1.4822e-02,
         -2.8421e-02, -3.6635e-02, -4.6051e-02, -6.5379e-02, -4.1929e-03,
         -2.1918e-02, -1.8752e-02, -1.9798e-02, -2.5645e-02, -9.5690e-04,
         -3.5447e-02,  5.0926e-03,  3.2687e-02]], device='cuda:0',
       grad_fn=<SliceBackward0>)
Layer: module.fc.bias | Size: torch.Size([2]) | Values : tensor([0.0335, 0.0689], device='cuda:0', grad_fn=<SliceBackward0>)
