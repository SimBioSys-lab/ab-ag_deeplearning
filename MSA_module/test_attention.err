Traceback (most recent call last):
  File "/work/SimBioSys/Xing/ab-ag_deeplearning/MSA_module/attention.py", line 96, in <module>
    predictions = model(sequences_tensor)
                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/SimBioSys/Xing/ab-ag_deeplearning/MSA_module/attention.py", line 72, in forward
    embedded = self.embedding(sequences)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 164, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/home/xing.liu/miniconda3/envs/msa_attention/lib/python3.12/site-packages/torch/nn/functional.py", line 2267, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 306385238016 bytes. Error code 12 (Cannot allocate memory)
